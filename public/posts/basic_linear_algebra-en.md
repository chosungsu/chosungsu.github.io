---
title: 'Basic linear algebra'
date: '2024-05-03'
tags: ['Probability&Statistics', 'math']
---

### vector space

A real solution vector space is a set of vectors composed of scalar products and vector sums. It is denoted by the symbol $R^n$.

---

### linear function

When there are two vector spaces $V$ and $W$, $f : V \rightarrow W$ is said to have linearity when $f(cx+y) = cf(x) + f(y)$ holds under the condition $\{\forall x, y \in V \And c \in R\}$.

All linear functions can be expressed as matrix multiplication as follows:

$$
f(x) = Ax
$$

---

### matrix

If $A = (a_{ij})$ is a matrix of size $n*p$, it can show linearity from $R^p$ to $R^n$.

Therefore, it can be defined as follows:

$$
A \in R^{n*p} \\
A^T \in R^{p*n} \\
I_n : n*n
$$

---

### linear subspace

$$
x, y \in L \And c \in R \\
cx + y \in L
$$

In the above case, $L \subset R^n$ is said to be a subspace of $R^n$.

$$
L = \{c_1x_1 + ... + c_kx_k : c_j \in R\} \\ 
\overset{\underset{\mathrm{def}}{}}{=} \{x_1, ..., x_k\}
$$

In the above case, $L$ is said to be the set generated from the set of $x$.

---

### linear independence and basis

If the condition $c_1 = ... = c_k = 0$ is satisfied in $c_1x_1 + ... + c_kx_k = 0$, then $\{x_1, ..., x_k\}$ is said to be linearly independent.

And if the above set is $L$, it can be called a basis of $L$, and here $k$ is defined as the dimension of $L$, that is, dim($L$).

---

### inner product space and normed space

For an inner product space $V$, if an inner product is formed as $V*V \rightarrow R$, then for $\forall x, y \in V$, <$x, y$> = <$y, x$>, <$cx + y , z$> = a<$x,z$> + <$y,z$>, <$x, x$> > 0 are satisfied.

For a normed space $V$, if it is formed as $V \rightarrow R$, then $\{\forall x \in V\} \rightarrow |x| \ge 0$, $x = 0 \rightarrow |x| = 0$, $|cx| = |c||x|$, $\{\forall x, y \in V\} \rightarrow |x+y| \le |x| + |y|$ are satisfied.

The inner product is expressed as $|x| = \sqrt{<x, x>}$, and the generalized formula for $l^p$ norm is as follows:

$$
|x| = |x|_{p} \overset{\underset{\mathrm{def}}{}}{=} (\sum_{i=1}^{n} x_i^{p})^{1/p}
$$

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFIyhg%2FbtqCRyyEJ86%2FUbaS1cpDzIk0wvuc1DeeK1%2Fimg.png" style="display: block; margin: 0 auto; height:200;" />

Referring to the generalized formula and image, it can be seen that $l^1$ norm is the sum of absolute values, and $l^2$ norm is the square root of the sum of squared values.

---

### cauchy-schwartz inequality

When $V$ is in an inner product space, $|<x, y>|^2  \le |x|^2|y|^2$ holds.

---

### orthonormal basis

The basis set of the inner product space of $L$ is orthogonal. For example, if $i \neq j$, then $<x_i, x_j> = 0$. Such an orthogonal basis set is said to be orthonormal if $|x_i| = 1$.

---

### column, row and null spaces

When $A \in R^{n*p}$, the column space is defined as $C(A)$ and means the linear subspace generated by the columns of $A$. The row space is defined as $R(A)$ and means the subspace generated by the rows of $A$.

First, to find this, matrix A must be reduced to upper triangular U using Gaussian elimination. At this time, when the rank is found, these columns become the basis of the column space. Conversely, the corresponding number of rows becomes the basis of the row space.

Below is an example:

$$
A = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix} \\

\rightarrow 

RREF(A) = \begin{bmatrix}
1 & 0 & -1 \\
0 & 1 & 2 \\
0 & 0 & 0
\end{bmatrix}
$$

When this matrix exists, since the rank is 2, the pivot columns of the column space are columns 1 and 2, so the basis of the column space must be found from matrix A, resulting in $C(A) = span\{\begin{bmatrix} 1 \\ 4 \\ 7 \end{bmatrix}, \begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix}\}$, and the row space must be found from the RREF(A) matrix, resulting in $R(A) = span\{\begin{bmatrix} 1 & 0 & -1 \end{bmatrix}, \begin{bmatrix} 0 & 1 & 2 \end{bmatrix}\}$.

Therefore, comparing dimensions, dim($C(A)$) = dim($R(A)$) = rank($A$).

---

### eigenvalue and eigenvector

The eigenvalue of matrix $A$ is denoted as $\lambda$ and can be calculated as $Ax = \lambda x$, where $x$ becomes the eigenvector.

The condition for $\lambda$ to be an eigenvalue is when det($A-\lambda I_n$) = 0.

---

### References

[Original Path #1](https://www.dropbox.com/scl/fi/v36rpgglbohl4v2mw55x5/Chap1-linearAlgebra.pdf?rlkey=4mgcyr1cc4xs8ik7gurh1q93y&e=1&dl=0)



