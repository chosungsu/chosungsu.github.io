---
title: 'Markov Decision Processes'
date: '2025-03-07'
tags: ['reinforcement learning', 'lecture']
---

### Overview

마르코프 결정 과정은 강화 학습 환경을 공식적으로 설명하는 도구입니다.

환경이 완전히 관측 가능한 경우 현재 상태가 전체 과정을 완벽하게 특징지을 수 있게 됩니다. 마르코프 속성(property)는 현재가 주어지면 미래는 과거와 독립이라는 의미를 갖고 있는데 이는 상태가 히스토리로부터 모든 정보를 받을 수 있는 충분 통계량이기 때문입니다.

상태 전이 행렬(state transition matrix)은 모든 상태 $s$에서 모든 다음 상태 $s'$로의 전이 확률인데 이는 $P=\begin{bmatrix} P_{11} & \cdots & P_{1n} \\ \vdots & \ddots & \vdots \\ P_{n1} & \cdots & P_{nn} \end{bmatrix}$로부터 정의되고 각 행의 합은 1입니다.

---

### Markov Reward Process

마르코프 보상 과정은 가치(value)를 포함합니다. 이는 $<S, P, R, \gamma>$ 튜플 형태이고 각각은 유한한 상태 집합, 상태 전이 확률 행렬, 보상 함수, 할인율에 해당합니다.

#### 1. Return & Discount factor

Return인 $G_t$는 시점 $t$부터의 총 할인된 보상입니다. $G_t=R_{t+1}+\gamma*R_{t+2}+ \cdots=\sum_{k=0}^{\infty} \gamma^k*R_{t+k+1}$을 통해서 계산이 가능하고 $\gamma^k*R$은 $k+1$ 시점이 지난 후 받는 보상의 가치이며 이는 지연된 보상보다 즉각적인 보상을 더 높게 평가합니다. $\gamma$가 0에 가까울 수록 근시안적 평가를 하는 경향을 보입니다.

대부분의 마르코프 보상 결정과정은 할인계수가 있는데 이유로는 보상을 할인하는 것이 편리하고 순환과정에서 무한한 $G$값을 피하는 방법이기 때문입니다. 그리고 미래에 대한 불확실성이 완전 반영이 되지 않을 수 있으며, 보상이 재정적이면 지연보상보다 즉각적인 보상이 더 많게 됩니다.

#### 2. Value function

가치 함수는 상태 $s$의 장기적인 가치를 제공하는데 $v(s)=E[G_t|S_t=s]$처럼 상태 $s$에서 시작할 때의 기대값으로 정의합니다.

#### 3. Bellman Equation

가치 함수는 두 부분으로 분해될 수 있습니다.

$\rightarrow$ 즉각적인 보상인 $R_{t+1}$

$\rightarrow$ 다음 상태의 할인된 가치인 $\gamma*v(S_{t+1})$

위 가치 함수는 다음과 같이 재정의될 수 있습니다.

$$
v(s)=E[R_{t+1}+\gamma*R_{t+2}+\cdots|S_t=s] \\
=E[R_{t+1}+\gamma*(R_{t+2} + \gamma*R_{t+3} + \cdots)|] \\
=E[R_{t+1}+\gamma*G_{t+1}|S_t=s] \\
=E[R_{t+1}+\gamma*v(S_{t+1})|S_t=s]
$$

이처럼 벨만 방정식은 상태 $s$의 가치가 즉각적인 보상과 다음 상태의 기대가치로 구성됨을 알 수 있습니다. 계산 복잡도는 $n$개 상태에 대해 $O(n^3)$를 가집니다.

---

### Markov Decision Process

마르코프 결정 과정은 의사결정이 추가된 과정으로 모든 상태가 마르코프 속성을 갖습니다.

#### 1. Definition

$<S, A, P, R, \gamma>$로 구성된 튜플입니다.

#### 2. Policy

정책 $\pi$는 주어진 상태에 대한 행동의 확률 분포로 $\pi(a|s)=P[A_t=a|S_t=s]$와 같이 정의됩니다. 마르코프 결정 과정은 현재 상태에만 의존하며 정적입니다.

주어진 MDP $M$와 정책 $\pi$에 대해 상태 시퀀스인 $S_1, S_2, \dots$가 마르코프 과정이고 상태 및 보상 시퀀스는 마르코프 보상 과정 $<S,P^{\pi}, R^{\pi}, \gamma>$입니다.

여기서 $P_{s,s'}^{\pi}=\sum_{a \in A} \pi(a|s)*P_{ss'}^a$로 이는 상태 $s$에서 행동 $\pi$를 따랐을 때 다음 상태가 $s'$일 확률이고 $R_s^{\pi}=\sum_{a \in A} \pi(a|s)*R_{s}^a로 이는 상태 $s$에서 행동 $\pi$를 따랐을 때 얻게 될 기대 보상을 의미합니다.

#### 3. Value function

$v_{\pi}(s)=E_{\pi}[G_t|S_t=s]$로 상태 가치 함수는 상태 $s$에서 시작하여 정책 $\pi$를 따랐을 때의 기대 반환값이라 정의되며 행동 가치 함수 $q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s, A_t=a]$는 상태 $s$에서 행동 $a$를 취한 후 정책을 따랐을 때의 기대 반환값으로 사용됩니다.

가치 함수들은 다음과 같이 연결되어 있습니다.

$v_{\pi}(s)=\sum_{a \in A} \pi(a|s)*q_{\pi}(s,a)$에서 상태 $s$의 가치는 모든 가능한 행동 $a$에 대한 행동 가치를 정책의 확률에 따라 평균낸 값을 계산 가능하고 $q_{\pi}(s,a)=R_s^a + \gamma*\sum_{s' \in S} P_{ss'}^a v_{\pi}(s')$에서 행동 가치는 즉각적인 보상과 행동을 했을 때 갖는 모든 다음 상태의 가치를 전이 확률에 따라 가중평균한 값을 계산 가능합니다.

#### 4. Bellman Equation

위 두 식을 참고하여 아래와 같이 벨만 방정식을 유도할 수 있습니다.

$$
v_{\pi}=R^{\pi}+\gamma*P^{\pi}v_{\pi}
$$

최적 상태 가치 함수인 $v_*(s)$는 모든 정책들 중에서 가장 높은 가치를 가지는 함수로 $v_*(s)=max_{\pi}v_{\pi}(s)$로 정의됩니다. 그리고 최적 행동 가치 함수인 $q_*(s,a)$는 모든 정책들 중에서 가장 높은 행동 가치를 가지는 함수로 $q_*(s,a)=max_{\pi}q_{\pi}(s,a)$로 정의됩니다.

위와 같은 최적 가치 함수들은 MDP에서 가능한 최고의 성능을 나타내고 모든 정책에 대한 부분 순서(partial ordering)을 정의할 수 있습니다.

MDP에서는 항상 결정론적인(deterministic) 정책이 존재하고 $q_*(s, a)$를 알면, 즉시 최적 정책을 얻을 수 있습니다. 또한 벨만 최적 방정식을 통해 재귀적으로 연결됩니다.

$$
v_*(s)=max_a q_*(s,a), \\
v_*(s)=max_a (R_s^a+\gamma*\sum_{s' \in S} P_{ss'}^a v_*(s')), \\
q_*(s,a)=R_s^a+\gamma*\sum_{s' \in S} P_{ss'}^a max_{a'}q_*(s',a')
$$

---

### 참고 자료

[원본 경로 #1](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf)



