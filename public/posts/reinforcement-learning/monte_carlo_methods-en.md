---
title: 'Monte Carlo Methods'
date: '2025-03-14'
tags: ['reinforcement learning', 'lecture']
---

### Monte Carlo Prediction

Monte Carlo (MC) methods are algorithms used to learn the state-value function $v_\pi$ for a given policy through experience (episodes).

The value of a state is the expected return expected when starting from that state. Therefore, the most straightforward way to estimate this from experience is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value.

#### First-Visit vs. Every-Visit MC

Each occurrence of state $s$ in an episode is called a visit to $s$. In first-visit MC, $v_\pi(s)$ is estimated as the average of the returns following first visits to $s$ in the episode. In every-visit MC, $v_\pi(s)$ is estimated as the average of the returns following all visits to $s$.

Both methods converge to $v_\pi(s)$ as the number of visits goes to infinity. First-visit MC is guaranteed to converge by the law of large numbers, as each return is an independent, identically distributed (i.i.d.) estimate of $v_\pi(s)$.

---

### Monte Carlo Estimation of Action Values

When a model is not available, it is particularly useful to estimate action values ($q_\pi$) rather than state values ($v_\pi$). Without a model, $v_\pi$ alone is insufficient to determine a policy; we must explicitly estimate the value of each action to propose a useful policy.

MC methods for estimating action values $q_\pi(s, a)$ are essentially the same as for state values, but now we deal with visits to state-action pairs $(s, a)$. If we follow a deterministic policy, we will observe returns for only one action from each state. If we cannot average returns, then MC estimates for other actions will not improve through experience.

---

### Monte Carlo Control

We consider how MC estimation can be used to approximate optimal policies $\pi^*$.

In generalized policy iteration, we maintain an approximate policy $\pi$ and an approximate value function $q$, and they are iteratively updated toward each other.

$$
\pi \underset{\text{evaluation}}{\stackrel{q \to q_\pi}{\longrightarrow}} q \underset{\text{improvement}}{\stackrel{\pi \to \text{greedy}(q)}{\longrightarrow}} \pi
$$

By the policy improvement theorem, each $\pi_{k+1}$ is uniformly better than or equal to $\pi_k$.

#### Without Exploring Starts

The only general way to avoid the unrealistic assumption of exploring starts is to ensure that the agent continues to select all actions. There are two approaches to ensuring this, called on-policy and off-policy methods.

The former case attempts to evaluate or improve the policy that is used to make decisions. The latter case attempts to evaluate or improve a policy different from that used to generate the data.

In on-policy control, the policy is generally soft, meaning that $\pi(a|s) > 0$ for all states $s$ and all actions $a$, but gradually shifted closer to a deterministic optimal policy. We move the policy only to $\epsilon$-greedy policies. For any $\epsilon$-soft policy $\pi$, the $\epsilon$-greedy policy $\pi'$ with respect to $q_\pi$ is guaranteed to be better than or equal to $\pi$ by the policy improvement theorem.

$$
q_\pi(s, \pi'(s)) \ge v_\pi(s)
$$

Off-policy learning refers to learning about a target policy $\pi$ from data generated by a different behavior policy $\mu$. To use episodes from $\mu$ to estimate values for $\pi$, every action taken under $\pi$ must also be taken, at least occasionally, under $\mu$. This is called the coverage assumption.

Importance sampling is a general technique for estimating expected values under one distribution given samples from another. We weight the returns by the importance sampling ratio $\rho^T_t$, which is the relative probability of the trajectory occurring under the target and behavior policies. Given a starting state $S_t$, the importance sampling ratio for the trajectory $A_t, S_{t+1}, \dots, S_T$ is:

$$
\rho^T_t = \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
$$

#### Incremental Implementation

It can be implemented incrementally, episode by episode. In MC methods, we average returns instead of rewards.

For off-policy using weighted importance sampling, a slightly different incremental algorithm is needed because we must form a weighted average. When $G_n$ is the return and $W_n$ is the corresponding importance sampling ratio ($\rho$), the new estimate $V_{n+1}$ is computed using the following update rule:

$$
V_{n+1} = V_n + \frac{W_n}{C_n} \left[ G_n - V_n \right]
$$

where $C_n = \sum_{k=1}^{n} W_k$ is the cumulative sum of weights.

---

### Importance Sampling on Truncated Returns

When the discount rate $\gamma < 1$, there may be a better way to treat the importance sampling ratio than as a single lump. In this case, the return $G_t$ is primarily determined by early rewards. And the importance sampling ratio $\rho^T_t$ is the product of probability ratios for the entire trajectory, where unnecessary terms from later steps significantly increase variance.

#### Flat Partial Returns

Discounting can be thought of as partial termination. The conventional full return $G_t$ can be viewed as a sum of flat partial returns $\bar{G}^h_t = R_{t+1} + \dots + R_h$ without discounting:

$$
G_t = \gamma^{T-t-1} \bar{G}^T_t + (1-\gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \bar{G}^h_t
$$

#### Truncated Importance Sampling

Since the partial return $\bar{G}^h_t$ includes rewards only up to horizon $h$, we scale it using the importance sampling ratio $\rho^h_t$ truncated only up to $h$:

$$
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \left[ \gamma^{T(t)-t-1} \rho^{T(t)}_t \bar{G}^{T(t)}_t + (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho^h_t \bar{G}^h_t \right]}{|\mathcal{T}(s)|}
$$

This is the truncated estimate corresponding to the ordinary importance sampling estimate, and the weighted importance sampling is:

$$
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \left[ \gamma^{T(t)-t-1} \rho^{T(t)}_t \bar{G}^{T(t)}_t + (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho^h_t \bar{G}^h_t \right]}{\sum_{t \in \mathcal{T}(s)} \left[ \gamma^{T(t)-t-1} \rho^{T(t)}_t + (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho^h_t \right]}
$$

This can help significantly reduce variance by removing factors of the importance sampling ratio from actions in later steps.

---

### References

[Original source #1](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf?utm_source=chatgpt.com)
