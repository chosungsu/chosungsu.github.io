---
title: 'Introduction to RL'
date: '2025-03-03'
tags: ['reinforcement learning', 'lecture']
---

### Overview

강화 학습(RL)은 불확실성 하에서 좋은 의사 결정을 내리기 위해 경험/데이터를 통해 학습하는 것입니다. 이는 지능의 필수적인 부분이며, 1950년대 Richard Bellman의 이론에서 발전해왔습니다. 지난 10년간 알파고(AlphaGo), 플라즈마 제어 학습, COVID-19 국경 검사, 그리고 ChatGPT와 같은 인상적인 성공 사례들이 있었습니다.

---

### Key characteristics

<img src="https://velog.velcdn.com/images/devjo/post/364637cc-8f1d-49ef-ab82-21902ca10814/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

1.감독자는 없고 보상 신호만 존재합니다.

2.피드백은 지연되며 즉각적이지 않습니다.

3.순차적이고 독립 항등 분포(i.i.d)가 아닙니다.

4.에이전트의 행동이 다음에 받게 될 데이터에 영향을 미칩니다.

강화학습의 핵심 개념에는 보상(reward) $R_t$가 있으며 이는 스칼라 형태의 피드백 신호로 에이전트가 $t$ 시간에 얼마나 더 잘 수행하는지 나타냅니다. 에이전트의 목표는 누적 보상(cumulative reward)을 최대화하는 것입니다. 강화학습은 보상 가설(reward hypothesis)에 기초하고 있습니다. 다만 보상 해킹(reward hacking)이나 예기치 않은 부작용(unintended side effects)과 같은 문제가 발생할 가능성이 있습니다.

---

### Examples of rewards

$\rightarrow$ 헬리콥터 스턴트 비행

    - 긍정적 보상: 원하는 궤적을 따라 비행할 때

    - 부정적 보상: 추락할 때

$\rightarrow$ 투자 포트폴리오 관리

    - 긍정적 보상: 은행 잔고가 늘어날 때

    - 부정적 보상: 은행 잔고가 감소할 때

$\rightarrow$ 로봇 걷기

    - 긍정적 보상: 앞으로 이동할 때

    - 부정적 보상: 넘어질 때

---

### Agent and Environment

각 단계 $t$에서 에이전트와 환경은 상호작용합니다.

에이전트는 행동 $A_t$를 실행하고 관찰 $O_t$를 받습니다. 이에 따라 스칼라 보상인 $R_t$를 받습니다. 반대로 환경은 행동 $A_t$를 받고 관찰과 보상값을 방출합니다.

---

### History and State

히스토리는 관찰, 행동, 보상의 순차적 기록입니다.

$$
H_t=O_1,R_1,A_1, \cdots, A_{t-1}, O_t, R_t
$$

즉, 시간 $t$까지 관찰 가능한 모든 변수들의 집합이고 상태$(S_t=f(H_t))$는 다음에 무슨 일이 일어날지를 결정하기 위해 사용되는 정보입니다.

환경 상태$(S_t^e)$는 환경의 사적인 표현으로 환경이 다음 관찰/보상을 선택하는데 사용하는 모든 데이터이고 이는 일반적으로 에이전트에게 보이지 않습니다. 그리고 에이전트 상태$(S_t^a)$는 에이전트의 내부 표현으로 다음 행동을 선택하는데 사용하는 모든 정보입니다. 또한 히스토리의 모든 함수$S_t^a=f(H_t)$가 될 수 있습니다.

---

### Information state

정보 상태는 마르코프 상태라고도 불리며 히스토리로부터 모든 유용한 정보를 담고 있습니다. 어떤 상태 $S_t$가 마르코프일 필요충분조건은 다음이 성립하는 것입니다.

$$
P[S_{t+1}|S_t]=P[S_{t+1}|S_1, \dots, S_t]
$$

이는 현재가 주어지면 미래는 과거와 독립이라는 의미입니다. 그리고 상태가 알려지면 히스토리는 지워도 되는 이유가 상태는 미래에 대한 충분 통계량이기 때문입니다.

---

### Full observability vs Partial observability

완전 관측은 에이전트가 환경의 상태를 직접 관찰하는 것으로 $O_t=S_t^a=S_t^e$ 즉, 에이전트 상태와 환경 및 정보 상태의 동일성이 있고 이러한 모델을 마르코프 결정 과정(MDP)라고 합니다.

부분 관측은 에이전트 상태와 환경 상태의 비동일성이 있고 이러한 모델을 부분 관측 마르코프 결정 과정(POMDP)라고 합니다. 예를 들어 카메라 시야만 있는 로봇은 자신의 절대 위치를 알지 못하는 현상이 해당합니다. 에이전트는 상태표현$(S_t^a)$을 구축해야 하고 이는 전체 히스토리와 같으며 beliefs of environment state로서 $S_t^a=(P[S_t^e=s_1], \dots, P[S_t^e=s_n])$으로 정의를 합니다. 그리고 순환 신경망(RNN)을 사용하여 $S_t^a=\sigma(S_{t-1}^aW_s+O_tW_o)$와 같은 식을 통해 새로운 상태를 계산할 수 있어야 합니다.

---

### Inside an RL agent

강화학습 에이전트는 다음 구성 요소 중 하나 이상을 포함할 수 있습니다.

#### 1. 정책(policy)

에이전트의 행동 방식을 정의합니다. 이는 상태에서 행동으로의 사상입니다.

결정론적 정책(deterministic policy)는 $a=\pi(s)$를 통해 주어진 상태에 대해 하나의 행동을 결정합니다.

확률적 정책(stochastic policy)는 $\pi(a|s)=P[A_t=a|S_t=s]$와 같이 주어진 상태에서 가능한 행동들의 확류 분포를 제공합니다.

#### 2. 가치 함수(value function)

미래 보상에 대한 예측으로 상태의 좋고 나쁨을 평가하는데 사용합니다. 정책 $pi$에 따른 상태 $s$의 가치 함수는 현재 상태에서 정책을 따랐을 때 얻게 될 기대 누적보상의 합으로 $v_{\pi}(s)=E_{\pi}[R_{t+1}+\gamma*R_{t+2} + \dots|S_t=s]$와 같이 구할 수 있으며 여기서 $\gamma$는 미래 보상을 현재 가치로 할인해주는 discount factor입니다.

#### 3. 모델(model)

다음 상태를 예측하는 환경은 $P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]$와 같이 구하며 상태 $s$에서 행동 $a$를 했을 때 다음 상태가 $s'$일 확률을 말합니다. 그리고 다음 보상을 예측하는 환경은 $R_s^a=E[R_{t+1}|S_t=s,A_t=a]$와 같이 구하며 이는 상태 $s$에서 행동 $a$를 했을 때 받을 보상의 기댓값을 말합니다.

---

### 참고 자료

[원본 경로 #1](https://youtu.be/WsvFL-LjA6U?si=w6AiGLSlL14bTJ_a)

[원본 경로 #2](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/intro_rl.pdf)

