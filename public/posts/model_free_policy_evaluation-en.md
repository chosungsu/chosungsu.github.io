---
title: 'Model free policy evaluation : without knowing'
date: '2025-03-10'
tags: ['cs234', 'lecture']
---

### policy evaluation

A method to estimate the performance of a specific policy ($\pi$) in situations where dynamics and reward models are unknown. This lecture introduces four methods: Monte Carlo, Temporal Difference, Certainty Equivalence with DP, and Batch Policy Evaluation.

---

### monte carlo

A method that samples returns $G_t$ from episodes generated by following a policy and computes the average to estimate the state value function $V^{\pi}(s)$.

Characteristics include not requiring dynamic/reward models of Markov Decision Process. It does not assume whether states satisfy Markov assumptions.

#### first visit monte carlo

Initialize $N(s)=G(s)=0$ and compute the return value $G_{i, t}=r_{i,t}+\gamma r_{i, t+1}+\gamma^2 r_{i, t+2} + ... +\gamma^{T_i-1}r_{i, T_i}$ at time $t$ in the $i$-th episode.

Update at the earliest time $t$: total visit $N(s)=N(s)+1$, total return $G(s)=G(s) + G_{i, t}$, estimate $V^{\pi}(s)=G(s)/N(s)$.

#### every visit monte carlo

Unlike first visit, it uses the return value after that point every time state $s$ is visited.

The update is the same as above.

#### incremental monte carlo

Update total visit $N(s)=N(s)+1$ identically and update the estimate as $V^{\pi}(s)=V^{\pi}(s) \frac{N(s)-1}{N(s)}+\frac{G_{i,t}}{N(s)}=V^{\pi}(s)+\frac{1}{N(s)}(G_{i, t}-V^{\pi}(s))$. The reciprocal part of total visit can be replaced with $\alpha$, which can be thought of as a learning rate.

For the incremental estimate to converge to the value function, the learning rate must satisfy the following conditions:

- Sum of learning rates is infinite: Learning for a sufficiently long time to achieve convergence
- Sum of squares of learning rates is finite: Learning rate must decrease sufficiently quickly

These two conditions are called Robbins-Monro conditions.

---

### temporal difference learning

Combines elements of Monte Carlo and DP. It works without knowing the model and can be used in both episodic and infinite horizon non-episodic settings.

For 1-step learning ($TD(0)$), the value function is updated as follows.

$$
V^{\pi}(s_t)=V^{\pi}(s_t)+\alpha([r_t+\gamma V^{\pi}(s_{t+1})]-V^{\pi}(s_t))
$$

Here, $r_t+\gamma V^{\pi}(s_{t+1})$ is called the TD target and $[r_t+\gamma V^{\pi}(s_{t+1})]-V^{\pi}(s_t)$ is called the TD error.

Unlike MC, it doesn't need to wait for episode termination and generally has lower variance.

---

### certainty equivalence

A method to estimate transition probabilities ($\hat{P}$) and reward function ($r$) using maximum likelihood estimation in the MDP model.

$$
\hat{P}(s'|s,a)=\frac{1}{N(s,a)}\sum_{k=1}^{i}(s_k=s, a_k=a, s_{k+1}=s'), \\
\hat{r}(s,a)=\frac{1}{N(s,a)}\sum_{k=1}^{i}(s_k=s, a_k=a)
$$

Since MLE must be recalculated every time, the computational cost is very high.

---

### references

[Original source #1](https://youtu.be/jjq51TRNVvk?si=FeiVLMXaoJss02zc)



