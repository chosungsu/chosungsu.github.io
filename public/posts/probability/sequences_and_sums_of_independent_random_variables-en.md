---
title: 'Sequences and sums of independent random variables'
date: '2023-05-15'
tags: ['Probability', 'lecture']
---

### Kolmogorov's Zero-One Law and the Borel-Cantelli Lemma

Let $(\Omega, \mathcal{F}, \mathbf{P})$ be a given probability space. Consider a sequence of sub-$\sigma$-algebras ${\mathcal{G}_n : n \ge 1}$ of $\mathcal{F}$.

By the independence of $\sigma$-algebras, we have

$$
\mathbf{P}\left(G_{i_1} \cap \cdots \cap G_{i_n}\right) = \mathbf{P}(G_{i_1})\cdots \mathbf{P}(G_{i_n})
$$

$\mathcal{T}_n$ is the $\sigma$-algebra generated by the tail part of the sequence $\mathcal{G}_{n+1}, \mathcal{G}_{n+2}, \cdots$.

$$
\mathcal{T} \triangleq \bigcap_{n=1}^\infty \mathcal{T}_n
$$

The tail $\sigma$-algebra $\mathcal{T}$ consists of events that are determined solely by the information encoded in the infinitely distant tail of the sequence. An element $A$ of $\mathcal{T}$ is independent of the finite part of the sequence $\mathcal{F}_n = \sigma(\mathcal{G}_1, \cdots, \mathcal{G}_n)$, and also independent of the entire sequence $\mathcal{F}_\infty = \sigma(\bigcup_{n=1}^\infty \mathcal{F}_n)$.

When viewing $A \in \mathcal{T}$ as an element of $\mathcal{T}$, by the independence condition, we have $\mathbf{P}(A) = \mathbf{P}(A \cap A) = \mathbf{P}(A)\mathbf{P}(A) = \mathbf{P}(A)^2$, so $\mathbf{P}(A)=0$ or $\mathbf{P}(A)=1$, which defines Kolmogorov's zero-one law.

#### The Borel-Cantelli Lemma

The event that $A_n$ occurs infinitely often is denoted by $\limsup_{n\to\infty} A_n \triangleq \bigcap_{n=1}^\infty \bigcup_{m=n}^\infty A_m$.

The event that $A_n$ eventually occurs is denoted by $\liminf_{n\to\infty} A_n \triangleq \bigcup_{n=1}^\infty \bigcap_{m=n}^\infty A_m$.

For a sequence of events ${A_n : n \ge 1}$ on a probability space,

the first Borel-Cantelli lemma states that

$$
\text{If } \sum_{n=1}^\infty \mathbf{P}(A_n) < \infty \quad \text{then } \mathbf{P}\left(\limsup_{n\to\infty} A_n\right) = 0
$$

If the sum of probabilities of events is finite, then only finitely many of those events occur.

The second Borel-Cantelli lemma states that

$$
\text{If } \sum_{n=1}^\infty \mathbf{P}(A_n) = \infty, \quad \text{then } \mathbf{P}\left(\limsup_{n\to\infty} A_n\right) = 1
$$

If the sum of probabilities of independent events is infinite, then infinitely many of those events occur.

#### Random walks

${X_n}$ is an i.i.d. sequence with $\mathbf{P}(X_1 = 1) = p, \mathbf{P}(X_1 = -1) = q = 1-p$, and $S_n = X_1 + \cdots + X_n$.

A random walk is said to be transient if $p \ne 1/2$, in which case $\mathbf{P}(S_n = 0 \text{ i.o.}) = 0$, and recurrent if $p = 1/2$, in which case $\mathbf{P}(S_n = 0 \text{ i.o.}) = 1$.

---

### The Weak Law of Large Numbers

Let ${X_n : n \ge 1}$ be a sequence of pairwise independent and identically distributed random variables with finite mean $m$.

$$
\lim_{n\to\infty} \frac{S_n}{n} = m
$$

When $\mathbf{E}[X_1^2] < \infty$, this is easily proved using Chebyshev's inequality.

---

### Kolmogorov's Two-Series Theorem

A random series $\sum_{n=1}^\infty X_n$ converges almost surely means that $\mathbf{P}\left(\left\{\omega : \sum_{n=1}^\infty X_n(\omega) \text{ converges}\right\}\right) = 1$.

#### Kolmogorov's Maximal Inequality

When $X_1, \cdots, X_n$ are independent with $\mathbf{E}[X_k] = 0, \mathbf{Var}[X_k] < \infty$, for all $\varepsilon > 0$,

$$
\mathbf{P}\left(\max_{1\le k \le n} |S_k| \ge \varepsilon\right) \le \frac{1}{\varepsilon^2} \sum_{k=1}^n \mathbf{Var}[X_k]
$$

#### Kolmogorov's Two-Series Theorem

Let ${X_n : n \ge 1}$ be a sequence of independent random variables with finite second moments. If the following two real series both converge, then the random series $\sum_{n=1}^\infty X_n$ converges almost surely.

$$
\begin{aligned}
&\sum_{n=1}^\infty \mathbf{E}[X_n] < \infty \\
&\sum_{n=1}^\infty \mathbf{Var}[X_n] < \infty
\end{aligned}
$$

---

### The Strong Law of Large Numbers

The strong law of large numbers states that if $\mathbf{E}[|X_1|] < \infty$, then $\lim_{n\to\infty} \frac{S_n}{n} = \mathbf{E}[X_1]$ holds, and if $\mathbf{E}[|X_1|] = \infty$, then $\lim_{n\to\infty} \frac{|S_n|}{n} = \infty$ holds.

Fundamentally, the LLN is related to convergence properties of the form

$$
\frac{1}{a_n} \sum_{j=1}^n x_j \to 0
$$

#### Bernstein's Polynomial Approximation Theorem

The WLLN can be used to prove Bernstein's theorem on approximating continuous functions by polynomials.

Let $f(x)$ be a continuous function on $[0, 1]$ and define the polynomial $p_n(x)$ as follows:

$$
p_n(x) \triangleq \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k} x^k (1 - x)^{n-k}
$$

Then $p_n(x)$ converges uniformly to $f$ on $[0, 1]$ as $n \to \infty$. If $S_n = X_1 + \cdots + X_n$ is the sum of $n$ Bernoulli trials with parameter $x$, then $S_n$ follows a binomial distribution $B(n, x)$.

$$
p_n(x) = \mathbf{E}\left[f\left(\frac{S_n}{n}\right)\right]
$$

By the WLLN, $S_n/n \to \mathbf{E}[X_1] = x$, and since $f$ is continuous, we have $p_n(x) \to f(x)$.

#### Borel's Theorem on Normal Numbers

A real number $x \in (0, 1)$ is said to be simply normal (in base 10) if, for $k = 0, 1, \cdots, 9$, the frequency $\nu_n^{(k)}(x)/n$ of the digit $k$ among the first $n$ decimal places of $x$ satisfies

$$
\lim_{n\to\infty} \frac{\nu_n^{(k)}(x)}{n} = \frac{1}{10}
$$

A point $X \sim U(0, 1)$ chosen uniformly at random from the interval $(0, 1)$ is simply normal with probability 1.

#### CramÃ©r's Theorem

To quantify the exact decay rate of the probability $\mathbf{P}(\bar{S}_n \in B)$, we define the cumulant generating function (CGF) $\Lambda(\lambda)$.

$$
\Lambda(\lambda) \triangleq \log \mathbf{E}[e^{\lambda X_1}]
$$

Using Markov's inequality to optimize the upper bound of $\mathbf{P}(\bar{S}_n \in B)$ with respect to $\lambda$, we obtain a bound of the form

$$
\mathbf{P}\left(\bar{S}_n \in B\right) \le \exp\left(- n \inf_{y\in B} \Lambda^*(y)\right)
$$

Here, $\Lambda^*(x)$ is the Legendre transform of $\Lambda(\lambda)$, which becomes the rate function of the LDP. The Legendre transform of $\Lambda(\lambda)$ is the function defined as

$$
\Lambda^*(x) \triangleq \sup_{\lambda \in \mathbb{R}} \{\lambda x - \Lambda(\lambda)\}
$$

It measures the maximum amount by which the line $\lambda \mapsto x\lambda$ exceeds the function $\Lambda(\lambda)$.

---

### References

[Original source #1](https://researchers.ms.unimelb.edu.au/~xgge@unimelb/Files/Notes/Lecture%20Notes%20on%20Advanced%20Probability.pdf)

[Original source #2](https://minerva.it.manchester.ac.uk/~saralees/statbook2.pdf)
