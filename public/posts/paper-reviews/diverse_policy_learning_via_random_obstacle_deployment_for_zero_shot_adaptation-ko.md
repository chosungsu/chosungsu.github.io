---
title: 'Diverse Policy Learning via Random Obstacle Deployment for Zero-Shot Adaptation'
date: '2025-10-08'
tags: ['embodied ai', 'paper review']
---

### Abstract

본 논문에서는 미지의 동적 장애물이 있는 환경에서 제로샷 정책 적응 (zero-shot policy adaptation)을 가능하게 하는 새로운 강화 학습 프레임워크를 제안합니다.

다양한 행동을 생성할 수 있는 정책을 학습하는 것이 이러한 적응성을 달성하는 데 핵심이라는 아이디어를 채택하여, 무작위 장애물 배치를 통합하여 정책이 다양한 행동을 탐색하고 학습할 수 있도록 하는 새로운 학습 알고리즘을 제안합니다. 이 방법은 다양성 증진을 위해 주로 상호 정보 최대화에 의존하는 기존의 다양한 정책 학습 접근 방식의 한계를 극복합니다.

제로샷 동적 적응을 가능하게 하기 위해, 두 가지 핵심 구성 요소인 상태 의존적 잠재 기술 샘플러 (state-dependent latent skill sampler)와 동작 예측기 (motion predictor)를 포함합니다. 기술 샘플러를 사용하여 각 상태에서 여러 기술 변수를 샘플링한 다음, 동작 예측기를 사용하여 안전하지 않은 기술을 걸러내고, 결과적으로 안전한 기술에 해당하는 행동을 실행합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/0023c75a-8ec3-4757-b0bc-3933eff15861/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### 다양성 정책의 중요성 및 기존 방법의 한계

정책이 더 다양한 행동을 생성할 수 있을수록, 새로운 환경에서 예기치 않은 장애물을 성공적으로 회피하고 충돌을 피하기 위한 적절한 움직임을 선택할 수 있는 가능성이 높아집니다.

LfD는 훈련하기는 더 쉽지만, 정책의 다양성은 시연 데이터의 다양성에 근본적으로 제한되어 데이터 수집 과정이 중요하고 종종 비용이 많이 듭니다. 반면에 RL 방법은 훈련하는 데 더 오래 걸리고 더 어려울 수 있지만, 효과적인 다양성 장려 알고리즘이 사용된다면 광범위한 데이터 없이 다양한 정책을 학습할 잠재력이 있습니다. 본 논문에서는 이러한 효과적인 알고리즘을 개발하는 것을 목표로 후자의 접근 방식에 중점을 둡니다.

이 접근 방식은 정책이 잠재 기술 변수 $z$에 의존하도록 모델링된다는 것입니다. 예를 들어, 정책은 상태 $s$와 기술 $z$에 조건화된 행동의 분포, 즉 $\pi(a|s, z)$로 정의됩니다. 보상을 최대화하는 동안, 이들은 상태 $s$ 또는 상태-행동 쌍 $(s, a)$와 기술 변수 $z$ 사이의 상호 정보 (mutual information)를 최대화하는 것을 목표로 합니다. 그러나 그 결과로 얻어지는 다양성은 원하는 수준에 훨씬 미치지 못하며, 실험에서 입증된 바와 같이 만족스럽지 못한 적응성으로 이어집니다.

또 다른 주요 한계는 동적으로 변화하는 환경에 적응할 수 없다는 것입니다. 이는 에피소드 내에서 잠재 기술 변수 $z$를 고정하여 정책이 새로운 제약 조건에 온라인으로 조정되는 것을 방지하기 때문입니다. 만약 잠재 기술이 에피소드 중간에 변경된다면, 현재 상태는 새로운 잠재 기술에 대한 이상치 (outlier)일 수 있으며, 잠재적으로 저조한 성능으로 이어질 수 있습니다.

#### DIVO 프레임워크 제안

본 논문에서는 기존 방법보다 훨씬 더 다양한 행동을 생성할 수 있는 정책을 학습하는 새로운 강화 학습 프레임워크를 제안합니다. 핵심 아이디어는 존재하지 않는 장애물이 존재하는 것처럼 가장하여 무작위 장애물 배치로 정책을 훈련시키는 것입니다. 예를 들어, 위 이미지에서 실행 가능한 행동 공간을 제한하는 무작위 장애물을 도입합니다. 이 장애물은 정책이 이러한 장애물을 피할 수 있는 다양한 행동 집합을 탐색하고 학습하도록 강제하며, 궁극적으로 광범위한 행동을 생성할 수 있도록 합니다. 다양한 정책을 성공적으로 학습하기 위해서는 무작위 장애물의 분포를 설계하는 데 주의를 기울여야 합니다.

기술 샘플러는 각 상태에 대해 다중 기술 변수를 생성합니다. 조건부 분포 $p(z|s)$가 복잡하기 때문에, 최근의 흐름 기반 신경망 모델을 사용합니다. 그런 다음 동작 예측기는 궤적을 예측하고 새로 접한 장애물을 위반하는 궤적을 식별하여 안전하지 않은 행동을 걸러냅니다.

---

### Related works

#### 1. 비지도 기술 발견 및 상호 정보 기반 접근법

외적 작업 보상 없이 다양한 기술을 식별하는 비지도 기술 발견 (unsupervised skill discovery) 프레임워크입니다. 예를 들어, DIAYN, DADS는 상태 또는 상태-행동 쌍과 기술 변수 사이의 상호 정보 (mutual information)를 최대화하여 다양한 기술을 획득합니다. 하지만 이러한 방법들은 상호 정보의 내재적 한계로 인해 종종 원하는 수준보다 낮은 기술 다양성을 산출합니다.

#### 2. 특징 다양화 및 기술 고정의 한계

최근에는 Cheng et al.이 사족 보행 로봇 네비게이션 작업에서 속도 방향과 같은 궤적의 특징을 다양화하여 다양한 정책을 학습하기 위한 프레임워크를 개발했습니다. 원하는 다양성을 달성하기 위해 신중한 특징 선택과 성공 특징 네트워크와 같은 추가적인 네트워크 훈련에 의존합니다.

본 논문의 접근 방식은 잠재 기술이 에피소드 내에서 변하는 것을 허용합니다. 이는 상태 의존적 잠재 기술 샘플러와 동작 예측기를 통해 동적으로 변화하는 환경에 대한 적응을 가능하게 합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/2773cbe7-5fee-4ae3-ad16-2a9098bf0937/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### Latent Conditioned Policies

다양한 행동 생성을 가능하게 하기 위해, 잠재 변수 정책 모델을 고려합니다. 잠재 기술 공간 $Z$와 잠재 기술 변수 $z \in Z$를 정의합니다. 그리고 상태 $s$와 기술 $z$ 모두에 조건화된 행동의 분포 $\pi(a|s, z)$를 고려합니다. 기술 변수 $z$가 상태 $s$에 의존하는 분포 $p(z|s)$를 가진다고 가정합니다.

기존 연구는 $z$의 분포가 비조건부라고 가정했습니다. 그러나 이 접근 방식은 상태 의존성으로 인해 발생하는 모든 복잡성을 신경망 모델 $\pi(a|s, z)$ 단독으로 처리해야 하므로 정책의 전반적인 표현력을 제한합니다. 따라서 상태 의존적 기술 분포를 채택합니다.

$$
\pi(a|s) = \int_Z \pi(a|s, z)p(z|s)dz
$$

이러한 방식으로 모델을 설계하고 표준 강화 학습 알고리즘을 적용하는 것만으로는 다양한 정책을 학습하기에 충분하지 않습니다.

#### Learning Diverse Policies via Random Obstacle Deployment

<img src="https://velog.velcdn.com/images/devjo/post/a092d1a8-8264-4036-9cb4-9e4d5c0ca9f1/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

각 $z$가 다른 행동 분포로 이어지도록 $\pi(a|s, z)$를 훈련하는 방법을 소개합니다.

첫 번째 단계는 무작위 장애물의 분포를 설계하는 것입니다. 구체적으로, $\omega \in \Omega$로 매개변수화된 부등식 제약 조건 $c_\omega(s) \leq 0$으로 장애물을 정의하고, 초기 상태 $s_0$에 조건화된 매개변수 $\omega$의 분포 $p(\omega|s_0)$를 정의합니다. 그런 다음, 초기 상태 $s_0$를 가진 에피소드의 시작에서 $\omega \sim p(\cdot|s_0)$를 통해 무작위 장애물을 샘플링할 수 있습니다. 훈련 중, 샘플링된 장애물이 주어졌을 때, 상태 $s$가 $c_\omega(s) \leq 0$를 위반하면 에피소드가 종료되고 상당한 음의 보상을 받습니다.

장애물의 분포를 설계할 때 신중하게 고려해야 할 두 가지 핵심 사항이 있습니다.

장애물이 있는 해결책이 장애물이 없는 해결책과 달라야 합니다. 그렇지 않으면, 다른 $\omega$에 대해 동일한 기술이 학습될 것입니다. 예를 들어, $\text{push-T}$ 작업에서 원형 객체는 $\text{T}$-블록의 초기 및 목표 포즈에서 너무 멀리 배치되어서는 안 되며 포즈 사이의 공간을 방해해야 합니다.

초기 포즈 또는 목표 포즈에 너무 가깝게 위치하면 실행 불가능한 해결책이 발생할 수 있습니다.

#### Learning Latent Skill Samplers and Flow Matching

$p(z|s)$를 훈련하기 위해, 훈련된 인코더와 정책을 다음과 같이 사용하여 $(z, s)$ 쌍의 데이터셋을 수집합니다.

조건부 분포 $p(z|s)$는 매우 복잡할 수 있으며 단순한 고전적 확률론적 접근 방식으로 모델링하기 어려울 수 있습니다. 이를 해결하기 위해, 최근 다양한 로봇 학습 환경에서 효과적인 것으로 입증된 연속 정규화 흐름 (continuous normalizing flow) 모델과 흐름 매칭 (flow matching) 알고리즘을 활용합니다.

#### Motion Predictor

행동 입력에 대한 상태 변화를 예측하도록 설계된 동작 예측기 네트워크를 도입합니다. 이 맥락에서 '동작 ($\text{motion}$)'은 시간에 따른 상태의 궤적을 나타냅니다.

현재 상태를 $s_t$, 현재 행동을 $a_t$라고 하고, 다음 상태를 $s_{t+1}$로 나타냅니다. 논문의 접근 방식에서 동작은 $s_t$에서 $s_{t+1}$까지의 상태 시퀀스로 정의되며, $s_{t:t+1}$로 표시됩니다. 예를 들어, 시퀀스 길이 $H$의 경우 $s_{t:t+1} := \{s_{t + \frac{k}{H-1}}\}^{H-1}_{k=0} \in S^H$ 입니다. 상태를 $s_t$ 대신 $s$ (시간 $t$ 없이)로 표기할 때, $s_{t:t+1}$를 $\text{s}_{\text{traj}}$로 나타냅니다.

동작 예측기가 환경의 역학을 효과적으로 학습한다는 점을 고려할 때, 모델 예측 제어 (MPC)와 같은 궤적 최적화 기반 기술이 장애물을 피하면서 작업을 해결할 수 있으며, 잠재적으로 정책 학습의 필요성을 무효화할 수 있다고 주장할 수 있습니다. 그러나 이러한 방법들은 특히 복잡하고 고차원적인 환경에서 높은 계산 비용이라는 중대한 도전에 직면합니다.

#### Dynamic Skill Adjustment

<img src="https://velog.velcdn.com/images/devjo/post/30382099-e947-44f2-ace2-88d012637759/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

기존 방법은 초기 상태 $s_0$와 새로운 주어진 제약 $c_{\text{new}}(s) \leq 0$을 기반으로 기술 변수를 선택하며, 에피소드 내에서 이를 변경하지 않습니다. 따라서 제약 조건이 동적으로 변하는 환경에서는 적용하기 어렵습니다. 이와 대조적으로, 본 논문에서는 다음 절차를 통해 현재 상태에 따라 기술을 다시 샘플링할 수 있습니다.

$\Rightarrow$ $v_\alpha$를 사용하여 다중 기술을 샘플링합니다.

$\Rightarrow$ $\pi_\phi$를 사용하여 다중 행동을 계산합니다.

$\Rightarrow$ $m_\beta$를 사용하여 동작을 예측합니다.

$\Rightarrow$ 예측된 동작이 **현재 제약 $c_{\text{new}}$**를 만족하는지 확인합니다.

$\Rightarrow$ 안전한 행동 후보를 얻습니다.

이를 위해, 각 행동의 예측된 동작 $\text{s}_{\text{traj}}$을 기반으로 선호도 수준을 출력하는 점수 함수 (SCORE)를 설계합니다.

---

### Conclusion

본 논문에서 모든 모델 프리 액터-크리틱 강화 학습 알고리즘과 함께 사용할 수 있는 새로운 다양성 정책 학습 프레임워크를 제안했습니다.

훈련 중 무작위 장애물 배치를 도입함으로써, 우리는 정책 탐색과 다양한 행동 학습을 장려했습니다. 복잡한 조건부 분포로부터 기술 변수를 생성하는 잠재 기술 샘플러와, 행동이 환경 제약 조건을 만족하는지 예측하는 동작 예측기가 포함됩니다.

기존 방법에 비해 훨씬 더 다양한 행동을 생성하는 정책을 학습할 수 있지만, 보 논문에서 접근 방식의 행동 다양성은 무작위 장애물 분포의 선택에 크게 의존합니다. 본 연구에서 이 분포는 각 작업에 대해 수동으로 설계되었으며, 어느 정도 직관적으로 설계될 수 있지만, 이 접근 방식이 임의의 작업에 적합하다는 보장은 없습니다.

---

### 참고 자료

[원본 경로 #1](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10847909)
