---
title: 'MODEL-BASED OFFLINE REINFORCEMENT LEARNING WITH LOWER EXPECTILE Q-LEARNING'
date: '2025-03-04'
tags: ['embodied ai', 'paper review']
---

### Abstract

Model-based offline reinforcement learning is an attractive approach to addressing the challenge of learning from limited and static data by using a learned model to generate fictitious trajectories. However, such approaches often suffer from inaccurate value estimation arising from model rollouts.

In this paper, the authors introduce LEQ (Lower Expectile Q-learning), a new model-based offline RL method that provides low-bias model-based value estimates via lower-expectile regression of $\\lambda$-returns.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/5eb3d37f-cb1e-426e-bd84-d76ac836c8cb/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:100;" />

One of the main challenges in offline reinforcement learning is overestimation of the value of out-of-distribution (OOD) actions, caused by the lack of direct interaction with the environment.

Model-based offline RL tackles this problem by generating additional training data using a learned dynamics model, thereby augmenting the given offline dataset with synthetic experiences that cover OOD states and actions. While this approach has shown strong performance on relatively simple, short-horizon tasks, it struggles in long-horizon tasks due to noisy model predictions and inaccurate value estimates.

Typical model-based offline RL methods mitigate inaccurate value estimation (mostly overestimation) by penalizing the $\\text{Q}$ values estimated from model rollouts using uncertainty measures over model predictions or value predictions. Although such penalty terms prevent the policy from exploiting erroneous value estimates, the policy is now trained to maximize a penalized value that depends on a heuristically estimated uncertainty, rather than the true value function, which can lead to suboptimal behavior.

---

### Methods

#### Problem formulation

We formulate the problem as a Markov decision process (MDP) defined by the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, r, p, \\rho, \\gamma)$.

$\\rho(\\mathbf{s}_0) \\in \\Delta(\\mathcal{S})$ denotes the initial state distribution and $\\gamma$ is the discount factor. The goal of RL is to find a policy $\\pi: \\mathcal{S} \\to \\Delta(\\mathcal{A})$ that maximizes the expected return

$$
\\mathbb{E}_{\\mathcal{T} \\sim p(\\cdot \\mid \\pi, \\mathbf{s}_0 \\sim \\rho)} \\left[ \\sum_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_t, \\mathbf{a}_t) \\right]
$$

over transition sequences $\\mathcal{T} = (\\mathbf{s}_0, \\mathbf{a}_0, r_0, \\mathbf{s}_1, \\mathbf{a}_1, r_1, \\ldots, \\mathbf{s}_T)$ of finite horizon $T$, where the trajectory is generated by starting from $\\mathbf{s}_0 \\sim \\rho(\\cdot)$, taking actions according to $\\pi(\\mathbf{a}_t \\mid \\mathbf{s}_t)$, and transitioning according to $p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t)$.

#### LOWER EXPECTILE Q-LEARNING

Most offline RL algorithms focus on learning conservative value functions for out-of-distribution actions. In this paper, the authors propose LEQ, which learns a conservative $\\text{Q}$-function via lower-expectile regression, thereby avoiding reliance on potentially unreliable uncertainty estimates and avoiding over-confident $\\text{Q}$ estimates.

The target value for $\\text{Q}_{\\phi}(\\mathbf{s}, \\mathbf{a})$ (with $\\mathbf{a} \\leftarrow \\pi_{\\theta}(\\mathbf{s})$) can be estimated by rolling out an ensemble of world models and averaging $r + \\gamma \\text{Q}_{\\phi}(\\mathbf{s}', \\pi_{\\theta}(\\mathbf{s}'))$ over all possible $\\mathbf{s}'$:

$$
\\hat{y}_{\\text{model}} = \\mathbb{E}_{\\psi \\sim \\{\\psi_1, \\ldots, \\psi_M\\}} \\mathbb{E}_{(\\mathbf{s}', r) \\sim p_{\\psi}(\\cdot \\mid \\mathbf{s}, \\mathbf{a})} [r + \\gamma \\text{Q}_{\\phi}(\\mathbf{s}', \\pi_{\\theta}(\\mathbf{s}'))].
$$

This target value suffers from three sources of error. To reduce overestimation of $\\hat{y}_{\\text{model}}$ caused by inaccurate $H$-step model rollouts, the authors propose to use lower-expectile regression with a small expectile level $\\tau$ when fitting target $\\text{Q}$ values:

$$
\\mathcal{L}_{\\text{Q,model}}(\\phi) = \\mathbb{E}_{\\mathbf{s}_0 \\in \\mathcal{D}_{\\text{model}}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\left[ \\frac{1}{H} \\sum_{t=0}^{H} \\mathcal{L}_{\\tau}^2 \\left( \\text{Q}_{\\phi}(\\mathbf{s}_t, \\pi_{\\theta}(\\mathbf{s}_t)) - \\hat{y}_{\\text{model}} \\right) \\right].
$$

In addition, the $\\text{Q}$-function is trained on transitions from the dataset $\\mathcal{D}_{\\text{env}}$ using the standard Bellman update, where there is no risk of model-induced overestimation:

$$
\\mathcal{L}_{\\text{Q,env}}(\\phi) = \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s}') \\in \\mathcal{D}_{\\text{env}}} \\left[ \\frac{1}{2} (\\text{Q}_{\\phi}(\\mathbf{s}, \\mathbf{a}) - \\hat{y}_{\\text{env}})^2 \\right].
$$

To stabilize training of the $\\text{Q}$-function, the method adopts EMA regularization, which minimizes the difference between the predictions of $\\text{Q}_{\\phi}$ and those of its exponential moving average $\\overline{\\text{Q}}_{\\phi}$, thereby preventing drastic changes in $\\text{Q}$ values:

$$
\\mathcal{L}_{\\text{Q,EMA}}(\\phi) = \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{D}_{\\text{env}}} (\\text{Q}_{\\phi}(\\mathbf{s}, \\mathbf{a}) - \\overline{\\text{Q}}_{\\phi}(\\mathbf{s}, \\mathbf{a}))^2.
$$

Finally, these three losses are combined to define the critic loss:

$$
\\mathcal{L}_{\\text{Q}}(\\phi) = \\beta \\mathcal{L}_{\\text{Q,model}}(\\phi) + (1 - \\beta) \\mathcal{L}_{\\text{Q,env}}(\\phi) + \\omega_{\\text{EMA}} \\mathcal{L}_{\\text{Q,EMA}}(\\phi).
$$

#### $\\lambda$-return

For $\\text{Q}$-learning, LEQ uses $\\lambda$-returns instead of 1-step returns. $\\lambda$-returns allow the $\\text{Q}$-function and policy to learn from low-bias multi-step returns.

Using the $N$-step return

$$
G_{t:t+N}(\\mathcal{T}) = \\sum_{i=0}^{N-1} \\gamma^i r_{t+i} + \\gamma^N \\text{Q}_{\\phi}(\\mathbf{s}_{t+N}, \\mathbf{a}_{t+N}),
$$

the $\\lambda$-return $\\text{Q}^{\\lambda}_t(\\mathcal{T})$ for an $H$-step trajectory $\\mathcal{T}$ at time step $t$ is defined as

$$
\\text{Q}^{\\lambda}_t(\\mathcal{T}) = \\frac{1-\\lambda}{1-\\lambda^{H-t-1}} \\sum_{i=1}^{H-t} \\lambda^{i-1} G_{t:t+i}(\\mathcal{T}).
$$

---

### Conclusion

Following prior work on model-based offline RL (Sun et al., 2023; Jeong et al., 2023), the method assumes access to the ground-truth termination function of the task, unlike online model-based RL approaches that learn a termination function from interactions.

When a learned termination function is used instead, performance degrades substantially, especially on diverse datasets where termination signals are scarce because data are collected by moving toward randomly selected goals.

This paper proposes LEQ, a new offline model-based reinforcement learning method that uses expectile regression to obtain a conservative evaluation of a policy from model-generated trajectories. Expectile regression alleviates the difficulty of modeling the full distribution of $\\text{Q}$ targets and enables learning a conservative $\\text{Q}$-function via sampling. Combined with $\\lambda$-returns for both critic and policy updates during fictitious rollouts, the policy receives learning signals that are more robust to both model error and critic error.

---

### References

[Original source #1](https://arxiv.org/pdf/2407.00699)
