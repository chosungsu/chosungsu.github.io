---
title: 'DREAMSMOOTH: IMPROVING MODEL-BASED REINFORCEMENT LEARNING VIA REWARD SMOOTHING'
date: '2024-09-30'
tags: ['embodied ai', 'paper review']
---

### Abstract

모델 기반 강화 학습(MBRL)은 예측된 보상으로 가상 궤적을 생성하여 행동을 계획함으로써 표본 효율적 (sample-efficient)인 방식으로 복잡한 행동을 학습하는 능력으로 많은 관심을 받았습니다.

그 성공에도 불구하고, 우리는 놀랍게도 보상 예측이 종종 MBRL의 병목 현상이 된다는 것을 발견했는데, 특히 예측하기 어렵거나 희소 보상 (sparse rewards)의 경우에 더욱 그렇습니다.

인간이 거친 보상 추정치로부터 학습할 수 있다는 직관에 동기를 받아, 주어진 타임스텝에서 정확한 보상 대신 시간적으로 평활화된 보상을 예측하도록 학습하는 단순하지만 효과적인 보상 평활화 접근 방식인 DreamSmooth를 제안합니다.

---

### Introduction

인간은 정확한 순간의 정확한 보상 대신 미래 보상의 대략적인 추정치를 가지고 행동을 계획하는 경우가 많습니다. 대략적인 보상 추정치는 대부분 작업을 학습하는 데 충분하며, 정확한 보상을 예측하는 것은 종종 어렵습니다. 이는 보상이 모호하거나, 지연되거나, 관찰할 수 없기 때문입니다.

예를 들어 테이블 위의 블록을 빈으로 밀어 넣는 조작 작업을 생각해 보면 블록이 처음 빈에 닿는 타임스텝에서만 큰 보상이 주어집니다. 에이전트와 동일한 이미지 관찰을 사용하면 인간에게도 올바른 보상 시퀀스를 예측하는 것이 어렵습니다.

너무 높은 보상 추정치는 에이전트가 실제로 성능이 낮은 행동을 선택하게 하고, 너무 낮은 추정치는 에이전트가 높은 보상을 무시하도록 이끌 것입니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/36acb3f8-9d62-47f7-880e-a10e0c9dcb4f/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### BACKGROUND

문제를 부분적으로 관찰 가능한 마르코프 결정 과정 (POMDP, Partially Observable Markov Decision Process)으로 공식화하며, 이것은 튜플 $(\mathcal{O}, \mathcal{A}, P, R, \gamma)$로 정의됩니다.

강화 학습은 예상되는 보상 합 $\mathbb{E}_{\pi}[\sum_{t=1}^T \gamma^{t-1} r_t]$을 최대화하는 정책을 찾는 것을 목표로 합니다.

본 논문은 에이전트 경험으로부터 월드 모델 $P_{\theta}(z_{t+1}|z_t, a_t)$와 보상 모델 $R_{\theta}(r_t|z_t)$를 학습하는 MBRL 알고리즘에 중점을 둡니다. 여기서 $z_t$는 타임스텝 $t$에서의 학습된 잠재 상태 latent state입니다.

#### REWARD PREDICTION

보상 예측은 많은 환경에서 놀라울 만큼 어렵습니다. 보상 모델 훈련에 일반적으로 사용되는 평균 제곱 오차 (MSE)손실 $\mathbb{E}_{(z, r) \sim \mathcal{D}} \left[ (R_{\theta}(z) - r)^2 \right]$가 희소 보상이 존재할 때 보상 예측 정확도를 떨어뜨릴 것이라고 가설을 세웁니다. 이는 희소 보상을 단 한 스텝 일찍 또는 늦게 예측하는 것이 모든 스텝에서 단순히 $0$ 보상을 예측하는 것보다 더 높은 손실을 초래하기 때문입니다.

더 중요하게는, 이러한 저조한 보상 예측이 정책 학습의 병목 현상이 될 수 있습니다.

#### Dreamsmooth

보상 예측 문제를 해결하기 위해, 시간적 평활화를 수행하여 모델이 정확한 타임스텝에서 희소 보상을 예측해야 하는 요구 사항을 완화하는 단순하지만 효과적인 솔루션인 DreamSmooth를 제안합니다. 보상 모델이 몇 타임스텝 정도 실측 보상에서 벗어난 보상을 예측할 수 있도록 허용하면 학습이 더 쉬워지며, 특히 보상이 모호하거나 희소할 때 그렇습니다.

$$
\tilde{r}_t \leftarrow f(r_{t-L:t+L}) = \sum_{i=-L}^{L} f_i \cdot r_{\text{clip}(t+i, 0, T)}
$$

여기서 $T$와 $L$은 각각 에피소드 및 평활화 지평 smoothing horizons을 나타냅니다. 평활화된 보상을 가진 에피소드는 재생 버퍼에 저장되고 보상 모델을 훈련하는 데 사용됩니다. 에이전트는 원래 보상을 전혀 보지 않고 평활화된 보상으로부터만 학습합니다. 평활화된 보상은 모델이 큰 손실을 발생시키지 않고 몇 타임스텝 더 일찍 또는 더 늦게 보상을 예측할 수 있도록 허용함으로써 보상 예측을 용이하게 합니다.

---

### Conclusion

본 논문에서, MBRL에서의 보상 예측 문제를 식별하고 단순하지만 효과적인 해결책, 즉 보상 평활화 (reward smoothing)를 제공합니다.

DreamSmooth는 주로 환경의 부분 관찰 가능성 또는 확률성으로 인해 보상 예측이 쉽지 않은 희소 보상 작업에서 우수한 성능을 보여줍니다. 하지만 보상 예측의 어려움을 완화한다는 것을 보여주었던 일부 실험도 있지만 개선된 보상 예측이 항상 작업 성능을 향상시키지는 않습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2311.01450)
