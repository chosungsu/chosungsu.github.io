---
title: 'ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation'
date: '2025-05-31'
tags: ['robotics', 'paper review']
---

### Abstract

투명한 물체는 일상생활에서 흔히 접할 수 있는 부분이지만, 고유한 시각적 특성을 가지고 있어서 표준 3차원(3D) 센서가 정확한 깊이 추정(depth estimates)을 하는 것을 매우 어렵게 만듭니다. 대부분의 경우, 투명한 물체는 종종 그 뒤에 있는 표면의 노이즈가 많거나 왜곡된 근사치로 나타납니다. 이러한 어려움을 해결하기 위해 로봇 조작을 위한 단일 RGB-D 이미지로부터 투명한 물체의 정확한 3차원 기하학적 구조를 추정하는 ClearGrasp라는 딥러닝 접근 방식을 제시합니다.

심층 합성곱 신경망을 사용하여 표면 법선(surface normals), 투명 표면 마스크, 그리고 가려짐 경계(occlusion boundaries)를 추론합니다. 그런 다음 이 출력들을 활용하여 장면 내 모든 투명 표면에 대한 초기 깊이 추정치를 개선합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/3ba33461-3160-4ee2-8a66-5c3da6403f15/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

투명 물체는 안경부터 플라스틱 병에 이르기까지 일상생활의 흔한 부분이지만, 기계가 인식하고 조작하기 어렵게 만드는 독특한 시각적 속성을 가지고 있습니다. 특히, 투명한 재료(굴절성(refractive)과 반사성(specular)을 모두 가짐)는 고전적인 스테레오 비전 알고리즘에서 가정하는 기하학적 광 경로 가정을 따르지 않습니다. 이로 인해 표준 3D 센서가 투명 물체에 대해 정확한 깊이 추정치를 생성하는 것이 어려워지며, 투명 물체는 종종 그 뒤에 놓인 표면을 노이즈가 많거나 왜곡된 형태로 근사하여 나타냅니다. 따라서 RGB-D 이미지나 포인트 클라우드와 같은 3D 데이터를 사용하여 물체를 로봇으로 조작하는 데 상당한 연구가 집중되었지만 이러한 알고리즘 중 다수는 투명 물체에 즉시 적용될 수 없습니다. 그럼에도 불구하고 투명 물체는 식기 세척이나 플라스틱 용기 분류/청소와 같은 응용 분야에서 매우 중요하게 남아 있습니다.

본 연구에서는 ClearGrasp라는 알고리즘을 제시합니다. 이 알고리즘은 합성 훈련 데이터(synthetic training data)를 사용하여 딥러닝을 활용하고, 로봇 조작을 위한 투명 물체의 정확한 3D 기하학을 추론합니다.

상용 RGB-D 카메라는 일반적인 불투명 표면에 대해 종종 우수한 깊이 추정치를 제공합니다. 따라서 모든 기하학적 정보를 처음부터 직접 추정하기보다는, RGB-D 카메라의 초기 깊이 추정치를 수정하는 것이 더 실용적이라고 본 연구에서 추론합니다. 투명 물체에 나타나는 굴절 및 반사 패턴은 절대적인 깊이보다는 물체의 곡률 (예: 표면 법선)에 대한 더 강력한 시각적 단서를 제공합니다. 투명 물체에 대한 실제 환경의 진실값(ground truth) 3D 훈련 데이터를 얻는 것은 어렵지만 도메인 무작위화를 사용한 고품질의 렌더링된 합성 이미지를 훈련 데이터로 사용하여 실제 환경 데이터에서 합리적인 결과를 얻는 것이 가능함을 보여줍니다. 흥미롭게도 실제 환경의 범위 외(out-of-domain) 데이터와 혼합함으로써 실제 환경 이미지와 훈련 중에 보지 못한 새로운 투명 물체 모두에 대해 더 잘 일반화할 수 있음을 발견했습니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/0cf22f36-6472-41ad-9a12-2a141a76d794/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

딥 컨볼루션 네트워크를 사용하여 컬러 이미지로부터 표면 법선 (surface normals), 투명 표면 마스크, 그리고 가려짐 경계 (occlusion boundaries)를 포함한 정보를 추론합니다. 이 정보와 초기 깊이 이미지를 전역 최적화 (global optimization)에 입력으로 사용합니다.

#### Estimating 3D Geometry of Transparent Object

Zhang와 Funkhouser가 제안한 depth completion pipeline을 채택하되, 투명 물체가 제시하는 고유한 문제를 해결하기 위해 몇 가지 중요한 수정을 가했습니다. 누락된 깊이 영역만 채우는 대신, 투명 표면에 대한 픽셀 단위 마스크를 예측하는 추가 네트워크를 훈련하고 이를 사용하여 깊이 카메라에서 얻은 신뢰할 수 없는 깊이 측정값을 제거합니다. 가려짐 경계만 예측하는 대신, 가려짐 경계와 접촉 경계 모두를 예측하도록 제안합니다. 이는 네트워크가 다른 유형의 경계를 구별하고 더 정확한 깊이 불연속성 경계를 예측할 수 있게 하며, 이는 전역 최적화 단계에 중요합니다.

최적화 알고리즘은 제거된 깊이를 채우는 데 예측된 법선을 사용하여 재구성의 모양을 안내하고, 가려짐 경계로 표시된 깊이 불연속성을 관찰합니다.

$$
E = \lambda_D E_D + \lambda_S E_S + \lambda_N E_N B
$$

여기서 $E_D$는 추정된 깊이와 관찰된 원시 깊이 사이의 거리를 측정합니다. $E_S$는 이웃 픽셀의 깊이 차이를 측정합니다. $E_N$은 추정된 깊이와 예측된 표면 법선 간의 일관성을 측정합니다.

#### Synthetic Training data generation

Blender의 물리 엔진과 물리 기반 레이 트레이싱 Blender Cycles 렌더링 엔진을 사용하는 Synthesis AI 플랫폼을 선택하여 합성 데이터를 생성했습니다. 이 플랫폼은 고도로 구성 가능하며, 다중 표면을 통한 굴절 및 반사와 부드러운 그림자와 같은 투명 물체에 중요한 효과를 시뮬레이션할 수 있기 때문에 선택되었습니다. 데이터셋은 실제 투명 플라스틱 물체를 모델로 한 9개의 CAD 모델로 구성되며, 이 중 4개는 알고리즘의 일반화 능력을 테스트하기 위해 훈련 중에 제외되었습니다. 추가적으로, 하나의 회색 토트 상자가 배경 물체로 사용됩니다.

#### Grasp planning

ClearGrasp를 로봇 피킹 시스템에 통합함으로써 최첨단 파지 알고리즘을 적용했습니다. 팽창 컨볼루션 (dilated convolutions)과 ReLU 활성화를 가진 18개 레이어 완전 컨볼루션 잔차 네트워크를 사용하며 최대 풀링과 2개 레이어의 공간 쌍선형 2배 업샘플링이 교차되어 있습니다. 네트워크는 4채널 이미지를 입력으로 받습니다.

---

### Conclusion

합성 훈련 데이터와 다중 센서 양식 (색상 및 깊이)을 딥러닝과 함께 활용하여 조작을 위한 투명 물체의 정확한 3D 기하학을 추론하는 알고리즘인 ClearGrasp를 제시합니다. 가능한 향후 연구 방향은 다음과 같습니다.

추론 단계에서 조명 정보를 명시적으로 활용하여 다른 조명 조건 하에서 알고리즘의 정확도를 개선합니다. 정확한 가려짐 및 접촉 경계를 예측하는 것이 더 어려운 복잡한 환경에서의 알고리즘 강건성을 개선합니다. 또한 그림자에 강건하게 만듭니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/1910.02550)
