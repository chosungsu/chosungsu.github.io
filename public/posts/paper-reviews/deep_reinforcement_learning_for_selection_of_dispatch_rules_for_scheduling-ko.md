---
title: 'Deep Reinforcement Learning for Selection of Dispatch Rules for Scheduling of Production Systems'
date: '2025-01-14'
tags: ['robotics', 'paper review']
---

### Abstract

생산 스케줄링은 제조 시스템 관리에서 매우 중요한 작업입니다. 문제의 복잡성으로 인해 최적의 스케줄을 도출하기 어렵고, 계산 비용이 많이 들고 시간 소모적인 솔루션은 기업이 고객 요구를 충족하는 데 큰 문제를 야기했습니다. 단순한 디스패칭 규칙은 일반적으로 제조 현장에서 적용되어 왔으며, 특히 중소기업(SME)에게는 좋은 스케줄링 옵션으로 사용됩니다. 그러나 최근 인공지능(AI) 및 기계 학습(ML) 솔루션으로 가능해진 스마트 시스템의 발전은 스케줄링 접근 방식을 혁신했습니다.

다양한 생산 환경에서는 하나의 디스패칭 규칙이 다른 규칙보다 더 나은 성능을 보일 수 있으며, 어떤 규칙을 선택할지는 전문가의 지식이 필요합니다. 본 연구의 목표는 단기 생산 스케줄링을 지원하기 위해 딥 강화 학습(DRL) 에이전트 모델링 및 배포를 위한 프레임워크를 설계하고 구현하는 것입니다. DRL 에이전트는 제조 자원에 작업을 할당하기 위한 디스패칭 규칙을 선택합니다. 이 모델은 자전거 생산 산업의 파일럿 사례를 시뮬레이션하는 이산 사건 시뮬레이션(DES) 모델을 사용하여 훈련, 테스트 및 평가됩니다. DRL 에이전트는 최상의 디스패칭 정책을 학습하여 최고의 생산 완료 시간(makespan)을 가진 스케줄을 생성할 수 있습니다.

---

### Introduction

인더스트리 4.0 시대에 제조 기술은 디지털화와 지능형 자동화를 수용하며 상호 연결된 스마트 생산의 새로운 시대로 혁명적인 변화를 겪고 있습니다. 인더스트리 4.0이라 불리는 새로운 제조 혁명에서 제조 시스템을 구성하는 모든 시스템의 디지털화는 스마트 제조를 향한 핵심 목표입니다. 정보 기술은 데이터 수집, 데이터 관리, 지식 개발 및 복잡한 생산 시스템 스케줄링과 같은 정교한 작업을 포함한 다양한 응용 분야에서 사용됩니다.

스케줄링은 고객 주문을 생산 능력과 연결하여 고품질 제품의 효율적인 생산을 실현하는 데 중요합니다. 여기에는 자원 제약, 작업 특성 및 제조 환경의 동적 특성과 같은 복잡성을 고려하여 사용 가능한 자원으로 작업 처리를 위한 최적의 순서와 타이밍을 결정하는 것이 포함되며, 최적화 방법과 인공지능(AI)과 같은 고급 기술을 활용합니다. 특히, 유전 알고리즘 기반 스케줄링 접근 방식은 작업 순서의 반복적인 최적화를 통해 제조 작업장의 적응성을 향상시키는 것으로 나타났습니다.

본 연구는 작업장 문제(job shop problems)에 대한 딥 강화 학습(DRL) 에이전트를 사용하여 생산 스케줄링을 최적화하는 데 중점을 둡니다. DES(이산 사건 시뮬레이션) 모델 내에서 훈련 및 테스트되며, 특히 자전거 생산 산업의 파일럿 사례를 시뮬레이션하도록 설계된 DRL 에이전트는 최상의 생산 완료 시간(makespan)을 달성하기 위해 디스패칭 규칙을 동적으로 선택합니다.

---

### Literature review

제조업에서 스케줄링은 생산 시스템의 작업과 작업량을 배열, 제어 및 최적화하는 과정을 의미합니다. 근본적인 목표는 생산 비용과 리드 타임을 최소화하고 품질 기준을 달성하는 방식으로 제조 활동을 배열하는 것입니다. 산업 현장에서 디스패칭 규칙(dispatch rules)은 구현이 간단하고 허용 가능한 솔루션을 신속하게 제공할 수 있으므로 스케줄링에 사용되는 접근 방식 중 하나입니다.

Chryssolouris는 탱크 팜, 재고 및 증류 공정을 동시에 관리하는 정유소 단기 스케줄링을 위한 통합 시뮬레이션 기반 접근 방식이 소개되었습니다. 상호 연결된 작업의 전체적인 모델링을 통해 최적화된 생산 순서 지정, 최소화된 가동 중단 시간 및 전반적인 정유소 성능 향상을 달성합니다.

Wei는 다중 목표 유전 알고리즘이 도입되어 에너지 효율적인 작업장 스케줄링을 달성했습니다. 여러 기준을 동시에 최적화함으로써 에너지 소비를 줄이고 생산 시간을 단축하여 전반적인 운영 지속 가능성을 향상시켰습니다.

실제로 대부분의 경우, 새로운 작업 또는 기계 고장과 같은 예상치 못한 사건으로 인해 초기 스케줄에서 편차가 발생합니다. 이러한 경우, 재스케줄링 메커니즘이 필요합니다. Yan은 재진입 생산 라인에서 조기 완료 및 지연을 최소화하기 위한 2계층 동적 스케줄링 방법을 제안하였고 의사 결정 프로세스를 계층화하여 복잡한 생산 흐름을 더 잘 처리함으로써 향상된 스케줄 품질이 달성되었습니다.

Sutton과 Barto는 RL의 핵심 문제는 이전에 적용된 스케줄에 대한 사전 지식이 필요 없다는 것이고 단지 이 알고리즘은 에이전트가 보상을 최대화하는 최적 또는 거의 최적의 정책을 학습하는 것으로 정의하였습니다. Zhou 등은 다중 목표 학습을 위한 스케줄링을 가능하게 하기 위해 여러 훈련 세션 및 복합 보상 함수의 공식화를 통해 개발된 자기 조직화 및 자기 학습 기능을 갖춘 AI 스케줄러를 제안했습니다. 자기 학습의 결과로, 에이전트는 재구성 작업을 최소화하면서 할당된 주문 목록을 완료함으로써 기존의 선입선출(FIFO) 디스패칭 규칙보다 우수한 성능을 보였습니다.

강화 학습과 디스패칭 규칙을 결합하여, Q-학습을 사용하는 단일 기계 에이전트가 Wang과 Usher에 의해 제안되었습니다. 상태-행동 가치 함수의 근사를 위해 딥 Q-네트워크(DQN)에 연속 상태 특징을 입력으로 고려하는 변형도 연구되었습니다. 에이전트는 새로운 작업이 도착하고 기계에 할당되어야 할 때마다 디스패칭 규칙을 제안합니다.

대부분의 접근 방식은 작업 할당을 위해 단일 기계를 고려하거나 제한된 성능을 가진 단일 디스패칭 규칙을 사용하여 여러 기계의 작업 할당을 고려합니다. 더 복잡하고 실제적인 생산 시스템의 스케줄링을 위해 Lin 등은 디스패칭 규칙을 사용하여 작업장 문제를 해결하기 위해 다중 클래스 DQN(MDQN)을 제안합니다. 따라서 고전적인 DQN과 달리 저자가 제안한 MDQN은 각 그룹에 대해 하나씩 여러 출력 노드를 선택합니다. 훈련 후, 에이전트는 각 기계에 대한 최상의 디스패칭 규칙을 선택하는 방법을 학습합니다. 그러나 이 구현은 한 번에 하나의 주문만 스케줄링하는 것을 의미하며, 대규모 문제에 대한 성능 조사가 필요합니다.

---

### Method

#### 1. 스케줄링 문제 공식화

연구 중인 생산 스케줄링 문제는 사전에 정의된 목표에 따라 최적의 계획을 생성하는 순서로 작업을 기계에 할당하는 것을 의미합니다. 본 연구에서 스케줄링 알고리즘의 목표는 생산 완료 시간(makespan) 최소화가 될 것입니다. 스케줄링 문제 모델을 위해 생산 시스템의 매개변수를 정의해야 합니다. 또한, 스케줄링 매개변수 및 변수는 아래와 같이 정의됩니다.

제품 유형은 $P$, 공정 계획은 $H$, 자원은 $M$, 처리시간은 $I(M_r, Pr_{ki}) \ge 0$, 과업은 $T_b$, 작업은 $J_c$, 주문은 $O_g$로 표기합니다. 제약 조건으로는 각 기계는 한 번에 최대 하나의 공정만 수행하여야 하며 특정 작업의 과업이 다른 작업장에서 처리될 수 없음을 보장해야 합니다.

#### 2. 강화 학습 접근 방식

스케줄링 문제에 RL 접근 방식을 적용하려면 스케줄링 관점에서 주요 개념을 정의해야 합니다. 다음 하위 섹션에서는 MDP, 학습 알고리즘, 환경, 상태, 행동 및 보상이 정의됩니다.

본 연구에서 제안된 RL 알고리즘은 Q-학습에 기반을 두고 있으며, 이는 심층 학습 기술(특히 신경망)을 사용하여 구현됩니다. 이 접근 방식을 일반적으로 딥 강화 학습(DRL)이라고 합니다. 다음 섹션에서는 Q-학습 방법과 심층 신경망을 통합할 때의 변형을 설명합니다.

Q-학습은 AI 및 ML 분야의 기본 강화 학습 알고리즘으로, 에이전트가 환경과 상호 작용하여 순차적 의사 결정 작업을 위한 최적 정책을 학습하는 문제를 해결하는 데 사용됩니다. 강화 학습에서 에이전트는 환경에서 일련의 결정(행동)을 수행하여 누적 보상을 최대화하는 것을 학습합니다. Q-학습은 일반적으로 MDPs(마르코프 의사 결정 과정)로 모델링된 문제에 적용되며, 여기서 에이전트는 상태, 행동, 전이 확률 및 보상으로 구성된 환경과 상호 작용합니다. Q-학습의 핵심 통찰력은 현재 및 미래 상태의 Q-값 간의 관계를 표현하는 벨만 방정식입니다.

$$
Q(s,a)=R(s,a)+\gamma*max_{a'}Q(s', a')
$$

여기서 $Q$는 상태 행동 쌍에 대한 결과 값이고 $R$은 상태에서 행동을 취했을 때의 즉각적인 보상, $\gamma$는 미래 보상에 대한 즉각적인 보상에 대한 선호도를 나타내는 할인 인자입니다. Q-학습에서는 벨만 방정식을 반복 업데이트로 사용합니다. 에이전트가 행동 $a_t$를 취한 후, 전이 확률 $p(s_{t+1}|s_t, a_t) \in P(S \times A \to S)$와 보상 $r_t∈R$을 가지고 새로운 상태 $s_{t+1}$에 진입합니다. 에이전트의 목표는 예상 보상 합계를 최대화하는 최적 정책 $\pi^*$를 찾는 것입니다.

$$
Q^{\pi}(s_t, a_t)=max_{\pi} E \\
[r_{t+1}+\gamma r_{t+2} + \gamma^2 r_{t+3} + ... | s_t=s, a_t=a, \pi]
$$

그러나 Q-학습은 환경이 복잡해질수록 몇 가지 한계를 가지기 때문에 DQN(Deep Q-Network) 개념이 사용됩니다. DQN은 고차원 상태 공간에 대한 Q-값을 근사화하기 위해 Q-학습과 DNN(심층 신경망)을 결합한 RL 알고리즘입니다. DQN은 Watkins와 Dayan이 제안했습니다. Mnih 등은 RL을 심층 학습 기술과 결합했는데, 이는 가중치를 가진 신경망 Q-함수 근사자로 간주될 수 있습니다. 심층 학습과 강화 학습을 통합하는 DQN은 RL에서 강력한 방법으로 간주됩니다.

$$
L_i(\theta_i) = E_{(s,a,r,s') \sim U(D)} \\
[(r+\gamma max_{a'} Q(s',a';\theta_i^-) \\
-Q(s,a;\theta_i))^2]
$$

DQN은 딥 컨볼루션 신경망을 사용하여 근사한 값을 $Q(s,a;theta_i)$로 표기하며 여기서 $\theta_i$는 반복 $i$에서의 가중치입니다. 각 시간 $t$에서의 경험 $e_t = (s_t, a_t, r_t, s_{t+1})$은 데이터 세트에 저장됩니다.

---

### Discussion

결과는 DRL 접근 방식이 스케줄링을 위한 좋은 해결책이며 유망한 결과를 보여주고 전통적인 방법을 능가함을 보여줍니다. 더 구체적으로, DRL 에이전트는 다른 스케줄링 접근 방식보다 85% 더 짧은 완료 시간을 초래하는 스케줄을 제안할 수 있습니다.

유전 알고리즘(GAs)과 같은 전통적인 최적화 방법은 전역 탐색 휴리스틱을 통해 복잡한 스케줄링 문제를 처리하는 능력으로 잘 알려져 있습니다. 그러나 이러한 방법은 특히 동적이고 불확실한 생산 환경에서 상당한 계산 시간과 수동 매개변수 조정을 필요로 하는 경우가 많습니다. 대조적으로, DRL은 몇 가지 뚜렷한 장점을 제공합니다. DRL은 처음부터 완전한 재스케줄링을 필요로 하지 않고 변화하는 작업량 및 시스템 상태에 동적으로 적응하여 계산 시간을 크게 단축합니다. 이러한 방법은 반복적이고 데이터 집약적인 특성으로 인해 훈련 중에 상당한 계산 자원을 필요로 하지만, 다양한 시나리오에 걸쳐 일반화하는 능력은 초기 비용보다 훨씬 큽니다. 또한, DRL은 미리 정의된 휴리스틱 규칙에 의존하지 않고 환경으로부터 학습하여 의사 결정 유연성을 통합합니다.

---

### Conclusion

결론적으로, 본 연구는 DNN으로 안내되는 DRL 에이전트를 활용하여 생산 스케줄링에 대한 새로운 접근 방식을 성공적으로 도입하고 구현했습니다. DRL 스케줄링 에이전트는 스케줄링 문제 인스턴스에서 각 자원에 대한 최적의 디스패칭 규칙(SPT, LPT, MTRJ)을 선택하는 데 85%의 놀라운 향상을 달성하는 탁월한 성능을 보였습니다. SPT, LPT, MTRJ 및 무작위 작업 선택과 같은 전통적인 규칙과의 비교는 특히 자전거 산업의 실제 데이터에 적용될 때 DRL 에이전트의 우수성을 강조했습니다. 자전거 산업의 역학을 재현하는 DES 모델의 개발은 DRL 에이전트를 위한 중요한 훈련 기반이 되었습니다. 결과는 DRL 에이전트의 효과를 검증할 뿐만 아니라 특히 확립된 디스패칭 규칙과 함께 생산 스케줄링을 향상시키는 데 귀중한 도구로서의 잠재력을 강조합니다.

DRL은 동적 스케줄링에서 상당한 잠재력을 보여주지만, 고려해야 할 한계가 있습니다. 구현된 딥 Q-네트워크와 같은 DRL 모델을 훈련하는 것은 계산 집약적이며, 특히 대규모 또는 복잡한 생산 환경의 경우 상당한 시간과 자원이 필요합니다. 또한, 제안된 구현은 공급망 중단, 신제품 또는 제품 변형 추가와 같은 실제 불확실성을 완전히 포착하지 못합니다. 마지막으로, DRL의 하이퍼파라미터 튜닝에 대한 민감성과 매우 확률적인 설정에서 준최적 정책의 가능성은 보상 구조 및 상태-행동 표현의 신중한 설계 및 평가를 요구하는 과제를 제기합니다.

향후 연구에서는 FIFO 및 EDD(Earliest Due Date)와 같은 추가 디스패칭 규칙을 DRL 스케줄링 에이전트 프레임워크에 통합하여 본 연구의 범위를 확장할 계획입니다.

---

### 참고 자료

[원본 경로 #1](https://www.mdpi.com/2076-3417/15/1/232)



