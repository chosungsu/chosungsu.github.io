---
title: 'PairwiseNet: Pairwise Collision Distance Learning for High-dof Robot Systems'
date: '2025-10-06'
tags: ['embodied ai', 'paper review']
---

### Abstract

복잡한 환경에서 작동하는 로봇 조작 시스템을 위한 모션 계획은 여전히 어려운 문제입니다. 이는 충돌 거리와 그 미분을 모두 평가해야 합니다. 계산 복잡성 때문에 최근 연구들은 데이터 기반 접근 방식을 활용하여 충돌 거리를 학습하려고 시도했습니다. 그러나 다중 팔 로봇과 같은 복잡한 고자유도 (high-dof) 시스템에서는 그 성능이 크게 저하됩니다. 또한, 환경이 사소한 변화라도 겪을 때마다 모델을 다시 훈련해야 합니다.

본 논문에서 두 기하학적 모양 사이의 최소 거리를 추정하고 현재 모델의 많은 한계를 극복하는 모델인 PairwiseNet을 제안합니다. 전역 충돌 거리 학습 문제를 더 작은 쌍별 하위 문제 (pairwise sub-problems)로 나눔으로써, PairwiseNet은 전역 충돌 거리를 효율적으로 계산하는 데 사용될 수 있습니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/a1221526-77de-47f1-8acd-5dd6aa465844/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

계산 부담을 완화하는 한 가지 가능한 해결책은 데이터를 사용하여 충돌 거리 함수를 훈련시키는 것입니다. 로봇 구성과 그에 상응하는 충돌 거리로 구성된 충분한 데이터를 수집하여, 커널 퍼셉트론 모델, 서포트 벡터 머신 (SVM), 신경망과 같은 기계 학습 모델을 사용하여 충돌 거리 함수를 학습할 수 있습니다. 이 학습된 함수는 주어진 구성이 충돌 없는지를 신속하게 결정하는 데 사용될 수 있습니다.

이러한 데이터 기반 접근 방식은 저자유도 (low-dof) 로봇 시스템에서는 만족스러운 결과를 보여주었지만, 고자유도 로봇의 경우 종종 성능이 저하됩니다. 문제는 고자유도 로봇에 대한 충돌 거리 함수가 복잡하고 비볼록성이 높다는 사실에 있습니다.

기존의 데이터 기반 방법이 직면한 또 다른 도전 과제는 작은 환경 변화에 대한 민감성입니다. 예를 들어, 새로운 장애물 추가나 로봇 베이스 위치 변경은 완전히 다른 충돌 거리 함수로 이어질 수 있습니다. 이러한 방법 중 다수는 데이터 수집부터 모델 훈련까지 전체 훈련 절차를 반복해야 합니다.

본 논문에서 전역 충돌 거리를 예측하는 데 사용되는 기존 데이터 기반 접근 방식에 대한 유망한 대안을 제공하는 충돌 거리 추정 방법인 PairwiseNet을 제시합니다. PairwiseNet은 전역 충돌 거리를 직접 추정하는 대신, 쌍별 충돌 거리를 추정하는 데 중점을 둡니다. 쌍별 충돌 거리는 로봇 시스템 내의 두 요소 사이의 최소 거리입니다.

---

### Related works

#### 1. SVM

SVM 분류기를 사용하여 휴머노이드 로봇의 각 부분 쌍이 주어진 관절 구성에서 안전하거나 위험한 자체 충돌 상태에 있는지 식별했습니다. 위험한 부분 쌍의 최소 거리만이 캡슐 기반 BV 알고리즘을 사용하여 추정되어 충돌 거리 및 미분 계산을 단순화했습니다.

또한 14-자유도 이중 팔 로봇 조작 시스템에 SVM 분류기를 사용했습니다. SVM 분류기는 시스템의 모든 관절 위치로 구성된 벡터를 입력으로 받아, 충돌 레이블 1을 출력합니다.

#### 2. Neural network

다층 퍼셉트론 신경망 모델의 입력으로 관절 위치를 활용했습니다. 관절 구성의 위치 인코딩 벡터를 사용했습니다. 최근에는 그래프 신경망 (GNN) 모델인 GraphDistNet이 충돌 거리 추정을 위해 제안되었습니다. 이 모델은 매니퓰레이터 링크와 장애물 모두에 대해 그래프로 표현된 기하학적 모양에 대한 정보를 입력으로 받아, 두 그래프 사이의 기하학적 관계를 활용하여 충돌 거리를 예측합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/fd1f3b9f-e46c-4cf6-83ad-eeb1fcddd640/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

로봇 운동학 및 링크와 장애물의 기하학적 모양을 포함하는 대상 시스템의 시뮬레이터 환경을 사용할 수 있다고 가정합니다. 목표는 임의의 두 기하학적 모양 쌍 사이의 충돌 거리를 예측할 수 있는 쌍별 충돌 거리 추정 모델 $f_\psi$에 대한 최적의 모델 매개변수 $\psi$를 결정하는 것입니다.

이 모델은 두 기하학적 모양 $P_i, P_j$의 포인트 클라우드 데이터 (각 해당 객체 좌표로 표현됨)와 상대 변환 $T_{ij} \in \text{SE}(3)$을 입력으로 받아, 두 모양 사이의 추정된 쌍별 충돌 거리 $\hat{d}_{ij}$를 출력합니다.

$$
\hat{d}_{ij} = f_\psi(P_i, P_j, T_{ij})
$$

먼저, 주어진 관절 구성 $q$에서 요소 쌍 및 해당 변환의 집합 $S(q) = {(P_i, P_j, T_{ij}(q))}_{i, j}$가 대상 로봇 시스템으로부터 추출됩니다. 다음으로, PairwiseNet은 $S(q)$에 있는 각 요소 쌍 사이의 쌍별 충돌 거리를 결정하고, 이들 중에서 발견된 최소 거리가 로봇 시스템의 전역 충돌 거리로 간주됩니다.

#### Network Architecture

인코더는 동적 그래프 컨볼루션 신경망 (Dynamic Graph Convolutional Neural Network)의 두 개의 EdgeConv 레이어를 사용하여 포인트 클라우드 데이터로부터 32차원 모양 특징 벡터를 추출합니다.

그런 다음 회귀기는 두 모양 특징 벡터와 변환을 단일 벡터로 결합하고, 은닉 상태 차원이 $(\text{128, 128, 128})$인 네 개의 완전 연결 레이어를 사용하여 최소 거리를 출력합니다.

PairwiseNet의 훈련은 추정된 충돌 거리와 실제 충돌 거리 사이의 평균 제곱 오차 (MSE)를 손실 함수로 사용합니다.

$$
\mathcal{L} = \frac{1}{|\mathcal{D}_{\text{train}}|} \sum_{(P_i, P_j, T_{ij}, d_{ij}) \in \mathcal{D}_{\text{train}}} ||f_\psi(P_i, P_j, T_{ij}) - d_{ij}||^2
$$

이 접근 방식에는 인코더를 실행할 필요성을 제거하여 전역 충돌 거리 계산을 위한 효율적인 추론 전략이 포함됩니다. 인코더는 포인트 클라우드 데이터를 특징 벡터로 변환하는 심층 신경망입니다. 요소 쌍의 포인트 클라우드 데이터는 관절 구성에 관계없이 변하지 않기 때문에, 충돌 거리를 계산하기 전에 각 로봇 시스템에 대해 요소 쌍의 모양 특징 벡터를 한 번 계산하고 저장할 수 있습니다. 이러한 미리 계산된 모양 특징 벡터를 사용하여, PairwiseNet은 완전 연결 레이어로 구성된 단순한 신경망인 회귀기만을 사용하여 충돌 거리를 추정할 수 있습니다.

---

### Conclusion

본 논문에서 로봇 시스템의 전역 충돌 거리를 직접 예측하는 대신 요소 쌍 사이의 최소 거리를 추정하는 새로운 충돌 거리 추정 방법인 PairwiseNet을 제시합니다.

문제를 더 작은 하위 문제로 단순화함으로써, 전역 충돌 거리를 직접 예측하는 방법과 비교하여 고자유도 (high-dof) 로봇 시스템에 대해 상당한 성능 향상을 달성합니다.

또한, PairwiseNet은 추가 훈련이나 미세 조정 없이 로봇 베이스 재배치와 같은 환경 변화를 처리할 수 있습니다. 고자유도 다중 팔 로봇 시스템과 장애물이 있는 단일 팔 시스템 모두에 대해 PairwiseNet의 충돌 거리 추정 성능을 평가하고 비교했으며, 그 정확한 충돌 거리 추정과 환경 변화에 대한 일반화 능력을 검증했습니다.

---

### 참고 자료

[원본 경로 #1](https://openreview.net/pdf?id=Id4b5SY1Y8)
