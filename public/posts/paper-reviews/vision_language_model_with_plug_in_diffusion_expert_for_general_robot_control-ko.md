---
title: 'DexVLA: Vision-Language Model with Plug in Diffusion Expert for General Robot Control'
date: '2025-12-05'
tags: ['embodied ai', 'paper review']
---

### Abstract

다양한 환경에서 로봇이 다양한 작업을 수행할 수 있도록 하는 것은 로봇 학습의 중요한 도전 과제입니다. 비전-언어-행동 (VLA) 모델이 일반화 가능한 로봇 기술에 대한 잠재력을 보여주었지만 그것들의 잠재력을 완전히 실현하는 것은 행동 표현과 효율적인 훈련의 한계를 해결할 필요가 있습니다.

본 논문은 다양한 로봇 구현에 걸친 복잡하고 장기 작업을 위해 VLA의 효율성과 일반화 능력을 향상시킨 DexVLA를 소개합니다. 이는 교차 구현 학습을 위해 설계된 10억 개의 매개변수로 확장된 새로운 확산 기반 행동 전문가를 특징으로 합니다.

단일 팔, 양팔, 정교한 손을 포함한 다중 구현 전반에 걸쳐 포괄적인 실험을 수행하며 작업별 적응 없이 도전적인 작업에 대한 적응성, 제한된 데이터로 새로운 구현에서 정교한 기술을 학습하는 능력, 및 세탁물 접기와 같은 직접적인 언어 프롬프트만을 사용하여 복잡하고 장기적인 작업을 완료하는 능력을 입증합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/eebd07e4-b9be-4810-8958-21357b8aa582/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

전지전능한 로봇 파운데이션 모델의 비전을 실현하는 것은 지속적인 도전 과제에 직면해 있습니다. 두 가지 핵심 병목 현상이 진전을 방해합니다.

OpenVLA 및 Octo와 같은 모델은 Open-X Embodiment 데이터셋 또는 $\pi_0$에서 사용되는 데이터셋과 같이 큰 코퍼스에 의존합니다. 인간 시연을 통해 이러한 데이터를 수집하는 것은 극도로 비용이 많이 들고 노동 집약적입니다. 그리고 인터넷 규모 데이터 사전 훈련을 통한 향상된 시각적 및 언어적 이해에도 불구하고 VLM 구성 요소는 로봇 행동의 구현된, 감각 운동 맥락과 단절된 채로 남아 있습니다.

기존 행동 전문가의 한계 특히 교차 구현 데이터를 처리하는 데 있어 한계를 인식하여 새로운 확산 기반 행동 전문가를 제안합니다. 확산 전문가는 각각 특정 구현에 해당하는 다중 헤드 아키텍처를 활용하여 다양한 형태론 전반에 걸쳐 효과적인 학습을 가능하게 합니다. 더 나아가 확산 전문가의 모델 크기를 10억 매개변수로 확장하며 기존의 수백만 매개변수 규모에서 상당히 증가되었습니다. 점진적으로 더 어려운 작업을 학습하는 3단계 훈련 전략입니다. 이것은 인간이 학습하는 방식과 개념적으로 유사하며 학습자를 압도하는 것을 피하기 위해 간단한 작업으로 시작한 다음 점차적으로 복잡성을 도입합니다.

1단계인 교차 구현 사전 훈련 단계에서는 저수준, 구현 불가지론적 운동 기술을 학습하는 데 초점을 맞춥니다. 비전-언어 모델을 포함하지 않고 교차 구현 데이터를 사용하여 확산 전문가만 사전 훈련합니다. 다음으로 2단계인 구현별 정렬은 추상적인 비전-언어 표현을 특정 로봇의 물리적 제약에 연결합니다. 놀랍게도, 이 단계만으로도 모델이 셔츠 접기와 빈 피킹과 같은 다양한 작업을 도메인 내 객체에서 완료할 수 있도록 합니다. 마지막은 3단계인 작업별 적응으로 로봇이 복잡한 작업을 마스터하도록 하는 것을 목표로 합니다. 이러한 작업에는 장기 작업을 완료하고 새로운 객체로 일반화하는 것이 포함됩니다.

---

### Methods

#### Architecture

DexVLA 모델은 주로 트랜스포머 언어 모델 백본(Qwen2-VL)을 기반으로 합니다. 이미지 인코더를 사용하여 로봇의 이미지 관찰을 언어 토큰과 동일한 임베딩 공간으로 투영합니다. 다중 카메라 뷰의 경우, 이러한 시각적 토큰은 연결됩니다.

VLM 구성 요소는 두 가지 출력을 생성합니다. 추론 토큰과 행동 토큰입니다. 행동 토큰은 LayerNorm을 포함한 두 개의 선형 계층으로 구성된 투영 모듈을 통과합니다. 이 모듈은 LLaVA와 같은 비전-언어 모델에서 설계된 커넥터와 유사하며 VLM의 임베딩 공간을 변환하여 행동 전문가의 입력 요구 사항과 정렬하는 역할을 합니다. 추론 토큰은 FiLM 계층을 사용하여 정책 모델에 주입되며 이것은 정책 내의 투영 계층의 매개변수를 스케일하고 이동시킵니다. 결과적으로, 모델은 자율적으로 추론을 생성하고 이 추론을 확산 전문가 내에서 활용하여 행동 생성을 안내할 수 있습니다.

행동 전문가가 로봇의 행동 학습 프로세스를 지배하기 때문에 더 나은 시각-운동 정책 학습을 위한 좋은 신경 아키텍처를 설계하는 것이 필수적입니다. Scale Diffusion Policy를 활용했으며, 이것은 트랜스포머 아키텍처의 DP 변형으로 교차 구현 사전 훈련을 위해 설계되지 않았습니다. 각 헤드는 단일 로봇 구성을 담당합니다.

---

### Conclusion

DexVLA는 비전-언어 모델을 활용하여 의미론적 정보를 학습하고 10억 매개변수 확산 전문가를 사용하여 강력하고 일반화 가능한 시각-운동 정책을 학습하는 새로운 아키텍처입니다. 구현 커리큘럼 학습 전략을 도입하여, 네트워크가 세 가지 훈련 단계를 통해 구현 불가지론적 운동 기술에서 복잡하고 구현별 정교한 기술로 점진적으로 학습할 수 있도록 합니다.

더 나아가 하위 단계 추론을 통합하여 모델이 고수준 정책 모델에 의존하지 않고도 매우 장기적인 작업을 수행할 수 있도록 합니다. 작업별 적응 없이 복잡한 작업을 수행하는 능력, 제한된 데이터로 새로운 구현에서 미세 조정하는 능력, 및 고수준 정책 모델의 도움 없이 극도로 복잡하고 장기적인 작업을 실행하는 능력을 포함하여 다각적인 관점에서 평가됩니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2502.05855)
