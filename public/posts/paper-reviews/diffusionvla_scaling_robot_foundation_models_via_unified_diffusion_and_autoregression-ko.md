---
title: 'Diffusion-VLA, Scaling Robot Foundation Models via Unified Diffusion and Autoregression'
date: '2025-10-22'
tags: ['embodied ai', 'paper review']
---

### Abstract

본 논문에서 기존 방법의 한계를 해결하기 위해 자기회귀적 추론을 확산 정책과 통합하는 새로운 프레임워크인 DiffusionVLA를 제시합니다. 자기회귀적 VLA 모델은 정확하고 강력한 행동 생성이 부족한 반면 확산 기반 정책은 본질적으로 추론 능력이 부족합니다.

사전 훈련된 VLM에 의해 활성화된 작업 분해 및 설명 프로세스인 자기회귀적 추론을 사용하여 확산 기반 행동 정책을 안내하는 것이 핵심입니다. 추론을 행동 생성과 밀접하게 결합하기 위해 자체 생성된 추론 구문을 정책 학습 프로세스에 직접 내장하는 추론 주입 모듈 (reasoning injection module)을 도입합니다. 이 프레임워크는 단순하고, 유연하며, 효율적이어서 다양한 로봇 플랫폼에 걸쳐 원활한 배포를 가능하게 합니다.

---

### Introduction

최근 Vision-Language-Action 모델은 로봇 공학에서 유망한 방향으로 부상했습니다. 이러한 VLA 중에서 두드러진 접근 방식은 액션 예측을 LLM의 지배적인 자기회귀 패러다임을 반영하는 다음 토큰 예측 작업으로 구성하며, 각 토큰의 생성이 이전 토큰에 조건화되어 순차적으로 개별 토큰을 예측하여 작동합니다. RT-2, OpenVLA와 같은 이러한 모델은 주목할 만한 성공을 입증했지만, 내재적인 한계를 가지고 있습니다.

첫째, 연속적인 행동 데이터를 고정 크기 토큰으로 이산화하는 것은 행동의 일관성과 정밀도를 방해할 수 있습니다. 둘째, NTP는 행동 생성에 본질적으로 비효율적이며, 성능이 중요한 실시간 로봇 응용 프로그램에서 특히 그러합니다.

수많은 방법이 액션 시퀀스 생성을 노이즈 제거 프로세스로 모델링하여 조작 작업에서 강력한 성능을 입증했습니다. 이 접근 방식은 로봇 행동의 다중 모드 특성을 더 잘 포착하고 NTP 기반 VLA 모델에 비해 더 빠른 시퀀스 생성을 가능하게 합니다.  그러나 정책 학습을 위한 확산 모델의 장점에도 불구하고, 그것들은 VLA 모델이 복잡한 작업을 효과적으로 해결하는 데 결정적인 추론 능력이 부족하며, 이것은 LLM을 분명히 개선하는 구성 요소입니다.

논리적 추론과 실행 가능한 로봇 정책 사이에는 종종 암묵적인 격차가 있기 때문에 요소를 결합하는 것은 추론 잠재력을 완전히 활용하지 못합니다. 이러한 격차를 해소하기 위해 출력을 재사용하고 그것들을 정책 헤드에 직접 삽입하여 명시적인 추론 신호로 정책 학습 프로세스를 풍부하게 하는 추론 주입 모듈을 제안합니다.

---

### Methods

#### Architecture

<img src="https://velog.velcdn.com/images/devjo/post/bbf2e7c1-c019-43dc-9d55-b4a80a81eb83/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

이미지, 텍스트 및 비디오가 교차된 모든 시퀀스가 주어지면, 먼저 SigLIP (Zhai et al., 2023)를 사용하여 이미지를 조밀한 시각적 특징으로 인코딩합니다. 이러한 인코딩은 그런 다음 Transformer를 통해 고정된 수의 $N$ 시각적 임베딩으로 변환됩니다. 로봇 학습에서 일반적인 시각적 입력은 종종 여러 카메라 뷰를 포함한다는 점은 주목할 가치가 있습니다. 이것을 관리하기 위해, 공유된 SigLIP 시각 백본을 각 뷰에 적용한 후 결과 시각적 토큰을 연결했습니다.

시각 언어 처리를 위해, 시각 언어 모델인 Qwen2-VL (Wang et al., 2024b)을 활용했습니다. 시각 언어 이해를 행동 생성과 분리하기 때문에 다른 사전 훈련된 VLM을 백본으로 사용하는 것도 가능하며 이것은 전반적인 아키텍처를 새로운 고급 모델에 적합하도록 유연하게 만듭니다.

#### Projection layer for action tokens

VLM의 최종 임베딩 계층을 따라, 고정된 수의 행동 토큰이 생성됩니다. 이러한 토큰은 그런 다음 LayerNorm을 가진 두 개의 MLP 계층으로 구성된 투영 모듈로 전달됩니다. 이 투영 모듈은 기존 시각 언어 모델인 LLaVA (Liu et al., 2023b;a)에서 발견되는 것과 유사하게 작동하며 VLM의 출력 임베딩을 확산 모델로 연결하고 그것들의 출력 차원을 정렬합니다.

확산 모델 자체는 랜덤으로 초기화된 가중치를 가진 표준 Diffusion Policy 설계 (Chi et al., 2023)를 따릅니다. 이 구성 요소는 또한 LLM의 추론을 통합하며 로봇의 관절 공간을 예측하기 위해 행동 디코더의 바닥에 있는 마지막 계층에 MLP 계층이 부착됩니다.

---

### Conclusion

본 연구에서는 단일 팔 및 이중 팔 로봇을 포함한 시뮬레이션과 실제 시나리오 모두에서 강력한 성능을 제공하는 최첨단 시각 언어 행동 모델인 DiVLA를 제시합니다. 핵심은 다음 토큰 예측 목표와 확산 모델을 결합하는 데 있습니다. 전자는 작업 추론을 위해, 후자는 행동 예측을 위해 사용됩니다.

행동 생성을 향상시키기 위해 도입한 추론 재사용 모듈은 강력한 일반화 능력을 가지고 있어, 새로운 지침, 작업, 및 환경에 효과적으로 적응한다는 것을 보여줍니다.

---

### 참고 자료

[원본 경로 #1](https://openreview.net/pdf?id=VdwdU81Uzy)
