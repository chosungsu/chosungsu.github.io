---
title: 'Foundation Model Driven Robotics, A comprehensive Review'
date: '2025-09-08'
tags: ['embodied ai', 'paper review']
---

### Abstract

대규모 언어 모델과 비전-언어 모델을 중심으로 하는 파운데이션 모델의 빠른 등장은 로봇 공학에 변혁적인 패러다임을 도입했습니다. 이러한 모델들은 의미론적 이해, 고수준 추론, 교차 모달 일반화에서 강력한 능력을 제공하여 인지, 계획, 제어, 인간-로봇 상호 작용에서 상당한 발전을 가능하게 합니다.

절차적 장면 생성, 정책 일반화, 다중 모드 추론과 같은 핵심 활성화 추세는 제한된 구현, 다중 모드 데이터 부족, 안전 위험, 계산 제약을 포함한 핵심 병목 현상과 함께 논의됩니다. 본 논문은 파운데이션 모델 기반 로봇 공학의 아키텍처적 강점과 결정적 한계 모두를 식별하고, 실시간 작동, 그라운딩, 복원력, 신뢰에서의 해결되지 않은 도전 과제를 강조합니다.

---

### Introduction

로봇 시스템은 여전히 특히 실제 응용 분야에 필요한 유연성, 적응성, 일반화 측면에서 인간 수준 지능을 가지고 있지 않습니다. 그것들은 종종 작업 전반에 걸쳐 지식을 전달하거나, 예상치 못한 시나리오에 적응하거나, 인간 행동을 특징짓는 미묘한 의사 결정을 보이는 데 어려움을 겪습니다. 전통적으로, 로봇 자율성은 명시적 프로그래밍 또는 좁은 작업별 학습을 기반으로 했습니다.

LLM의 로봇 공학으로의 최근 통합은 그것들의 풍부한 의미론적 지식과 추론 능력을 활용하여 로봇 에이전트의 통신, 계획, 적응성을 개선하는 새로운 패러다임을 도입합니다. 이것은 고수준 인간 명령을 해석하고, 목표와 행동에 대해 추론하며, 심지어 저수준 제어 코드를 생성할 수 있습니다. 로봇이 언어로부터 학습된 방대한 사전 지식을 활용하여 더 광범위한 작업과 환경에 대처할 수 있도록 허용하는 더 일반적인 목적의 로봇 지능을 가능하게 합니다.

그러나 LLM 단독으로는 물리적 맥락을 알지 못합니다. 그것들은 구현이 부족하고 본질적으로 측정 항목, 센서 데이터 또는 동적 물리학을 이해하지 못합니다. 시뮬레이션에서 오픈 월드에 이르는 다양한 환경에서의 진행 상황은 LLM 기반 로봇 공학의 잠재력과 현재 한계 모두를 드러냅니다. 의미론적 그라운딩과 실시간 성능을 포함한 주요 병목 현상은 언어 이해와 물리적 실행 사이의 격차를 해소하는 데 도움이 되는 출현 솔루션과 함께 논의됩니다.

---

### Methods

#### Foundation

<img src="https://velog.velcdn.com/images/devjo/post/7338212a-f29f-4faa-88f7-2394a8243e9b/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

LLM이 사용자 의도 해석부터 실행 가능한 계획 생성에 이르기까지 로봇에서 고수준 인지를 가능하게 하는 데 중심적이 됨에 따라, 그것들의 아키텍처적 및 기능적 진행은 구현된 설정에서의 적합성을 평가하는 데 결정적인 맥락을 제공합니다. 최신 세대 모델은 시각적 입력에 대한 지원, 정제된 추론, 개선된 대화 관리를 통해 이러한 능력을 더욱 확장합니다. 병행하여 오픈 소스 대안은 개인 정보 보호에 민감한 실시간 로봇 시스템에 대한 중요한 고려 사항인 로컬 하드웨어에 배포 가능한 접근 가능하고 미세 조정 가능한 모델을 도입했습니다.

그러나 로봇 시스템은 종종 풍부한 인지 입력에 의존하며, 이것은 언어와 함께 시각적 데이터를 해석하기 위해 텍스트 처리를 넘어서는 모델을 필요로 합니다. VLM은 시각적 인지 능력으로 LLM을 확장하여 이미지 캡션, 시각적 질의 응답 같은 작업을 위한 다중 모드 추론을 가능하게 하며 이 모든 것은 로봇에서 그라운딩된 언어를 이해하는 데 중요합니다.

초기 노력은 시각적 및 텍스트적 특성을 정렬하기 위해 공동 아키텍처를 도입하여 인지 기반 상호 작용을 위한 기반을 마련했습니다. 특히, CLIP과 같은 대조 학습 접근 방식은 이미지-텍스트 임베딩을 사용하여 오픈-어휘 시각적 인식을 입증했습니다. 이것은 로봇이 명시적인 범주 훈련 없이 이름으로 객체를 인지할 수 있도록 허용하여 의미론적 매핑과 언어 프롬프트로부터의 객체 검색과 같은 능력을 지원합니다. 후속 VLM은 다중 모드 훈련을 더욱 확장하여 대규모 이미지-텍스트 코퍼스에서 훈련된 통합 트랜스포머 아키텍처가 최소한의 감독으로 작업 전반에 걸쳐 일반화할 수 있음을 보여주었습니다.

#### Integration in Robotics

<img src="https://velog.velcdn.com/images/devjo/post/9748eff7-16ac-4c3d-9ac7-9a9be02762e3/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

로봇 공학의 시각적 인지는 대규모 VLM로 인해 일반화에 있어 도약을 보았습니다. 전통적인 로봇 비전 시스템은 제한된 훈련 데이터와 좁게 정의된 객체 범주에 의해 제약되었습니다. 대조적으로 OpenAI의 CLIP 모델은 4억개의 이미지-텍스트 쌍에서 시각적 개념을 학습하여 제로샷 인식을 가능하게 하고 로봇이 단순히 텍스트 레이블을 제공함으로써 새로운 객체나 장면을 식별할 수 있도록 허용하여 각 범주에 대한 작업별 훈련의 필요성을 없애줍니다. 이러한 능력은 오픈 월드 인지를 극적으로 개선합니다.

전통적인 플래너는 다양한 시나리오를 위한 명시적인 프로그래밍 또는 광범위한 작업별 훈련을 필요로 했습니다. 이와 대조적으로 GPT-3, GPT-4 모델들은 행동 시퀀스, 상식 추론, 세계 지식에 대한 풍부한 사전 지식을 제공합니다. 이러한 모델은 추가 훈련 없이 추상적인 목표를 일관된 단계로 분해할 수 있습니다. 그러나 LLM 생성 계획에 직접 의존하는 것은 제안된 행동이 로봇의 물리적 능력이나 제약 조건을 초과할 때 실패로 이어질 수 있습니다. 이러한 한계를 해결하는 결정적인 개발은 SayCan 프레임워크이며 이것은 의미론적 추론을 로봇의 학습된 어포던스를 기반으로 하는 타당성 추정과 통합합니다. LLM은 고수준 행동(예를 들어 스펀지를 잡아라, 흘린 것을 닦아라)를 제안하는 반면 어포던스 모델은 현재 맥락에서 성공적인 실행의 가능성을 평가합니다.

움직임 제어와 정책 실행의 수준에서, 파운데이션 모델은 로봇 공학에 일반화와 적응성을 위한 메커니즘을 도입합니다. 하나의 연구 방향은 많은 작업과 구현 전반에 걸쳐 작동할 수 있는 일반주의 컨트롤러라는 아이디어 아래 대규모 모델을 직접 정책 네트워크로 사용합니다. 주목할 만한 예는 DeepMind의 Gato이며 이것은 게임 플레이, 이미지 캡션, 물리적 로봇 조작을 포함한 600개 이상의 작업으로부터의 데이터로 훈련된 트랜스포머 모델입니다. 입력 양식에 따라 텍스트나 토크 명령을 생성할 수 있어 교차 도메인 정책 유연성을 입증합니다. 전문 모델보다 우수하지는 않지만 다중 작업 다중 모드 제어 아키텍처의 타당성을 검증합니다.

#### Open World

파운데이션 모델의 최근 발전은 로봇 자율성을 좁게 스크립트화된 작업을 넘어서 오픈 월드, 비정형 환경으로 확장할 놀라운 잠재력을 보여주었습니다.

오픈 월드 환경의 복잡성을 해결하기 위해, 몇 가지 방법은 다중 모드 인지를 언어 조건부 추론과 통합하여 적응형 작업 계획을 생성합니다. 주요 전략은 고수준 명령을 추상적인 목표를 실행 가능한 이동 및 조작 기본 요소에 매핑하는 행동 시퀀스 체인으로 분해하는 것을 포함합니다. VLM은 먼저 장면과 지침을 구문 분석하여 의미론적 및 공간적 정보를 추출하고 LLM이 시간적으로 정렬된 행동 체인을 합성합니다. 이러한 분해는 어포던스 추론을 통합하여 생성된 하위 목표가 로봇의 구현과 현재 환경 맥락에 물리적으로 기반을 두도록 보장합니다. 이러한 어포던스 기반 분해는 객체의 가용성, 파지 가능성, 도달 가능한 표면을 지속적으로 확인함으로써 실행 불가능하거나 안전하지 않은 하위 행동을 완화하는 데 도움이 됩니다.

고수준 계획을 넘어서, 오픈 월드 작동은 작업, 환경, 구현 전반에 걸쳐 일반화될 수 있는 적응형 제어 정책을 필요로 합니다. 일반주의 정책 아키텍처는 감각 입력과 작업 설명에서 연속적인 모터 출력으로의 종단 간 매핑을 학습하는 대규모 비전-언어 행동 모델을 통해 이것을 해결합니다. Nvidia의 Groot는 비전 언어 트랜스포머가 관찰과 목표를 해석하고, 확산-트랜스포머 정책이 시간적으로 일관된 고차원 모터 명령을 생성하는 유사한 아키텍처 설계를 통해 이러한 종류의 모델을 예시합니다.

---

### Conclusion

현재 파운데이션 모델은 계산 집약적이며 온보드 로봇 배포에 종종 너무 느립니다. 그것들의 높은 추론 지연 시간과 메모리 사용량은 자율 로봇의 엄격한 실시간 요구 사항과 충돌합니다. 이러한 한계는 폐쇄 루프 제어와 신속한 의사 결정을 방해합니다. 로봇 플랫폼의 지연 시간과 전력 제약을 충족시키기 위해 모델 압축, 증류, 온디바이스 최적화의 진전이 필요합니다.

파운데이션 모델은 주로 로봇별 정보에 대한 노출이 제한된 인터넷 규모 텍스트나 이미지 데이터에 훈련됩니다. 그것들은 신뢰할 수 있는 인지와 제어에 결정적인 물리적 시나리오와 예외 상황의 긴 꼬리를 포착하지 못합니다. 이러한 구현 격차는 모델이 아직 실제 역학이나 센서 양식을 완전히 이해하지 못할 수 있음을 의미합니다. 이 격차를 해소하는 것은 모델을 물리적 현실에 그라운딩하기 위해 시뮬레이션 생성 경험과 협업 데이터 수집을 통한 광범위한 로봇 관련 훈련 데이터를 필요로 할 것입니다.

모델은 잘못된 출력을 환각하거나 적대적 프롬프트에 의해 조작되어 잠재적으로 물리적으로 해로운 행동으로 이어질 수 있습니다. 전통적인 로봇 안전 조치는 이러한 실패 모드를 포괄하지 않으며, 현재 LLM 안전 기술은 로봇 작동의 물리적 맥락을 간과합니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2507.10087)
