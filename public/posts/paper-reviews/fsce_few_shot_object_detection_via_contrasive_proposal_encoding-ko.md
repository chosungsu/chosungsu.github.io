---
title: 'FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding'
date: '2023-07-21'
tags: ['object detection', 'paper review']
---

### Abstract

소수 샘플 객체 탐지 FSOD로 알려진, 매우 적은 수의 훈련 예시만 주어졌을 때 이전에 보지 못한 객체를 인식하는 것에 대한 관심이 새롭게 부각되고 있습니다. 최근 연구들은 좋은 특징 임베딩이 유리한 소수 샘플 학습 성능에 도달하는 핵심임을 입증합니다.

본 논문에서는 서로 다른 $\text{IoU}$ 점수를 가진 객체 제안 (object proposals)이 대조적 시각 표현 학습에서 사용되는 이미지 내 데이터 증강과 유사하다는 점을 관찰했습니다. 그리고 이 유사점을 활용하여 지도 대조 학습 (supervised contrastive learning)을 통합함으로써 FSOD에서 더욱 강력한 객체 표현을 달성합니다.

대조적 제안 인코딩을 통한 소수 샘플 객체 탐지 FSCE를 제안합니다. 이는 탐지된 객체의 분류를 용이하게 하는 대조 인지 객체 제안 인코딩을 학습하기 위한 단순하지만 효과적인 접근 방식입니다. 희귀 객체에 대한 평균 정밀도 AP의 저하가 주로 novel 인스턴스를 혼동 가능한 클래스로 오분류하는 데서 비롯된다는 점에 주목했습니다. 그리고 대조적 제안 인코딩 손실 (CPE loss)을 통해 인스턴스 수준의 클래스 내 압축성과 클래스 간 분산을 촉진함으로써 이러한 오분류 문제를 완화합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/d359529b-7313-467a-bcd9-deeded151c2c/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

현대 합성곱 신경망의 발전은 일반 객체 탐지에서 큰 진보를 이루었습니다. Deep detector는 성능을 포화시키기 위해 대량의 주석이 달린 훈련 데이터를 요구합니다. 소수 샘플 학습 시나리오에서, 딥 탐지기는 더 심각한 과적합을 겪으며, 소수 샘플 탐지와 일반 객체 탐지 사이의 격차는 소수 샘플 이미지 분류에서의 격차보다 더 큽니다. 반면에, 사람은 매우 적은 예시만 주어져도 새로운 시각적 개념을 빠르게 이해하고 새로 학습한 범주의 객체를 인식할 수 있습니다. 

소수 샘플 이미지 분류를 선례로 하여, 소수 샘플 객체 탐지의 초기 시도들은 메타 학습 전략 을 활용했습니다. meta learner는 개별 작업의 에피소드로 훈련되며, 일반 객체 (base 클래스)의 메타-작업 샘플을 희귀 객체 (novel 클래스)와 짝지어 소수 샘플 탐지 작업을 시뮬레이션합니다.

최근, 2단계 미세 조정 기반 접근 방식 TFA가 소수 샘플 탐지 개선에 더 많은 잠재력을 드러냈습니다. 이를 기준점으로 잡고 base 클래스에서 훈련된 모든 매개변수를 고정하고 novel 데이터로 박스 분류기 및 박스 회귀기만 미세 조정한 결과 이전의 meta learner보다 더 나은 성능을 보입니다. MPSR은 소수 샘플 데이터셋에 내재된 스케일 편향을 완화함으로써 TFA를 개선하지만, 그들의 긍정적 개선 브랜치 (positive refinement branch)는 수동 선택을 요구하여 다소 깔끔하지 못합니다.

객체 탐지는 나타난 객체의 지역화 (localization)와 분류 (classification)를 포함합니다. 소수 샘플 탐지에서, 사람들은 희귀 객체가 배경으로 간주될 수 있다는 우려로 novel 객체의 지역화가 base 범주에 비해 저조할 것이라고 자연스럽게 추측할 수 있습니다. 그러나 소수 샘플 탐지에서 일반적으로 채택되는 탐지기인 Faster R-CNN을 사용한 실험에 따르면, 클래스 불가지론적 영역 제안 네트워크 (RPN)는 novel 인스턴스에 대한 전경 제안을 만들 수 있으며, 최종 박스 회귀기는 novel 인스턴스를 매우 정확하게 지역화할 수 있습니다.

따라서 대조적 제안 인코딩(CPE)를 제안하는데 잘 분리된 결정 경계를 학습하는 일반적인 접근 방식은 대규모 마진 분류기를 사용하는 것이지만, 범주 수준의 긍정 마진 기반 분류기는 이러한 데이터 부족 설정에서는 작동하지 않는 문제를 해결하기 위함입니다. 인스턴스 수준의 판별적 특징 표현을 학습하기 위해, 대조 학습은 인식, 식별, 그리고 최근 성공적인 자가 지도 모델을 포함한 여러 작업에서 그 효과를 입증했습니다. 객체에 대한 서로 다른 $\text{IoU}$를 가진 영역 제안이 위 이미지에 묘사된 것처럼, 이미지 내 증강 크롭핑과 자연스럽게 유사하다고 생각합니다.

---

### Related Work

#### $\text{Few-shot Learning}$

소수 샘플 학습 ($\text{Few-shot learning}$)은 제한된 레이블링된 예시가 주어진 새로운 개념을 인식하는 것을 목표로 합니다.

메타 학습 ($\text{Meta-learning}$) 접근 방식은 개별 작업의 에피소드에 대해 메타 모델을 훈련하여, 적은 수의 샘플로 새로운 작업에 적응할 수 있도록 하는 것을 목표로 하며, 이는 학습 방법을 학습하는 것 ($\text{learning-to-learn}$)으로 알려져 있습니다.

딥 측정 학습 ($\text{Deep metric-learning}$) 기반 접근 방식은 후속 작업 ($\text{downstream tasks}$)을 용이하게 하는 좋은 특징 표현 임베딩을 학습하는 것을 강조합니다.

환각 생성기 기반 ($\text{hallucinator-based}$) 방법은 가짜 데이터 생성을 학습하여 데이터 부족을 해결합니다.

#### $\text{Few-Shot Object Detection}$

도전적인 소수 샘플 객체 탐지 ($\text{FSOD}$) 문제를 다루는 연구는 크게 두 가지 흐름이 있습니다.

메타 학습 기반 접근 방식은 base 클래스에서 지식 이전을 돕는 메타 학습자를 훈련합니다. $\text{Meta R-CNN}$은 ROI 헤드를 재구성하기 위한 채널별 어텐션 계층을 메타 학습하고 $\text{MetaDet}$은 가중치 예측 메타 모델을 적용하여 기반 탐지기로부터 범주별 매개변수를 동적으로 전달합니다.

미세 조정 기반 탐지기는 $\text{MPSR}$이 소수 샘플 데이터셋의 스케일 희소성을 완화하여 현재 최신 기술 성능을 설정했지만, 긍정적 개선 브랜치가 수동적 결정을 포함하므로 일반화 가능성이 제한적입니다.

#### $\text{Contrastive Learning}$

대조 학습 탐구에 대한 새로운 관심은 최근 자가 지도 모델의 성공에 기여했습니다.

대조 목표를 최적화하는 것은 긍정 쌍으로 정의된 유사한 인스턴스 간의 일치를 최대화하는 동시에, 부정 쌍인 비유사한 인스턴스 간의 차이를 장려합니다. 대조 학습을 통해, 알고리즘은 픽셀 수준의 세부 사항에 집중하지 않고, 다른 이미지를 구별할 만큼 충분히 효과적인 고급 특징을 인코딩하는 표현을 구축하는 것을 학습합니다.

---

### Method

<img src="https://velog.velcdn.com/images/devjo/post/9e7b8b69-7043-459a-8856-619e25efb994/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

우리가 제안하는 방법 FSCE는 단순한 2단계 훈련을 포함합니다.

첫 번째 단계: 표준 $Faster R-CNN$ 탐지 모델은 풍부한 기반 클래스 데이터 ($D_{\text{train}} = D_{\text{base}}$)로 훈련됩니다.

두 번째 단계: 기반 탐지기는 새로운 인스턴스와 무작위로 샘플링된 기반 인스턴스로 구성된 균형 잡힌 데이터셋을 사용하여 미세 조정 ($D_{\text{train}} = D_{\text{novel}} \cup D_{\text{base}}$)을 통해 새로운 데이터로 전달됩니다.

#### 대조적 객체 제안 인코딩 (Contrastive object proposal encoding)

2단계 탐지 프레임워크에서, RPN은 백본 특징 맵을 입력으로 받아 영역 제안을 생성하고, RoI 헤드는 각 영역 제안을 분류하고 객체를 포함한다고 예측되면 바운딩 박스를 회귀합니다. Faster R-CNN 파이프라인에서, RoI 헤드 특징 추출기는 먼저 영역 제안을 고정된 크기로 풀링한 다음, 이를 RoI 특징으로 알려진 벡터 임베딩 $\mathbf{x} \in \mathbb{R}^{D_R}$로 인코딩합니다.

일반 탐지기는 제한된 샷의 영역 제안에 대해 견고한 특징 표현을 구축하는 데 실패하여, 지역화된 객체의 오분류와 낮은 평균 정밀도를 초래합니다. 아이디어는 더욱 판별적인 객체 제안 임베딩을 학습하는 것이지만, 실험에 따르면 범주 수준의 긍정 마진 분류기는 이러한 데이터 부족 설정에서는 작동하지 않습니다.

더 적은 샷에서 더 견고한 객체 특징 표현을 학습하기 위해, 배치 대조 학습을 적용하여 객체 제안 임베딩의 인스턴스 수준 클래스 내 유사성 및 클래스 간 구별을 명시적으로 모델링할 것을 제안합니다.

Faster R-CNN 프레임워크에 대조 표현 학습을 통합하기 위해, 분류 및 회귀 브랜치에 평행하게 주 RoI 헤드에 대조 브랜치를 도입합니다. RoI 특징 벡터 $\mathbf{x}$는 ReLU 활성화 후 값을 포함하므로 0에서 잘려, 두 제안 임베딩 간의 유사성을 직접 측정할 수 없습니다. 따라서 대조 브랜치는 무시할 수 있는 비용으로 1-계층 다층 퍼셉트론 (MLP) 헤드를 적용하여 RoI 특징을 대조 특징 $\mathbf{z} \in \mathbb{R}^{D_C}$로 인코딩하며, 기본적으로 $D_C = 128$입니다.

이후, MLP-헤드 인코딩된 RoI 특징에 대해 객체 제안 표현 간의 유사성 점수를 측정하고 대조 목표를 최적화하여 동일 범주 객체 제안 간의 일치를 최대화하고 다른 범주 제안의 구별성을 촉진합니다. 객체 탐지를 위한 제안된 대조 손실은 다음 섹션에서 설명됩니다. 코사인 유사성 기반 바운딩 박스 분류기를 채택하며, $i$번째 인스턴스를 $j$번째 클래스로 예측하는 로짓은 하이퍼스피어에서 RoI 특징 $\mathbf{x}_i$와 클래스 가중치 $\mathbf{w}_j$ 사이의 스케일링된 코사인 유사성으로 계산됩니다.

$$
\text{logit}\{i,j\} = \alpha \frac{\mathbf{x}_i^\top \mathbf{w}_j}{\|\mathbf{x}_i\| \cdot \|\mathbf{w}_j\|}
$$

$\alpha$는 경사도를 확대하기 위한 스케일링 인자입니다. 실험에서 $\alpha = 20$으로 경험적으로 고정합니다. 제안된 대조 브랜치는 $\text{RoI}$ 헤드가 다른 범주 간의 구별을 완화하는 대조 인지 객체 제안 임베딩을 학습하도록 안내합니다.

#### 대조적 제안 인코딩 ($\text{CPE}$) 손실 (Contrastive Proposal Encoding (CPE) Loss)

분류 및 식별의 지도 대조 목표에서 영감을 받아, 우리의 $\text{CPE}$ 손실은 탐지에 맞춘 고려 사항과 함께 다음과 같이 정의됩니다. 구체적으로, $N$개의 RoI 박스 특징으로 구성된 미니 배치 $\{\mathbf{z}_i, u_i, y_i\}_{i=1}^N$에 대해, $\mathbf{z}_i$는 $i$번째 영역 제안에 대한 대조 헤드 인코딩 $\text{RoI}$ 특징이고, $u_i$는 일치하는 정답 바운딩 박스와의 $\text{IoU}$ 점수를 나타내며, $y_i$는 정답의 레이블을 나타냅니다.

$$
\mathcal{L}_{\text{CPE}} = \frac{1}{N}\sum_{i=1}^N f(u_i) \cdot \mathcal{L}_{z_i} 
$$

$$
\mathcal{L}_{z_i} = \frac{-1}{N_{y_i} - 1} \sum_{j=1, j \ne i}^N \mathbb{I}_{\{y_i = y_j\}} \cdot \log \frac{\exp(\tilde{\mathbf{z}}_i \cdot \tilde{\mathbf{z}}_j/\tau)}{\sum_{k=1, k \ne i}^N \exp(\tilde{\mathbf{z}}_i \cdot \tilde{\mathbf{z}}_k/\tau)}
$$

$N_{y_i}$는 $y_i$와 동일한 레이블을 가진 제안의 수이며, $\tau$는 InfoNCE에서와 같은 하이퍼 매개변수 온도입니다. 위 공식에서 $\tilde{\mathbf{z}}_i = \frac{\mathbf{z}_i}{|\mathbf{z}_i|}$는 정규화된 특징을 나타내므로, $\tilde{\mathbf{z}}_i \cdot \tilde{\mathbf{z}}_j$는 투영된 하이퍼스피어에서 $i$번째와 $j$번째 제안 간의 코사인 유사성을 측정합니다. 위 손실 함수의 최적화는 동일 레이블을 가진 객체 제안 간의 인스턴스 수준 유사성을 증가시키고, 다른 레이블을 가진 제안을 투영 공간에서 서로 떨어지게 합니다. 결과적으로, 각 범주의 인스턴스는 더 단단한 클러스터를 형성하고, 클러스터 주변의 마진이 확대됩니다. 제안 일관성 제어 (Proposal consistency control)의미론적 정보가 전체 이미지에서 오는 이미지 분류와 달리, 탐지에서의 분류 신호는 영역 제안에서 옵니다. 우리는 낮은 IoU 제안이 회귀된 객체의 중심에서 너무 많이 벗어나 무관한 의미론을 포함할 수 있다는 고려 하에, 대조되는 데 사용되는 제안의 일관성을 보장하기 위해 IoU 임계값을 사용할 것을 제안합니다.

위 공식에서 $f(u_i)$는 제안 일관성 임계값 $\varphi$와 재가중 함수 $g(\cdot)$로 정의되어 제안의 일관성을 제어합니다.

$$
f(u_i) = \mathbb{I}_{\{u_i > \varphi\}} \cdot g(u_i) 
$$

$g(\cdot)$는 IoU 점수 수준이 다른 객체 제안에 대해 다른 가중치 계수를 할당합니다. $\varphi=0.7$이 좋은 절단점이며, 대조 헤드가 가장 중심에 있는 객체 제안으로 훈련되도록 한다는 것을 발견했습니다.

훈련 목표의 첫 번째 단계에서, base 탐지기는 표준 Faster R-CNN 손실로 훈련됩니다. 이는 앵커로부터 전경 제안을 만들기 위한 이진 교차 엔트로피 손실 $\mathcal{L}_{\text{rpn}}$, 바운딩 박스 분류를 위한 교차 엔트로피 손실 $\mathcal{L}_{\text{cls}}$, 그리고 박스 회귀 델타를 위한 smoothed-$\text{L}1$ 손실 $\mathcal{L}_{\text{reg}}$입니다.

미세 조정 단계에서 새로운 데이터로 전달할 때, 대조 손실이 훈련을 불안정하게 만들지 않고 다중 작업 방식으로 주 Faster R-CNN 손실에 추가될 수 있음을 발견했습니다.

$$
\mathcal{L} = \mathcal{L}_{\text{rpn}} + \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{reg}} + \lambda \mathcal{L}_{\text{CPE}}
$$

$\lambda$는 손실의 균형을 맞추기 위해 $0.5$로 설정됩니다.

---

### Conclusion

본 연구에서 대조적 제안 인코딩 ($\text{contrastive proposals encoding}$)을 통해 소수 샘플 객체 탐지 (FSOD) 문제를 해결하는 새로운 관점을 제안합니다. 정확하게 지역화된 객체들이 오분류되는 것을 효과적으로 방지함으로써, 이 방법은 모든 샷과 두 벤치마크 모두에서 최신 기술 sota 결과를 달성했습니다.

제안된 대조적 제안 인코딩 헤드는 무시할 수 있는 비용으로 일반적으로 적용 가능합니다. 이는 훈련 파이프라인을 방해하지 않고 모든 2단계 탐지기에 통합될 수 있습니다. 이 연구는 대조 학습을 객체 탐지 프레임워크에 통합하는 것의 타당성을 입증합니다.

---

### 참고 자료

[원본 경로 #1](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_FSCE_Few-Shot_Object_Detection_via_Contrastive_Proposal_Encoding_CVPR_2021_paper.pdf)
