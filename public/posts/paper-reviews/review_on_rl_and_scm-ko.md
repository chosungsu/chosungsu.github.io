---
title: 'A review on reinforcement learning algorithms and applications in supply chain management'
date: '2024-04-08'
tags: ['scm', 'paper review']
---

### Abstract

공급망 의사결정은 높은 복잡성, 연속 및 이산 프로세스의 결합, 상호 의존적인 운영이라는 문제에 직면해 있습니다. 지능형 알고리즘은 adaptive data-driven 의사결정에서 새로운 잠재력을 열러주고 있습니다. 공급망 관리(SCM) 분야에서 강화 학습의 현재 최첨단 기술을 탐색하고 새로운 분류 프레임워크를 제안합니다. 이 프레임워크는 공급망 동인, 알고리즘, 데이터 소스 및 산업 분야를 기반으로 학술 논문을 분류합니다.

몇 가지 통찰력을 얻었는데 다음과 같습니다.

-고전적인 $Q$러닝 알고리즘이 여전히 가장 인기 있는 알고리즘입니다.

-재고 관리는 공급망 동기화의 핵심 요소입니다.

-대부분의 논문은 artificial data로 구현되는 toy like scm 문제를 다루고 있습니다.

따라서 향후 몇 년 동안 산업 규모로 이를 전환하는 것이 중요한 과제가 될 것입니다.

---

### Introduction

공급망은 점점 더 복잡하고 불확실한 환경에서 운영되고 있습니다. 이러한 환경에서 적응형 계획 및 제어는 불필요한 비용을 피하고 비즈니스 연속성을 유지하면서 최소한의 지연과 중단으로 최종 고객에게 배송을 보장하는 데 매우 중요합니다. 적응 기반 관리 원칙을 구현하기 위해서는 생산 스케줄링, 재고 관리, 배송 계획의 실시간 조정이 필요하며, 시스템의 제어 매개변수는 비용 최소화, 수익 극대화, 목표 서비스 수준 충족 또는 기타 정량화 가능한 목표를 달성하기 위해 역동성, 비정상성 및 불확실성을 고려하여 동적으로 조정되어야 합니다.

높은 수준의 복잡성을 감안할 때, 공급망은 운영 실패 및 정보 불일치로 인한 중단 및 최적 이하의 성능에 취약합니다. 가장 주목할 만한 파괴적 현상의 예로는 채찍 효과(bullwhip effect)와 파급 효과(ripple effect)가 있습니다. 채찍 효과는 Forrester 효과로도 널리 알려져 있으며, 공급망에서 하류로 전파될 때 생산 및 주문량의 수요 변동이 증폭되는 것으로 정의될 수 있습니다 (Xun Wang and Disney 2016). 반면, 파급 효과는 공급망의 한 부분에 국한되지 않고 하류로 파급되어 전체 공급망의 성능을 저해할 때 발생합니다 (Dolgui, Ivanov, and Rozhkov 2019). 두 가지 효과 모두 궁극적으로 과잉 생산 및 과소 생산 주기를 유발하여 과도한 재고 수준, 잠재적인 재고 부족 및 최적 이하의 네트워크 성능으로 이어지기 때문에 공급망 관리자에게 상당한 문제를 야기합니다. 이러한 문제는 공급망의 구조적 및 운영적 취약점이 상호 연결되어 있으면 더욱 악화될 수 있습니다.

지난 10년간은 데이터 양과 처리 능력의 증가를 보여주었습니다. 이러한 추세는 결국 방대한 컴퓨팅 자원을 활용하고 막대한 양의 데이터를 활용할 수 있는 머신러닝 접근 방식인 딥러닝(deep learning)의 출현으로 이어졌습니다 (Bengio 2016). 이러한 사실은 새로운 알고리즘 기술, 성숙한 소프트웨어 패키지, 그리고 비즈니스 분야의 강한 관심과 함께 강화 학습과 딥러닝의 유망한 조합인 딥 강화 학습(DRL)로 이어졌습니다 (Krakovsky 2016). DRL은 그 참신함에도 불구하고, 이미 도로 교통 내비게이션 (Vinitsky et al. 2020), 자율 주행 차량 (Isele et al. 2018), 로봇 공학 (Gu et al. 2017) 분야에서 놀라운 성능을 보여주었습니다. 그러나 RL 에이전트는 물리적 세계에서 직접 학습할 수 없다는 점을 강조하는 것이 중요합니다.

---

### Reinforcement learning

RL 패러다임은 지능형 시스템이 유사한 시행착오 과정을 통해 학습할 수 있다는 아이디어에 의해 구동됩니다. 이 아이디어는 공급망과 같은 고도로 복잡하고 동적인 시스템의 적응 제어에 적용될 때 매우 강력합니다 (Kegenbekov and Jackson 2021).

#### 마르코프 결정 과정(Markov Decision Process, MDP)을 통한 RL 정의

mdp는 목표 지향 학습을 위한 프레임워크로 $M = (S, A, P(s_{t+1}, r | s, a), R, \gamma)$로 정의됩니다. 여기서 $s$는 환경의 가능한 상태집합이고 $a$는 에이전트가 상호작용하는 행동집합, $P$는 전이할 확률, $\gamma$는 미래를 결정하는 discount factor입니다.

상태, 행동, 보상의 순서는 궤적(trajectory)를 생성하며 종료상태는 $s_n$을 나타냅니다. 에이전트의 목표는 상태를 행동으로 매핑하는 최적 정책 $\pi=S*A$를 찾아 누적 예상 수익을 최대화하는 것입니다.

#### MDP 한계와 DRL의 필요성

MDP 프레임워크가 완벽하지 않다는 점을 언급할 가치가 있습니다. 즉, 보상을 받기 전에 상태와 행동의 긴 궤적이 있을 경우, 어떤 행동이 최종 보상에 기여했는지 불분명합니다. 수행된 행동이 실제로 얻은 보상에 기여했는지 종종 불분명하기 때문에, 표준 해결책은 $n$-단계 할인 수익(n-step discounted returns)을 적용하는 것입니다.

여기서 $n$단계 동안의 행동 $a_t$에 대한 누적 보상은 할인 인자 $\gamma \in [0,1]$에 의해 지수적으로 가중됩니다.

심층(deep) 부분은 가능한 행동 시퀀스를 추정하고 이를 장기 보상과 연결하여 해결 공간의 관리 가능성을 높이기 위해 인공 신경망을 적용하는 것을 의미합니다. 또한, DRL 기반 에이전트는 모든 상태-값 쌍을 테이블에 저장하지 않으므로, 이전에 본 적이 없는 상태, 즉 훈련 단계에서 마주치지 않았던 상태의 값을 일반화할 수 있습니다.

---

### Framework

#### 1. supply chain driver

모든 공급망은 생성된 전체 가치를 최대화하는 것을 목표로 합니다. 이는 최종 제품의 가치와 공급망에서 발생한 비용 간의 차이입니다 (Chopra and Meindl 2013). 이 목표를 달성하는 방법은 하나만 있는 것이 아니라, 생성된 전체 가치에 영향을 미치는 다양한 상호 연결된 supply chain driver들이 있습니다. 시설(facilities), 재고(inventory), 운송(transportation), 정보(information), 소싱(sourcing), 가격 책정(pricing). 이 섹션은 이러한 동인들로부터 RL 모델을 위한 분류 프레임워크를 도출합니다. RL은 일반적으로 최적화 문제를 해결하는 데 사용되므로, 일반적으로 하나 이상의 동인을 조정하여 설정이 공급망 성능을 향상시키도록 합니다.

재고는 원자재, 재공품 및 완제품에 영향을 미치는 결정(예: 주문 관리 및 안전 재고 및 주기 재고 결정)을 포함합니다 (Chopra and Meindl 2013). 재고 관리 또는 보충(replenishment)은 SCM에서 잘 알려진 최적화 문제입니다. 이는 자재 가용성과 재고 비용 사이의 균형을 찾는 것을 목표로 언제 주문하고 얼마나 주문할 것인가에 대한 질문을 다룹니다. 실제 공급망에서는 변화하는 수요, 알 수 없는 리드 타임, 중단 및 불완전한 정보를 처리하는 것이 주요 과제입니다. RL 접근 방식은 고정된 주문량, 안전 재고 및 주기 재고를 가진 전통적인 재고 모델과 달리 적응적이고 상황에 따라 달라지는 주문 전략을 학습할 수 있다는 장점을 가집니다.

운송은 공급망 내에서 재고가 한 장소에서 다른 장소로 이동하는 것을 포함합니다 (Chopra and Meindl 2013). 차량 경로 문제(Vehicle Routing Problem) 또는 외판원 문제(Traveling Salesman Problem)와 같은 운송 문제는 학계에서 널리 퍼져 있지만, SCM 맥락에서만 고려되는 것은 아닙니다. 운송 문제는 일반적으로 여러 노드 간의 최단 또는 최속 경로를 찾는 것을 목표로 합니다. 또한 이러한 문제는 용량 제약, 시간 창 제약 또는 픽업 및 배송 제약과 같은 여러 제약 조건을 가질 수 있습니다. 전통적으로 차량 경로 문제는 계산 집약적인 제약 최적화, 휴리스틱 또는 메타휴리스틱을 사용하여 해결됩니다. RL은 더 적은 계산 시간으로 같거나 더 나은 결과를 제공할 것으로 기대됩니다.

소싱은 공급망 참여자의 조직, 특히 공급업체 선정(supplier selection) (Cavalcante et al. 2019) 및 세분화, 그리고 아웃소싱 및 인소싱 결정을 다룹니다 (Chopra and Meindl 2013). 많은 모델이 어느 정도 소싱 결정과 재고 결정을 모두 고려합니다. 따라서 두 클래스 간의 전환은 모호하며, 분류 프레임워크를 위해 이들의 범위를 구별하는 것이 중요합니다.

#### 2. algorithm

모든 RL 알고리즘은 에이전트가 환경 모델에 직접 접근할 수 있는지 여부에 따라 모델 기반(model-based)과 모델 프리(model-free)로 나눌 수 있습니다. 이 맥락에서 환경 모델은 상태 전이와 보상을 예측하는 함수를 의미합니다. 가장 유명한 모델 기반 접근 방식은 자가 플레이를 통해 고전 보드 게임을 마스터할 수 있었던 DRL 알고리즘인 AlphaZero입니다 (Silver et al. 2017).

일반적인 문제는 에이전트가 모델의 편향을 발견하고 악용할 수 있다는 점인데, 이는 학습된 모델에서는 잘 작동하지만 실제 환경에서는 최적이 아닌 정책으로 이어집니다. 이것이 바로 문제가 확률성, 장기 계획 기간, 부분 관측 가능성 및 불완전한 정보로 특징지어지는 경우 모델 기반 접근 방식이 거의 적용되지 않는 이유입니다. 위의 모든 특성이 복잡한 공급망에서 흔하기 때문에, 검토된 모든 연구가 모델 프리 RL을 활용했다는 점이 이해가 됩니다. 모델 프리 RL은 다시 정책 최적화(policy optimization)와 가치 기반(value-based) 알고리즘의 두 가지 주요 범주로 나뉩니다.

가치 기반 알고리즘은 최적 정책 $\pi(s)$를 찾기 위해 가치 기반 방법은 테이블에 저장된 것처럼 상태를 해당 행동에 매핑하는 매개변수 θ로 매개변수화된 근사치 $Q_\theta(s, a)$를 도출합니다. 이 최적화는 **오프-정책(off-policy)**으로 수행됩니다. 이는 각 업데이트가 훈련 중 어느 시점에서든 수집된 데이터를 활용한다는 것을 의미합니다. 결과적으로 정책 $\pi^$는 최적의 행동-가치 함수 $Q^(s, a)$에 의해 근사화되며, 에이전트는 $a(s)=\text{argmax}_a Q_{\theta}(s,a)$에 따라 행동을 취합니다.

다음으로 정책 최적화 알고리즘은 명시적으로 $\pi_\theta(a | s)$로 나타냅니다. 매개변수 θ는 일반적으로 기대 수익 $J(\pi_\theta)$에 대한 **경사 상승법(gradient ascent)**을 통해 직접 최적화됩니다. 이 최적화는 **온-정책(on-policy)**으로 수행됩니다. 이는 각 정책 업데이트가 정책의 최신 버전을 활용하는 동안 수집된 데이터를 기반으로 수행된다는 것을 의미합니다.

---

### Insights

RL 알고리즘이 다양한 SCM 작업에서 큰 잠재력을 보인다는 것을 보여주지만, 우리의 조사는 SCM 애플리케이션을 위한 RL 연구가 실질적인 산업적 관점에서는 여전히 초기 단계에 있다는 결론을 내립니다.

연구자들이 RL 애플리케이션을 개발하고 시범 운영하는 데 사용할 수 있는 실제 산업 공급망의 공개적으로 사용 가능한 데이터셋이 거의 없다는 데이터 획득 측면에서 실제 공급망에서 RL 훈련 에이전트를 의사결정에 배포할 때 훈련 후의 데이터에 실시간으로 지속적으로 접근 가능해야 하지만 종종 회사들은 경쟁 우위에 대한 우려 때문에 내부 데이터를 공개하는 것을 꺼려합니다.

RL에 의해 훈련된 에이전트 모델은 실시간으로 데이터 기반 및 자율적인 의사결정을 가능하게 합니다. 따라서 RL은 공급망의 최신 정보를 기반으로 대체 결정을 결정함으로써 공급망의 갑작스러운 방해에 대한 대응을 가능하게 합니다. 둘째, RL 방법은 갑작스러운 방해 발생 시 대체 결정을 계산하고 공급망 데이터의 점진적인 변화 시 제어 전략을 조정할 수 있습니다. 이를 통해 RL은 SCM을 위한 탄력적인 의사결정 지원 시스템 개발의 기반을 제공합니다. 셋째, RL 방법은 의사결정 정책을 학습할 때 인간의 경험을 고려하므로 인간 중심의 의사결정을 촉진합니다. 예를 들어, 이러한 정책은 에이전트가 계산한 결정이 적용되기 전에 인간이 먼저 확인해야 한다는 것을 구현함으로써 달성될 수 있습니다. 결정의 확인 또는 거부는 추가 보상으로 사용될 수 있으며, 따라서 에이전트의 학습 신호로 사용될 수 있습니다.

---

### Conclusion

본 논문에서는 여러 클래스와 서브클래스를 가진 네 가지 기준에 따라 SCM에 대한 RL 응용 프로그램을 분류하는 계층적 분류 프레임워크를 제안했습니다. 그리고 SCM에서 RL의 알고리즘, 응용 프로그램 및 실제 채택을 이해하기 위해 반체계적 검토를 수행했습니다. 결과적으로 SCM에서 RL 응용 프로그램이 2000년으로 거슬러 올라간다는 것을 보여줍니다. 출판 동향은 꾸준했지만, 2019년에 출판물 수가 급격히 증가했으며, 이는 컴퓨팅 하드웨어, 증가하는 데이터 양 및 딥러닝의 출현에 기인할 수 있습니다.

---

### 참고 자료

[원본 경로 #1](https://www.tandfonline.com/doi/epdf/10.1080/00207543.2022.2140221?needAccess=true)



