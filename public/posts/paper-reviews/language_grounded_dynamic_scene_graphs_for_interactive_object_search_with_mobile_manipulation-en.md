---
title: 'Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation'
date: '2025-10-18'
tags: ['embodied ai', 'paper review']
---

### Abstract

To fully utilize the capabilities of mobile manipulation robots, they must be able to autonomously execute long-horizon tasks in large, unexplored environments.

While Large Language Models (LLMs) have demonstrated novel reasoning abilities for arbitrary tasks, existing work primarily focuses on explored environments and typically concentrates on either navigation or manipulation tasks in isolation.

We propose a new approach called MoMa-LLM, which grounds language models in structured representations derived from open-vocabulary scene graphs that update dynamically as the environment is explored. These representations are tightly intersected with an object-centric action space. Given object detections, the resulting approach is zero-shot, open vocabulary, and easily extensible to a wide range of mobile manipulation and household robotics tasks.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/c702b114-2cef-422c-a88a-b956cfb9cc33/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

Embodied AI tasks that interact within large, unexplored, human-centric environments require reasoning over long horizons and numerous objects. In many cases, the environments under consideration are not known a priori or are continuously rearranged.

#### Limitations and Challenges in Prior Work

Recent advances have demonstrated the potential of Large Language Models (LLMs) to generate high-level plans. However, these efforts have mainly focused on fully observed environments (e.g., tabletop manipulation) or pre-explored scenes and struggle to produce feasible, grounded plans suitable for real robot execution. These issues become significantly worse in large scenes with many objects and long temporal horizons.

#### Proposed MoMa-LLM Approach

To address this, we ground LLMs in dynamically constructed scene graphs. This integrates a scene understanding module that builds open-vocabulary scene graphs from maps and Voronoi diagrams given object detections. These diverse representations are then tightly intersected with an object-centric action space.

---

### Methods

#### Environment and Agent Setup

The robot agent is situated in a large, unexplored environment and must complete a given task described by a language goal $\mathbf{g}$. It acts within a Partially Observable Markov Decision Process (POMDP) $\mathcal{M} = (S, A, O, T(s'|s, a), P(o|s), r(s, a))$.

#### Framework

<img src="https://velog.velcdn.com/images/devjo/post/9136065f-8b88-4238-9536-43f7205beb59/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

To provide structured inputs to the LLM, we create hierarchical scene graphs that include navigation Voronoi graphs. In dynamic RGB-D mapping, the agent perceives pose-annotated $\text{RGB-D}$ frames $\{I^0, \dots, I^t\}$ from the environment containing semantics. The map is dynamically updated based on newly explored areas or occurrences of object dynamics in the scene.

The Voronoi graph is abstracted from a dense map generated by computing an exploration graph $G_V$. For robustness, we inflate $\text{B}_t$ using the Euclidean Signed Distance Field (ESDF) formulation but overwrite free-space coordinates with zeros as given in $\text{F}_t$. Based on this, we compute the Generalized Voronoi Diagram (GVD) holding the set of points $V$ with equal clearance from the nearest obstacles in $\text{B}_t$. We prune nodes that are in the immediate vicinity of obstacles or do not reside within $\text{B}_t$. Given the GVD boundaries, we construct edges $E$ between $V$ to obtain the navigation Voronoi graph $G_V = (V, E)$.

In the high-level action space consisting of navigate, go to and open, close, explore, and done, ambiguity for multiple instances of a class in a room is resolved by choosing the nearest instance. Subsequently, sub-policies produce actions in the low-level action space and return upon success or failure.

---

### Conclusion

In this study, we developed a method for grounding language models for high-level reasoning with scalable, dynamic scene graphs and efficient low-level policies for interactive tasks requiring combined reasoning over manipulation, navigation, and exploration.

We demonstrated that extracting structured knowledge about large, unexplored scenes is critical for enabling LLMs to reason about efficient search strategies, outperforming fully learned or co-occurrence-based methods.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2403.08605)
