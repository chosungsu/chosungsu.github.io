---
title: 'Time Series Anomaly Detection via Reinforcement Learning-Based Model Selection'
date: '2022-09-05'
tags: ['time-series', 'paper review']
---

### Abstract

시계열 이상 탐지(Time series anomaly detection)는 실제 시스템의 신뢰할 수 있고 효율적인 운영을 위해 매우 중요하다고 인식되어 왔습니다. 이상 현상 특성에 대한 다양한 가정을 기반으로 많은 이상 탐지 방법들이 개발되었습니다. 그러나 실제 데이터의 복잡한 특성으로 인해, 시계열 내의 서로 다른 이상 현상들은 일반적으로 다양한 프로필을 가지며, 이는 서로 다른 이상 가정들을 뒷받침합니다. 이러한 이유로 다른 모델들을 지속적으로 능가할 수 있는 단일 이상 탐지기를 찾기가 어렵습니다.

본 연구에서는 다양한 기반 모델(base models)의 이점을 활용하기 위해, 강화 학습 기반 모델 선택 프레임워크를 제안합니다. 구체적으로, 먼저 다양한 이상 탐지 모델 풀(pool)을 학습하고, 이 기반 모델들 중에서 강화 학습을 활용하여 후보 모델을 동적으로 선택합니다. 실제 데이터에 대한 실험을 통해 제안된 전략이 전반적인 성능 측면에서 모든 기준 모델(baseline models)을 실제로 능가할 수 있음을 입증했습니다.

---

### Introduction

사이버-물리 시스템 기술의 필수적인 부분으로서, 스마트 그리드 기술은 고급 인프라, 통신 네트워크 및 컴퓨테이션 기술을 활용하여 전력망의 안전과 효율성을 향상시키는 것을 목표로 합니다. 최근 머신러닝의 발전과 함께 다양한 머신러닝 기술은 이미 지속 가능한 에너지 관리, 전력 부하 예측, 전기차 충전 예측, 전기차 충전 스케줄링 등 이 분야에서 응용 사례를 찾았습니다. 사이버-물리 시스템의 운영 상태를 결정하는 데 중요한 이상 탐지(Anomaly detection) 또한 이 분야의 잠재적인 응용 분야 중 하나이며 본 논문의 주요 주제가 될 것입니다.

이상(Anomalies), 또는 특이점(outliers)은 "데이터 집합의 나머지 부분과 일치하지 않는 것으로 보이는 관측치" 또는 "[대부분의 시퀀스 패턴이나 분포에서 벗어나는] 데이터 포인트"로 정의됩니다. 이상 현상의 발생은 일반적으로 시스템의 잠재적 위험을 나타냅니다. 즉, 전력망의 비정상적인 계량기 측정은 오작동 또는 가능한 사이버 공격을 시사할 수 있습니다. 금융 시계열의 이상 현상은 "사기, 위험, 신분 도용, 네트워크 침입, 계정 탈취 및 자금 세탁과 같은 불법 활동"을 나타낼 수 있습니다. 따라서 이상 탐지는 시스템의 운영 보안을 보장하는 데 중요하며, 의료 시스템, 온라인 소셜 네트워크, 사물 인터넷(IoT), 스마트 그리드 등과 같은 분야에서 응용 사례를 볼 수 있습니다.

이상은 발생 빈도가 낮거나 대부분의 데이터와 멀리 떨어져 있거나 낮은 밀도 영역에서 발생할 가능성이 가장 높습니다.

본 연구에서는 강화 학습 기반 이상 탐지 모델 선택(RLMSAD) 프레임워크를 제안합니다. 입력 시계열 관측 및 각 기반 모델의 예측을 기반으로 각 시간 단계에서 최적의 탐지기를 선택하는 것을 목표로 합니다. 실제 데이터셋인 SWaT(Secure Water Treatment)에 대한 실험은 제안된 프레임워크가 모델 정밀도 측면에서 각 기반 탐지기보다 우수함을 보여줍니다.

---

### Background

#### 시계열 비지도 이상 탐지

시계열 $X = {x_1, x_2, \dots, x_t}$는 시간 순서로 인덱싱된 데이터 시퀀스입니다. 각 $x_i$가 스칼라인 단변량(uni-variate)이거나, 각 $x_i$가 벡터인 다변량(multivariate)일 수 있습니다. 본 논문에서는 다변량 시계열의 이상 탐지 문제를 비지도(unsupervised) 설정에서 다룹니다.

훈련 시퀀스 $X_{train}$은 정상 인스턴스만 포함하는 시계열이며, 테스트 시퀀스 $X_{test}$는 이상 인스턴스로 오염되어 있습니다. 훈련 단계에서는 이상 탐지기가 $X_{train}$에서 사전 훈련되어 정상 인스턴스의 특성을 포착합니다. 테스트 중에는 탐지기가 $X_{test}$를 검사하고 각 인스턴스에 대한 이상 점수(anomaly score)를 출력합니다.

#### 마르코프 결정 과정 및 강화 학습

강화 학습(Reinforcement Learning, RL)은 순차적 의사 결정 문제를 다루는 머신러닝 패러다임 중 하나입니다. 이는 총 보상을 최대화하여 환경에서 최적의 행동을 발견하도록 에이전트를 훈련하는 것을 목표로 하며, 일반적으로 마르코프 결정 과정(Markov Decision Process, MDP)으로 모델링됩니다.

표준 MDP는 튜플 $M=⟨S,A,P,R,\gamma⟩$로 정의됩니다. 여기서 $S$는 상태 집합, $A$는 행동 집합, $P(s'|s, a)$는 상태 전이 확률 행렬, 그리고 $R(s, a)$는 보상 함수입니다. $\gamma$는 보상 계산을 위한 할인 인자(discount factor)이며 일반적으로 0과 1 사이의 값을 가집니다. 결정론적 MDP(Deterministic MDP)의 경우, 각 행동은 특정 상태로 이어집니다. 즉, 상태 전이 동역학이 고정되어 있으므로 행렬 $P(s'|s, a)$를 고려할 필요가 없으며, MDP는 $M=⟨S,A,R,\gamma⟩$로 나타낼 수 있습니다.

리턴(Return) $G_t=\sum_{k=t+1}^{T} \gamma^{k-t-1}R_k$는 현재 시간 단계 $t$ 이후의 누적 미래 보상입니다. 정책(Policy) $\pi(a|s)$는 현재 상태를 특정 행동을 선택할 가능성에 매핑하는 확률 분포입니다. 강화 학습 에이전트는 기대 총 리턴(expected total return)을 최대화하는 의사 결정 정책 $\pi(a|s)$를 학습하는 것을 목표로 합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/2c847358-d99c-4ea8-b32b-f3309e3cd066/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

모델의 워크플로우는 위 이미지와 같습니다. 시계열 입력은 sliding window를 사용하여 분할됩니다. 각 분할된 윈도우에서 마지막 타임스탬프는 분석을 위한 target이고 이전의 모든 타임스탬프는 입력 값으로 사용합니다. 각 후보 이상 탐지기는 먼저 훈련 세트에서 개별적으로 사전 훈련됩니다. 이후 경험적 이상 임계값을 결정해야 합니다.

#### 1. Characterizing the Performance of Base Detectors

후보 탐지기의 성능을 특성화하기 위해 아래의 두 점수를 제안합니다. 각 이상 탐지기는 테스트 데이터에 대해 일련의 이상 점수를 생성합니다. 이상 점수가 높을수록 이상 가능성이 높다는 것을 나타냅니다.

임계값까지의 거리 신뢰도는 $\frac{\text{score}-\text{threshold}}{\text{maxscore}-\text{minscore}}$로 제안되며 앙상블 학습의 다수결 투표에서 영감을 받아 예측 일치 신뢰도는 $\frac{\text{same label}}{\text{count of model in pool}}$로 제안하여 동일한 예측을 제공하는 모델 수를 전체 후보 모델 수로 나눈 값으로 계산됩니다.

#### 2. Markov Decision Process (MDP) Formulation

모델 선택 문제는 아래 방식으로 마르코프 결정 과정으로 공식화합니다. 상태 전이 확률 $P(s'|s)$는 $s_t$에서 $s_{t+1}$로의 모든 즉각적이고 연속적인 상태 쌍에 대해 1이고 다른 경우에는 0입니다.

상태 공간은 모델 풀과 동일한 크기입니다. 각 상태를 선택된 이상 탐지기로 간주하고 스케일링된 이상 점수와 이상 임계값, 이진 예측 레이블, 임계값까지의 거리 신뢰도, 예측 일치 신뢰도가 포함됩니다.

보상 함수는 예측된 레이블과 실제 레이블 간의 비교를 기반으로 결정합니다.

$$
R=\begin{cases}
r_1 & \text{for TP} \\
r_2 & \text{for TN} \\
-r_3 & \text{for FP} \\
-r_4 & \text{for FN}
\end{cases}
$$

보상은 에이전트가 동적 환경에서 적절한 모델 선택을 하도록 설계되었기 때문에 본 논문에서는 이상을 정상으로, 정상을 이상으로 간주합니다. TP는 예측된 레이블과 실제 레이블 모두 이상인 경우, TN은 예측된 레이블과 실제 레이블 모두 정상인 경우, FP는 예측된 레이블은 이상이지만 실제 레이블은 정상인 경우, FN은 예측된 레이블은 정상이지만 실제 레이블은 이상인 경우를 나타냅니다.

정상은 이상보다 대다수이며 따라서 본 논문에서는 정상을 올바르게 식별하는 것을 더 사소한 경우로 간주합니다. TN은 보상이 작아야 하고 TP에 대한 보상이 상대적으로 커야 합니다. $(\gamma_1 > \gamma_2)$이고 또한 모델은 더 많은 FP를 생성할 수 있지만 실제 이상을 간과할 위험을 줄일 수 있다는 장점이 있으므로 $(\gamma_4 > \gamma_3)$입니다.

---

### Experiments

평가에 사용한 데이터셋은 SWaT 데이터셋으로 산업용 수처리 테스트베드 내의 51개의 센서 및 액추에이터의 다변량 시계열 데이터입니다.

기본 모델로는 one-class svm, isolation forest, ecod, copod, usad와 같은 비지도 이상 탐지 알고리즘을 선택합니다. 평가 지표는 정밀도(precision), 재현율(recall), f1 score를 사용합니다.

데이터셋의 이상 비율이 12%에 가깝기 때문에 임계값도 12%로 고정하여 5개의 타임스탬프를 sliding window size로 설정합니다.

기본 모델의 정밀도 점수는 약 66%에서 75% 범위입니다. 모든 기본 모델은 약 63%의 재현율을 보였습니다. 기본 모델의 F1-점수는 약 65%에서 69% 범위입니다.

제안된 프레임워크 하에서, 전체 정밀도와 F1-점수 모두 상당히 증가했습니다. RLMSAD 하의 정밀도는 81.05%에 도달했으며, F1은 69.45%에 도달하여 이상 탐지 성능에서 실질적인 개선을 보였습니다.

FP와 FN에 대한 페널티의 영향을 조사하였을 때 FN을 고정하고 FP를 변화하면 오경보를 덜 보고하게 되고 예측에 대해 확신할 때만 이상을 보고하는 현상을 보입니다. 이는 FP 페널티가 증가함에 따라 재현율 점수의 일반적인 감소와 정밀도 점수의 증가로 입증될 수 있습니다. 반대로 FN을 증가시키면 모델이 더 대담하게 이상을 보고하는 현상을 보입니다. 따라서 정밀도 점수의 일반적인 감소와 재현율 점수의 증가로 입증될 수 있습니다.

---

### Conclusion

본 논문에서는 시계열 이상 탐지를 위한 강화 학습 기반 모델 선택 프레임워크를 제안했습니다. 구체적으로, 기본 모델의 탐지 성능을 특성화하기 위해 임계값까지의 거리 신뢰도(distance-to-threshold confidence)와 예측 일치 신뢰도(prediction-consensus confidence)라는 두 가지 점수를 도입했습니다. 그런 다음 이 두 점수를 RL 상태 변수로 사용하여 모델 선택 문제를 마르코프 결정 과정으로 공식화했습니다. 장기적인 예상 성능을 최적화하기 위해 이상 탐지 모델 선택 정책을 학습하는 것을 목표로 했습니다.

적응형 임계값 전략이 연구된다면 RL 에이전트에 더 유익한 상태 설명을 제공하고 잠재적으로 더 강력한 성능을 가져올 수 있습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2205.09884)
