---
title: 'ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model'
date: '2025-12-05'
tags: ['embodied ai', 'paper review']
---

### Abstract

<img src="https://velog.velcdn.com/images/devjo/post/c3e0799c-79a1-420a-a4ed-3892f2520b6a/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

비전-언어-행동 (VLA) 모델의 기존 훈련 패러다임에 대한 체계적인 분석을 통해 두 가지 주요 도전 과제를 식별합니다. 로봇 훈련이 결정적인 시각-텍스트 정렬을 덮어쓰는 가짜 망각과 경쟁적인 제어와 이해 작업이 공동으로 훈련될 때 성능을 저하시키는 작업 간섭이 있습니다.

이러한 한계를 극복하기 위해 제안된 ChatVLA는 초기 제어 숙련도 후 다중 모드 데이터를 점진적으로 통합하는 단계적 정렬 훈련과 작업 간섭을 최소화하기 위한 Mixture of Experts 아키텍처를 특징으로 합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/b53f7591-03e5-4319-845f-d4fa61d5309d/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

최근 비전-언어-행동 (VLA) 모델의 발전은 주로 로봇 행동 숙련도를 우선시해 왔습니다. 로봇 제어 작업에 대해 훈련된 모델은 저수준 조작과 물리적 상호 작용에서는 뛰어나지만, 이미지와 텍스트와 같은 다중 모드 데이터를 해석하고 추론하는 데 종종 어려움을 겪습니다. 사전 훈련된 비전-언어 모델은 인상적인 다중 모드 장면 이해를 입증하지만 환경과 물리적으로 상호 작용하는 능력이 부족합니다. 이러한 이중성은 결정적인 도전 과제를 강조합니다.

본 연구는 다중 모드 장면 이해, 대화 능력, 물리적 상호 작용이 가능한 단일 종단 간 신경망을 통합하는 방법을 탐구합니다. 로봇 행동 궤적을 포함하는 전문가 시연 데이터에만 훈련하는 것에는 OpenVLA가 대표적이고 행동을 안내하기 위해 로봇 데이터를 추론 문구로 증강하는 것에는 ECoT, DiffusionVLA가 있으며 시각-텍스트 쌍과 로봇 데이터 모두로 공동 훈련하는 것에는 RT-2가 있습니다.

모델은 먼저 구현된 제어를 숙달한 후 동결된 정렬 링크를 재활성화하기 위해 다중 모드 데이터를 점진적으로 통합합니다. 더 나아가, MLP 계층에 MoE를 도입합니다. 이것은 두 작업이 주의 계층을 공유하도록 허용하면서 작업별 MLP를 격리합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/34b615cc-92b7-4e08-bbf6-35eef7d524ed/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### Architecture

로봇 제어의 맥락에서 일반적으로 시연 데이터셋 $\text{D}_{\text{robot}} = \{\tau_i\}_{i=1}^{N}$을 구성하며, 각 시연 $\tau_i$는 상태-행동 쌍의 시퀀스를 포함합니다. 상태 $s$는 관찰 (이미지) $v$와 지침 (텍스트) $t$로 구성되며 $\text{s} = (v, t)$입니다. 상태-행동 쌍의 시퀀스를 $\tau_i = \{((v_1, t_1), a_1),((v_2, t_2), a_2), \dots ,((v_T, t_T), a_T)\}$로 표현할 수 있으며, 여기서 각 튜플 $((v_j, t_j), a_j)$는 시점 $j$에서의 상태와 취해진 해당 행동을 나타내며, $T$는 시연의 길이입니다.

다중 모드 이해와 시각적 대화 작업을 위해 데이터셋 $\text{D}_{v-t} = \{\phi_i\}_{i=1}^{M}$을 가지며 각 데이터 샘플 $\phi_i$는 시각적 이미지 $v_i$와 텍스트 형식의 해당 질문 $t'_i$로 구성됩니다. 여기서 $M$은 그러한 이미지-텍스트 쌍의 총 수를 나타냅니다.

목표는 두 분포를 효과적으로 학습할 수 있는 통합 모델을 만들어 로봇 제어 작업과 다중 모드 이해 시나리오 모두에서 잘 수행할 수 있도록 하는 것입니다.

#### ChatVLA

로봇 데이터에만 배타적으로 훈련하는 것은 시각-텍스트 정렬을 감소시켜 모델의 대화 능력을 저하시킬 수 있습니다. 먼저 가짜 망각을 해결하는 데 사용되는 훈련 전략을 설명한 후 두 번째 도전 과제를 해결하기 위한 우리의 방법의 일반 아키텍처를 설명할 것입니다.

단계적 정렬 훈련 측면에서 가짜 망각이 VLA가 채팅하고 장면을 이해하는 능력을 잃게 하는 주요 요인임을 식별했습니다. 사전 훈련된 VLM이 잘 훈련되어 시각 관련 작업에 뛰어나기 때문에, 적은 양의 시각-텍스트 쌍 데이터로 채팅하고 장면을 이해하는 능력을 재활성화할 수 있다는 것이 직관적입니다. 대조적으로, 로봇 제어 작업은 훈련하기에 훨씬 더 복잡하므로, 구현된 제어 작업에 뛰어난 우수한 모델을 개발하는 것이 우선 순위가 되어야 합니다.

훈련된 VLM으로부터 지식을 유지하도록 모델을 가능하게 하는 가짜 망각 문제를 해결하기 위한 단계적 정렬 훈련의 사용을 입증했습니다. 그러나 이 접근 방식은 모델이 여전히 시각-텍스트와 로봇 데이터 모두에 대해 공동 훈련해야 하므로 작업 간섭 문제를 완전히 해결하지 못합니다. 입력은 $\text{D}_{\text{robot}}$ 또는 $\text{D}_{v-t}$에 속할 수 있습니다. 주목할 만한 점은 이중 라우터를 설계하여 하나는 다중 모드 이해와 대화에 관한 작업을 처리하고 ($\text{f}(\text{FFN}_{v-t})$), 다른 하나는 로봇 제어에 대한 표현을 학습합니다 ($\text{f}(\text{FFN}_{\text{robot}})$). 입력은 먼저 다중 헤드 자기 주의 $x^{l\prime} = \text{MHA}(x^{l-1}) + x^{l-1}$를 통과하며, 여기서 $\text{MHA}(\cdot)$는 다중 헤드 자기 주의를 나타냅니다.

---

### Conclusion

비전-언어-행동 (VLA) 모델에서 구현된 제어와 다중 모드 이해를 통합하는 것은 어렵습니다. 왜냐하면 현재 방법이 종종 하나를 다른 하나를 위해 희생하기 때문입니다. 로봇 전용 훈련은 대화 능력을 저하시키고, 시각-텍스트 공동 훈련은 가짜 망각과 작업 간섭으로 인해 제어 성능을 감소시킵니다.

단계적 정렬 훈련과 MoE를 결합한 ChatVLA는 3.5배 더 적은 매개변수로 실제 로봇 제어에서 뛰어난 성능을 보입니다.

---

### 참고 자료

[원본 경로 #1](https://aclanthology.org/2025.emnlp-main.273.pdf)
