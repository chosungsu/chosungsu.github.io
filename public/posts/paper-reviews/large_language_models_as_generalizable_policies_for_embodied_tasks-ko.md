---
title: 'Large Language Models As Generalizable Policies For Embodied Tasks'
date: '2025-10-14'
tags: ['embodied ai', 'paper review']
---

### Abstract

대규모 언어 모델 (LLM)이 구체화된 시각 작업을 위한 일반화 가능한 정책으로 적응될 수 있음을 보여줍니다.

LLaRP (Large LAnguage model Reinforcement Learning Policy)이라고 불리는 접근 방식은 사전 훈련된 고정된 LLM을 텍스트 명령과 시각적 1인칭 관찰 (egocentric observations)을 입력으로 받고 환경에서 직접 행동을 출력하도록 적응시킵니다. 강화 학습을 사용하여 환경 상호 작용을 통해서만 보고 행동하도록 훈련합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/9e09912b-b67e-4237-9c9b-af7c305bedab/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### 대규모 언어 모델 (LLMs)의 능력

대규모 언어 모델 (LLM)은 방대한 양의 텍스트 데이터로 훈련된 수십억 개의 매개변수 모델로 특징지어지며, 전례 없는 언어 이해 능력을 입증해 왔습니다. 나아가, LLM은 핵심 언어 이해 문제를 넘어 대화 시스템, 시각적 이해 문제, 추론, 코드 생성, 구체화된 추론, 로봇 제어와 같은 강력한 능력을 보여주었습니다. 이러한 능력은 종종 각각의 능력을 위한 전용 훈련 데이터 없이 제로샷 방식으로 나타나며, LLM이 수많은 도메인에 적용될 만큼 일반적이고 광범위한 지식을 포함하고 있음을 시사합니다. 더욱이, 이러한 능력은 이러한 도메인의 입력 및 출력 공간이 종종 언어로 자연스럽게 표현되지 않음에도 불구하고 (예: 입력으로서의 이미지, 출력으로서의 로봇 명령) 나타납니다.

#### LLM과 구체화된 AI의 통합 과제

구체화된 AI의 핵심 목표는 새로운 작업으로 전이될 수 있는 일반화 가능한 의사 결정이므로 LLM의 일반화 능력이 구체화된 문제에 통합될 수 있는지 묻는 것은 자연스럽습니다.

기존의 발전은 정적인 전문가 데이터셋에 의존해 왔는데 이는 엄청나게 크고 비용이 많이 드는 다양한 전문가 데이터를 필요로 합니다. 반대로, 구체화된 AI 시뮬레이터는 에이전트가 직접적인 상호 작용, 탐색, 및 보상 피드백을 통해 환경으로부터 학습할 수 있도록 합니다. 그러나 이러한 에이전트의 많은 수의 새로운 구체화된 작업에 대한 일반화 능력은 앞서 언급된 도메인의 능력에 미치지 못합니다.

#### RL을 통한 LLM 적응

LLM은 제어 도메인이 자연어일 때 온라인 설정에서 적용 가능함을 보여주었습니다. 본 연구에서 시각-언어 정책으로서 강화 학습을 통해 성공적으로 적응될 수 있음을 보여줍니다.

첫째 사전 훈련되고 고정된 LLM을 학습된 입력 및 출력 어댑터 계층을 가진 시각-언어 모델 (VLM) 정책으로 사용하는 것이 강력한 일반화 능력을 나타내는 정책으로 이어진다는 것을 보여줍니다.

의역 강건성(Paraphrastic Robustness, PR)은 에이전트가 명령의 "의도"가 변하지 않는 언어적 변형 하에서 동일한 최적 행동을 생성합니다. 이는 동일한 행동을 설명하는 새로운 방식이나 보았던 객체를 언급하는 새로운 방식을 포함합니다.

행동 일반화(Behavior Generalization, BG)는 에이전트가 새로운 최적 행동을 요구하는 작업을 해결합니다. 이는 원하는 행동 결과가 훈련 중에 보았던 것과 구별됨을 의미합니다.

---

### Methods

#### Problem Formulation

부분적으로 관찰 가능한 마르코프 결정 과정으로 공식화될 수 있으며 기저 상태 공간 $S$, 관찰 공간 $O$, 행동 공간 $A$, 전이 분포 $P$, 보상 함수 $R$, 초기 상태 분포 $\rho_0$ 및 할인 인자 $\gamma$의 튜플로 정의됩니다.

관찰 공간은 1인칭 관찰로 장면의 일부만 관찰하는 로봇의 RGB 카메라와 같은 고차원 시각적 관찰입니다.

목표 분포 $G$를 포함하는 확장과 $s \in S$ 및 $g \in G$에 대해 보상 $R(s, g)$로 공식화되는 경우를 고려합니다. 목표 조건부 정책 $\pi(a|o, g)$를 학습하여 할인된 보상의 합 $E_{s_0 \sim \rho_0, g \sim G} [\sum_t \gamma^t R(s_t, g)]$를 최대화하고자 합니다.

#### LLM Backbone

대규모 자기 회귀 텍스트 예측 모델인 대규모 언어 모델을 활용합니다. 일련의 토큰 $l$로 표현된 텍스트가 주어지면, LLM은 이전의 모든 토큰 $\pi_{\text{LLM}}(l_{K+1} | l_1, \ldots, l_K)$을 조건으로 하여 그 시퀀스의 각 토큰을 예측하도록 훈련됩니다.

구체화된 에이전트 정책은 시각적 관찰 $O$를 소비하고 행동 $A$를 예측해야 하며, 이들은 모두 언어 토큰이 아니므로, LLM의 입력 및 출력 계층을 제거합니다. 특히, LLM 입력 계층은 텍스트 토큰을 벡터 임베딩 $e_k = E_T(l_k) \in \mathbb{R}^D$로 인코딩하는 반면, 출력 계층은 단어를 분류합니다.

#### Overall Architecture

<img src="https://velog.velcdn.com/images/devjo/post/2b876595-a02b-4de0-8f7c-d99acc201037/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

LLaRP는 두 가지 입력 유형을 가집니다. 언어로 표현된 목표 $g = (l_1, \ldots, l_k) \in G$를 조건으로 합니다. 이 목표는 언어 인코더 $E_T^\theta$를 사용하여 $D$ 차원 벡터의 시퀀스로 임베딩될 수 있습니다. 정책 롤아웃 동안 별도의 학습 가능한 관찰 인코더 모듈 $E_V^\phi : O \mapsto \mathbb{R}^D$을 사용하여 인코딩되는 시각적 관찰 $o_1, \ldots, o_t$를 소비합니다.

관찰 인코더 모듈은 시각적 임베딩을 생성하는 비전 인코더와 시각적 임베딩을 언어 모델 토큰 임베딩 차원으로 투영하는 MLP 네트워크로 구성됩니다. 인코딩된 텍스트와 시각적 관찰은 $D$ 차원 임베딩의 $k + t$ 길이 시퀀스를 생성하며, 이는 위에서 정의한 LLM 백본 $\psi_{\text{LLM}}^\theta$의 입력이 됩니다.

출력으로 행동을 디코딩하기 위해 LLM 백본의 출력을 $A$의 행동에 대한 분포로 변환하는 학습 가능한 행동 출력 모듈 $D_\omega : \mathbb{R}^D \mapsto \mathbb{D}(A)$를 사용합니다. 두 개의 추가 어댑터 모듈 $D_\omega$ 및 $E_V^\phi$를 통해 LLM을 시간 $t$의 행동을 출력하기 위해 시간 $t$까지의 목표 명세와 시각적 관찰을 입력으로 받도록 적응시킬 수 있습니다. 행동 출력 모듈은 환경 행동에 대한 분포를 예측하는 MLP입니다.

훈련 동안 LLM 백본과 시각적 인코더를 고정합니다. 고정된 시각적 인코더는 다양한 환경으로 일반화될 수 있는 시각적 특징을 유지하는 데 도움이 됩니. 그리고 고정된 LLM 백본은 미세 조정하는 동안 손실될 수 있는 언어 추론 능력을 유지하는 데 도움이 됩니다.

#### Language Rearrangement Problem

많은 수의 언어 조건부 작업에 걸친 일반화 속성을 연구하기 위해, "Language Rearrangement"라고 불리는 새로운 문제를 소개합니다. 이는 "머그잔을 소파로 가져와", "모든 과일을 냉장고에 보관해", "배고파, 주방에서 테이블로 뭔가를 가져와"와 같은 가정 환경 작업의 많은 수를 포괄하려고 노력합니다. 이 문제 공간은 $150,000$개의 훈련 작업과 $1,000$개의 테스트 작업을 정의하고 각각에 대한 텍스트 명령을 제공함으로써 재배치 작업을 확장합니다. 이 작업들은 에이전트가 여러 객체 상호 작용 (집기, 놓기, 열기, 닫기), 객체 검색 및 논리적 추론 (예: "만약" 구문)을 요구하는 다양한 보지 못한 명령 개념으로 일반화하도록 요구합니다.

이 과정에서 에이전트는 보지 못한 집에서 시작하여 지정된 시작 위치에서 원하는 목표 위치로 객체를 이동하는 것으로 귀결되는 일반적인 가정 활동을 실행하도록 지시받습니다. 에이전트에게는 원하는 목표 상태를 지정하는 자연어 명령이 제공됩니다. 전체 작업 또는 하위 작업을 성공적으로 완료하면 희소 보상 (sparse reward)이 제공됩니다.

#### 의역 강건성 (PR)

자연어 명령으로부터 작업을 해결할 수 있는 모델을 훈련하는 궁극적인 목표는 인간이 구체화된 에이전트에게 쉽게 명령을 제공할 수 있도록 하는 것입니다. 그러나 인간은 작업을 설명하는 방식에서 높은 가변성을 보입니다.

의역 강건성은 에이전트가 언어적 변형 하에서 동일한 행동을 생성할 수 있는지 평가합니다. 이러한 변형에는 명령을 말하는 새로운 방식과 이름 대신 간접적으로 객체를 언급하는 것이 포함됩니다. 이러한 명령의 기저 목표는 훈련 데이터셋에 포함되어 있습니다.

#### 행동 일반화 (BG)

훈련 데이터셋에 존재하지 않는 언어 명령으로 명시된 새로운 유형의 행동을 에이전트가 입증해야 합니다.

---

### Conclusion

본 논문에서는 강화 학습을 사용하여 사전 훈련된 LLM을 구체화된 작업에 활용하는 방안인 LLaRP를 소개합니다.

샘플 효율성과 일반화 모두에서 사전 훈련되지 않은 Transformer 및 LSTM 기반 모델보다 뛰어난 성능을 보입니다. 향후 해결해야 할 한계에는 일반적인 RL 모델보다 훨씬 큰 LLM의 크기가 포함됩니다. 또한 행동 디코더 모듈을 훈련하는 것이 LLM으로부터 범용 지식을 활용하는 정책의 능력을 저해할 수 있습니다.

---

### 참고 자료

[원본 경로 #1](https://openreview.net/pdf?id=u6imHU4Ebu)
