---
title: 'DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions'
date: '2025-05-31'
tags: ['robotics', 'paper review']
---

### Abstract

객체의 물리학을 이해하는 것은 성공적인 객체 조작에 매우 중요합니다. 하지만 물리적 객체 속성은 객체의 정적인 모습(static appearance)만으로는 거의 추론할 수 없기 때문에 어려운 문제입니다. 본 논문에서 DensePhysNet이라는 시스템을 제안합니다. 이 시스템은 일련의 동적 상호작용(예: 미끄러짐 및 충돌)을 능동적으로 실행하고 시각적 관찰에 대한 심층 예측 모델(deep predictive model)을 사용하여 관찰된 객체의 물리적 속성을 반영하는 조밀한, 픽셀 단위의 표현을 학습합니다.

시뮬레이션과 실제 환경 모두에서의 실험은 학습된 표현이 풍부한 물리적 정보를 담고 있으며 마찰(friction)이나 질량(mass)과 같은 물리적 객체 속성을 직접적으로 해독하는 데 사용될 수 있음을 보여줍니다. 조밀한 표현(dense representation)의 사용은 DensePhysNet이 훈련 시점보다 더 많은 객체가 있는 새로운 장면에도 잘 일반화될 수 있도록 합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/fb45386a-bac5-4a01-82fa-573e8fbe6bcb/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

지능적인 조작은 물체의 재료를 구별하고 시각 정보로부터 물리적 속성을 추론하는 능력으로부터 이점을 얻습니다. 예를 들어, 테이블 위 물체를 재배치할 때 무거운 재료와 가벼운 재료를 구별할 수 있다면 더 나은 조작 전략을 계획할 수 있습니다. 시각적 특징을 반영하는 물체 중심 표현을 학습하는 데 상당한 연구가 집중되어 왔지만, 이러한 연구들은 질량이나 마찰과 같은 잠재적인 물리적 속성을 고려하는 경우가 드뭅니다. 물리적 속성의 비지도 학습(Unsupervised learning)은 덜 탐구된 문제입니다.

대부분의 물리적 속성은 정적인 환경에서 시각적 외관 단서만으로는 직접 추론할 수 없습니다. 예를 들어, 알루미늄은 강철과 유사한 외관을 공유하지만 훨씬 가볍습니다. 대부분의 물리적 속성은 정적이거나 준정적인 상호작용 하에서는 두드러지지 않습니다. 나무 블록이나 금속 블록을 부드럽게 밀어낼 때, 재료와 밀도가 다름에도 불구하고 눈에 보이는 움직임의 차이는 미묘할 뿐입니다.

본 연구에서는 다단계의 자가 감독적 동적 상호작용의 시각적 관찰을 통해 물체의 물리적 속성을 발견하고 학습하는 방법을 제안합니다. DensePhysNet은 현재 장면과 행동을 입력으로 받아 상호작용 후 물체가 어떻게 움직이는지를 픽셀 단위의 광학 흐름(optical flow)으로 표현하여 예측합니다. 다양한 상호작용을 조건으로 미래 물체 상태를 예측하도록 학습함으로써, DensePhysNet은 물체의 물리적 속성과 그것이 관찰된 움직임에 어떻게 영향을 미치는지에 대한 암묵적인 이해를 얻게 됩니다.

---

### Methods

DensePhysNet의 목표는 자체 지도 학습을 통해 물체 중심의 물리적 속성 (예: 질량, 마찰)을 인코딩하는 잠재 표현을 학습하는 것입니다. 이를 위해, 저희는 관찰된 대규모 동적 로봇 상호 작용 데이터셋에서 깊이 이미지의 깊은 예측 모델을 훈련합니다. 기본 아이디어는 DensePhysNet이 다양한 상호 작용에 조건화된 물체의 미래 상태를 정확하게 예측하기 위해, 물체의 물리적 속성과 그것이 관찰된 움직임에 어떻게 영향을 미치는지에 대한 암묵적인 이해를 습득해야 한다는 것입니다. 설정은 로봇 앞에 놓인 경사로 위의 물체 모음으로 구성됩니다. 물체와 상호 작용할 때, 로봇은 시점 $t$에서의 상태의 깊이 이미지 $I_t$를 캡처하고, 행동 $a_t$를 실행한 다음, 다음 시점에서 또 다른 깊이 이미지 $I_{t+1}$을 캡처합니다. DensePhysNet을 시각적 관측 $I_t$와 실행된 행동 $a_t$를 입력으로 받아, 광학 흐름 $O_{t, t+1}$ 형태의 다음 상태 관측 $I_{t+1}$의 예측을 출력하는 신경망으로 모델링합니다.

#### Dynamic Interactions

로봇 에이전트는 물체의 물리적 속성을 강조하기 위해 두 가지 유형의 동적 상호 작용을 실행합니다. 미끄러짐 (sliding)과 충돌 (collision)입니다. 표현 학습의 대다수 이전 방법들은 밀기, 파지, 쿡쿡 찌르기와 같은 정적 또는 준정적 조작을 사용하며, 이 경우 물체 움직임으로부터 잠재적인 물리적 물체 속성을 관찰하기 어렵습니다. 준정적 밀기 중에는 물체가 말단 장치와 함께 움직이고 말단 장치가 멈출 때 멈춘다고 가정합니다. 이러한 시나리오에서는 물체의 물리적 속성으로 인한 움직임의 미묘한 차이를 관찰하는 것이 당연히 더 어렵습니다.

이와 대조적으로, 동적 조작은 조작자와의 접촉을 넘어서는 물체 움직임을 유발할 수 있으며, 이는 물체의 물리적 속성을 더 잘 드러낼 가능성이 높습니다. 예를 들어, 물체를 높은 속도로 밀면 가속력이 물체를 스스로 일정 거리 미끄러지게 할 수 있습니다. 접촉 후 움직임의 차이, 예를 들어 이동 거리는 표면 마찰의 차이에 대한 시각적 단서로 작용할 수 있습니다.

미끄러짐 행동은 로봇에 대한 방향 $\theta$와 속도 $v$로 매개변수화됩니다. 물체를 미끄러지게 하기 위해, 로봇은 $\theta$에서 물체에 접근하고 물체가 밀기 후 미끄러질 수 있도록 높은 속도로 밀기를 실행합니다. 실제 로봇의 물리적 힘-토크 안전 한계를 초과하지 않고 높은 밀기 속도를 달성하기 위해, 저희는 팔 베이스에 가까운 관절 (예: 팔꿈치, 어깨)이 말단 장치에 가까운 관절 (예: 손목)보다 느리게 움직이도록 동작 계획을 제약합니다. 방향 $\theta$와 속도 $v$는 이산 변수로 양자화되며, 저희는 이를 인코딩하기 위해 원-핫 벡터를 사용합니다. 다음으로 로봇은 보조 실린더를 파지하고 경사로 중 하나의 상단에 배치합니다. 그런 다음 실린더가 경사로를 굴러 내려와 중간에 있는 물체와 충돌합니다. 충돌 행동의 유일한 매개변수는 실린더의 시작 위치 $[d \times X_s, y, H_s]$입니다. 여기서 $d \in {-1, 1}$은 실린더가 왼쪽 또는 오른쪽 경사로에서 굴러오는지, $X_s$는 경사로에서 작업 공간 중심까지의 거리, $y$는 목표 물체의 $y$ 좌표와 동일, $H_s$는 경사로의 높이입니다.

#### DensePhysNet

<img src="https://velog.velcdn.com/images/devjo/post/c5a06f82-8c65-4fc1-a68f-6aba7b468687/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

학습된 물리적 표현이 물체의 시각적 외관과 행동에 대한 정보를 인코딩하는 표현과 분리되는 방식으로 모듈화되어야 합니다. 이는 학습된 물리적 표현을 새로운 작업, 물체, 행동 유형에 적용하는 데 중요합니다. 반복적인 구조를 가지므로, 여러 상호 작용으로부터 정보를 통합하여 물리적 속성을 더 잘 추론할 수 있습니다. 마지막으로 전체 장면을 단일 잠재 표현으로 인코딩하는 대신, 밀집된 픽셀 단위 표현을 생성합니다. 이는 여러 물체가 있는 복잡한 장면을 처리할 수 있게 합니다.

DensePhysNet는 이미지 인코더 (image encoder), 다단계 정보 통합기 (multi-step information aggregator), 행동 인코더 (action encoder), 교차 컨볼루션 레이어 (cross convolutional layer), 그리고 움직임 예측기 (motion predictor)와 같이 다섯 가지 모듈로 구성됩니다.

---

### Conclusion

이 모델은 자체 지도 상호 작용으로부터 밀집된 물리적 물체 표현을 학습합니다. DensePhysNet이 정성적 및 정량적 분석 모두를 통해 물체 재료와 물리학에 대해 학습한다는 것을 입증했습니다. 더 나아가, 학습된 표현이 평면 미끄러짐과 같은 다운스트림 제어 작업에 사용되어 더 정확한 행동 정책을 제안할 수 있음을 보여주었습니다.

행동 공간의 중요성 물체 물리학을 학습하는 데 있어 행동 공간의 설계는 중요합니다. 저희는 평면 미끄러짐과 충돌을 모두 사용하여 모델이 물체의 질량과 마찰 모두를 추론하는 것을 학습한다는 것을 보여주었습니다. DensePhysNet을 훨씬 더 풍부한 상호 작용 집합을 수용하도록 확장하는 것은 그 잠재력을 완전히 보여줄 것입니다. 입력 양식 확장 DensePhysNet은 깊이 이미지를 통한 물체 움직임으로부터 물리적 표현을 학습합니다. 색상 이미지를 입력으로 받지 않고 물체의 기하학적 구조와 잠재적으로 물리학에 대한 정보를 제공할 수 있습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/1906.03853)
