---
title: 'Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control'
date: '2025-10-10'
tags: ['embodied ai', 'paper review']
---

### Abstract

비전-언어-행동 (VLA) 모델과 같은 심층 신경망 (DNN) 기반 정책 모델은 다중 모달 입력으로부터 복잡한 의사 결정을 자동화하는 데 탁월합니다. 그러나 이러한 모델을 확장하면 계산 오버헤드가 크게 증가하여 로봇 조작 및 자율 주행과 같은 자원 제약이 있는 환경에 배포하는 것을 복잡하게 만듭니다.

이를 해결하기 위해, 양자화 인식 훈련 (quantization-aware training)과 임무 수행에 필수적인 상태 (mission-critical states)에 대한 선택적 손실 가중치 부여 전략을 결합한 주의도 인지 양자화 모방 학습 (aliency-Aware Quantized Imitation Learning, SQIL)을 제안합니다.

주의도 점수 (saliency scores)를 통해 이러한 상태를 식별하고 훈련 손실에서 이들을 강조함으로써, SQIL은 낮은 비트 정밀도에서도 의사 결정 충실도를 보존합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/729a9460-f768-406a-938d-28974d7d4788/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

#### 심층 신경망 정책 모델의 성공과 문제점

최근 몇 년 동안, 심층 신경망 (DNN) 기반 정책 모델은 주로 전문가 데이터로부터의 모방 학습 (Imitation Learning, IL)을 통해 전통적인 검색 기반 방법을 능가함으로써 로봇 조작 및 자율 주행을 크게 발전시켰습니다. 이러한 성공은 일반화 및 강건성을 개선하고 다양한 로봇 구현체, 작업, 또는 환경에 걸쳐 정책 전이를 촉진하기 위해 대규모 IL에 파운데이션 모델 (foundation models)을 채택하는 데 대한 관심 증가로 이어졌습니다. 비전-언어-행동 (VLA) 모델은 사전 훈련된 비전-언어 모델을 다음 토큰 예측을 통해 로봇 공학으로 확장하여 시각적 및 텍스트 정보를 통합하여 향상된 조작 기능을 제공합니다. 그러나 교차 구현체 전이를 지원하는 파운데이션 모델로서의 잠재력에도 불구하고, IL 기반 VLA 모델은 종종 느린 추론 속도, 높은 계산 비용, 상당한 메모리 사용량으로 어려움을 겪어 자원 제약이 있는 배터리 구동 로봇에 배포하는 것을 어렵게 만듭니다.

#### 양자화의 역할

양자화 기술 (Quantization techniques)은 전체 정밀도 FP 가중치 및 활성화를 낮은 정밀도로 변환하여 DNN 추론 비용을 줄이며 수치 오류를 완화하여 정확도를 보존하는 데 중점을 둡니다.

사후 훈련 양자화(Post Training Quantization, PTQ)는 양자화 손실을 줄이기 위해 가중치와 활성화를 조정하는 반면, 양자화 인식 훈련(Quantization Aware Training, QAT)은 강건성을 높이기 위해 훈련에 양자화를 직접 통합합니다.

#### 양자화 오류의 영향

양자화 오류가 IL 정책 결정에 어떻게 영향을 미치는지 조사하고 두 가지 주요 관찰을 주목합니다. 양자화 오류는 대부분의 시간 단계에서 미미한 영향을 미치며 작은 행동 불일치를 생성합니다. 하지만 특정 임계 상태는 양자화 오류로 인해 행동에서 큰 편차를 경험하고 궁극적으로 임무 실패를 야기합니다.

---

### Related works

#### 1. 모방 학습을 위한 DNN 기반 정책

<img src="https://velog.velcdn.com/images/devjo/post/1fc1d378-6e61-4644-ac38-ebfc3b105d92/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

모방 학습 IL을 통해 훈련된 심층 신경망 DNN 기반 정책 모델은 상당한 발전을 이끌었습니다. MuJoCo와 같은 시뮬레이션에서 DNN 모델은 상태 벡터 입력을 처리하여 연속적인 관절 공간에서 로봇을 제어하며 CILRS와 같은 자율주행은 카메라 이미지와 벡터로부터 가속도와 조향을 예측하는 것을 학습합니다. 이처럼 미리 수집된 데이터셋에서 전문가 행동을 모방하여 학습합니다.

#### 2. 효율적인 정책 추론을 위한 양자화

양자화는 가중치 또는 활성화의 비트 정밀도를 낮춤으로써 신경망의 계산과 메모리 요구 사항을 줄이며, 일반적으로 일반 행렬 곱셈 (GEMM)의 입력 피연산자에 적용됩니다. 그리고 연속적인 값을 이산적인 상태로 매핑하기 때문에 수치 오류가 정확도를 저하시킬 수 있습니다.

효율적인 정책 추론에 대한 수요가 증가하고 있음에도 불구하고, 대부분의 양자화 접근 방식은 양자화 오류를 완화하기 위해 IL에서는 사용할 수 없는 직접적인 환경 상호 작용에 의존하는 강화 학습 (RL)에 맞추어져 있습니다.

---

### Methods

#### 주의도 기반 상태-중요도 점수

임무 수행에 필수적인 상태는 작업과 환경에 따라 달라지므로, 모방 학습 중에 어떤 상태에 추가적인 주의가 필요한지 식별하는 것은 자명하지 않습니다. 따라서 임무 수행에 필수적인 상태를 식별하기 위해 상태 중요도 점수 (SIS)를 정량화합니다. 시각적 외란 하에서 큰 행동 불일치를 보이는 상태가 임무 수행에 필수적이라고 주장합니다. 이러한 불일치는 성공적인 의사 결정을 위한 결정적인 시각적 영역을 드러내기 때문입니다.

상태의 각 위치 $k$에서 외란 기반 행동 주의도를 다음과 같이 측정합니다.

$$
S_{\pi}(s_t, k) = \frac{1}{2} \|\pi(s_t) - \pi(\phi(s_t, k))\|_2^2
$$

여기서 $\phi(s_t, k)$는 위치 $k$에서 외란(가우시안 블러)을 적용합니다. $S_{\pi}(s_t, k)$가 높을수록 $k$에서의 국지적 수정이 정책의 출력에 더 큰 영향을 미친다는 것을 나타냅니다.

$$
\text{SIS}_{s_t}^{\pi} = E_k [S_{\pi^{\text{FP}}}(s_t, k)]
$$

#### 양자화-강건 행동 증류

상태 중요도 점수를 사용하여 임무 수행에 필수적인 상태에 선택적으로 집중합니다. 이를 달성하기 위해, 양자화-강건 행동 증류 (QRD)를 제안합니다. 이는 FP 정책과 데모 데이터를 활용하여 FP 정책의 행동 분포를 양자화된 모델로 증류함으로써 양자화 오류를 줄입니다.

$$
L_{\text{QRD}}(\theta) = \alpha_t \cdot E_{\tau_i \sim D_E} \left[ \frac{1}{|\tau_i|} \sum_{s_t \in \tau_i} D(\pi^Q(s_t), \pi^{\text{FP}}(s_t)) \right]
$$

QRD의 손실 함수는 양자화된 정책과 FP 정책의 행동 분포 사이 불일치를 측정합니다. 여기서 $\alpha_t = \beta$는 $\text{SIS}_{s_t}^{\pi^{\text{FP}}} > T$일 때, 그리고 $\alpha_t = 1$은 그렇지 않을 때입니다. $D$는 불일치 메트릭($\text{e.g.}$, $\text{L2}$-노름)이며, $\pi(s_t)$는 상태 $s_t$에 대한 모든 가능한 행동의 확률 분포입니다. 하이퍼파라미터 $\beta (> 1)$는 높은 중요도 상태에 추가 가중치를 적용하며, $T$는 상위 20%의 $\text{SIS}$ 값을 선택하는 임계값입니다.

#### 주의도 인지 양자화 모방 학습

IL의 가중치 조정을 선택적으로 강화하는 주의도 인지 양자화 모방 학습 (SQIL)을 도입합니다. SQIL의 손실 함수는 QAT와 QRD를 결합합니다.

$$
L_{\text{SQIL}}(\theta) = L_{\text{QAT}}(\theta) + L_{\text{QRD}}(\theta)
$$

이 결합된 접근 방식은 양자화 오류를 효과적으로 줄여, 양자화된 정책이 FP 정책과 비슷하게 일반화할 수 있도록 합니다. $L_{\text{QAT}}$는 양자화된 정책의 전문가 행동의 로그 우도를 최대화하는 반면, $L_{\text{QRD}}$는 양자화된 정책의 행동 분포를 FP 정책과 정렬합니다. 선택적 가중치 $\alpha_t$를 적용함으로써, QRD는 모든 상태에서 불일치를 균일하게 최소화하는 대신 정확한 제어가 필요한 상태를 강조합니다.

---

### Conclusion

본 논문은 IL 기반 모델을 위한 양자화 프레임워크를 제안하며, 낮은 비트 정밀도 오류에 대한 강건성을 향상시켜 자원 제한 하드웨어에서의 효율성을 보장합니다.

로봇 조작 및 자율 주행 모델에 대한 평가는 실제 CPU 및 GPU에서 우수한 속도 향상 및 에너지 절약을 보여주며, 전체 정밀도 정확도를 밀접하게 보존하고 실제 배포 잠재력을 입증합니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2505.15304)
