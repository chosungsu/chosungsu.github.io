---
title: 'Forge: Force-Guided Exploration for Robust Contact-Rich Manipulation under uncertainty'
date: '2025-05-24'
tags: ['robotics', 'paper review']
---

### Abstract

본 논문에서는 FORGE라는 방법론을 소개합니다. 이는 상당한 포즈 불확실성(pose uncertainty)이 존재하는 상황에서 힘을 인지하는 조작 정책(force-aware manipulation policies)을 시뮬레이션에서 실제 환경으로 성공적으로 전이(sim-to-real transfer)시키기 위한 방법입니다.

시뮬레이션 기반 정책 학습 과정에서, FORGE는 힘 임계값 메커니즘(force threshold mechanism)과 역학 무작위화 방식(dynamics randomization scheme)을 결합하여 학습된 정책이 실제 로봇으로 견고하게 전이될 수 있도록 합니다.

실제 로봇에 적용할 때, FORGE 정책은 최대 허용 힘에 따라 조건화되며, 컨트롤러 이득(controller gains)에 관계없이 접촉이 많은 작업을 적응적으로 수행하는 동시에 공격적이거나 안전하지 않은 행동을 피합니다. 추가적으로, FORGE 정책은 작업 성공 여부를 예측할 수 있어 효율적인 작업 종료와 힘 임계값의 자율적인 튜닝을 가능하게 합니다.

FORGE가 스냅핏 커넥터(snap-fit connectors)의 힘을 이용한 삽입을 포함하여 다양하고 견고한 접촉 밀착형 정책(contact-rich policies)을 학습하는 데 사용될 수 있음을 보여드립니다. 더 나아가, 유성 기어 시스템(planetary gear system)의 다단계 조립을 시연합니다. 이 작업은 너트 끼우기(nut threading), 삽입(insertion), 그리고 기어 맞물림(gear meshing)이라는 세 가지 조립 작업 모두에서 성공해야 합니다.

---

### Introduction

연구팀은 조립 프리미티브, 예를 들어 낮은 간격 삽입이나 너트 끼우기 등을 학습하기 위한 시뮬레이션-실세계(Sim-to-Real) 기술을 개발하는 데 관심을 두고 있습니다. 지난 10년 동안 Sim-to-Real 기술은 능숙한 조작 및 보행 로봇 분야에서 큰 발전을 이끌어냈습니다.

그러나 이와 유사한 성과가 로봇 조립 분야에서 나타나기 시작한 것은 비교적 최근의 일입니다. 이는 상세하고 간격이 낮은 부품들에 대한 효율적이고 정확한 시뮬레이션이 필수적이기 때문입니다. 이러한 발전에도 불구하고, 접촉이 많은 작업에 대해 Sim-to-Real 기술을 성공적으로 실제에 적용하는 것은 여전히 매우 어려운 과제입니다. 정책을 순진하게 적용할 경우, 로봇의 움직임이 너무 과격하여 부품이 치명적으로 미끄러지거나 손상되어 작업 완료가 어렵거나 불가능해질 수 있습니다. 이러한 문제는 자세 불확실성이 존재하고 접촉에 의존하는 탐색 행동이 필요할 때 특히 두드러지게 나타납니다.

<img src="https://velog.velcdn.com/images/devjo/post/4c217707-6a0c-485a-80d5-a401cf2f4e02/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

부품 간에 필요한 접촉이라 하더라도, 힘이 너무 높으면 바람직하지 않은 결과를 초래할 수 있습니다. 나선형 탐색과 같은 경험적 접근 방식은 가해지는 힘을 제한할 수는 있지만, 이러한 접근 방식은 작업별로 특화되어 있으며 비효율적일 수 있습니다. 예를 들어, IndustReal 프레임워크를 사용하여 훈련된 정책들은 접촉 힘을 관찰하거나 이에 적응하지 않습니다. 너트를 끼우는 작업은 가해지는 힘이 너무 높으면 실패할 수 있지만, 스냅-핏 커넥터는 적절한 삽입을 위해 큰 힘을 필요로 할 수 있습니다. 그러므로 정책의 힘 프로파일을 간단하고 효율적으로 조정할 수 있는 방법이 매우 중요합니다.

FORGE는 정책이 접촉에 강인하도록 보장하는 두 가지 상호 보완적인 구성 요소를 가지고 있습니다. 첫째, 저희는 정책이 작업 실행 중 초과해서는 안 되는 힘 임계값에 조건화되도록 제안합니다. 둘째, 정책들은 광범위한 역학 무작위화 조건 (로봇, 제어기, 부품 속성 무작위화) 하에서 이 임계값을 유지하도록 훈련됩니다.

---

### Methods

#### POMDP Formulation

목표는 기댓값 반환 $J(\pi_\theta)$를 최대화하는 정책 $\pi_\theta(a_t | o_1, \dots, o_t)$를 학습하는 것입니다.

$$
J(\pi_\theta) = E_{\tau \sim p(\tau|\pi_\theta,\Psi)} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$

여기서 $\tau = (s_0, a_0, o_0, s_1, a_1, o_1, \dots)$는 로봇이 정책 $\pi_\theta$를 따를 때 발생하는 상태, 행동, 관측의 궤적 (trajectory)입니다. 상태 $s_t \in S$는 말단 장치 (End-Effector, EE), 고정된 부품, 잡고 있는 부품의 포즈와 속도로 구성됩니다. 또한, 말단 장치가 경험하는 접촉 힘 $F^{ee} \in \mathbb{R}^3$과 로봇, 제어기, 부품의 동역학적 특성에 대한 불변 정보도 포함됩니다. 전체 상태를 정확하게 추정하기 어렵기 때문에 잡음이 있는 말단 장치 포즈 및 속도($\hat{p}^{ee}$), 추정된 접촉 힘($\hat{F}^{ee}$), 잡음이 있는 고정된 부품의 포즈 추정값($\hat{p}^{fixed}$)를 관측합니다. 행동은 태스크 공간 임피던스 제어기의 제어 목표입니다. 이전연구와 마찬가지로 모든 부품이 수직 방향으로 정렬되어 있다고 가정합니다. 따라서 정책은 (x, y, z, yaw) 차원에 대해서만 제어 권한을 갖는 것으로 충분합니다. 보상함수에서는 작업의 중요한 단계에 도달했을 때 주어지는 두 가지 이산 보너스 보상($\text{I}_{place}$, $\text{I}_{success}$)을 추가합니다.

#### Robust Search

본 논문에서 제안하는 Forge는 시뮬레이션에서 탐색 행동 (search behaviours)을 학습하기 위해 온-정책 (on-policy) RL을 사용합니다. 강건한 심투리얼 전송을 위해 힘 임계값 (Force threshold)과 동역학 무작위화 (Dynamics randomization)가 도입됩니다.

정책 실행 중에 과도한 힘은 부품이 미끄러지거나 손상되는 원인이 될 수 있습니다. 센서(예: 손목 카메라 또는 촉각)를 통해 작은 미끄러짐은 회복할 수 있을 수도 있지만 이러한 상황을 피하는 것을 선호합니다. 힘 임계값 $F_{th}$에 정책을 조건화하는 것을 제안합니다. 즉, $\pi(a | o, F_{th})$입니다. 훈련 중에 접촉 힘 $F^{ee}_t$가 임계값을 초과하면 정책에 페널티가 부과됩니다. 구체적으로, 보상 함수에 추가 항을 더합니다.

$$
R_{contact\text{-}pen}(F^{ee}_t) = - \beta \cdot \max(0, ||F^{ee}_t|| - F_{th})
$$

시뮬레이션에서 훈련된 정책을 성공적으로 배포하려면, 훈련 중에 경험하는 궤적 분포가 배포 시 경험할 분포와 유사해야 합니다.

$$
p(\tau^{real}|\pi_\theta, \Psi^{real}) \approx p(\tau^{sim}|\pi_\theta, \Psi^{sim})
$$

이 분포들 간의 차이는 일반적으로 심투리얼 갭이라고 불립니다. 시스템 식별의 목표는 $\Psi^{sim}$을 $\Psi^{real}$에 가깝게 조정하는 것입니다. 이것 자체로 복잡한 조정 절차이며, 새로운 부품 세트마다 다시 수행해야 할 수 있습니다. 따라서 본 논문에서는 동역학 무작위화 접근 방식을 따르며, 이는 광범위한 동역학 매개변수에 강건한 정책을 학습합니다.

$$
\tau \sim p_{DR}(\tau |\pi_\theta) = \int p(\tau |\pi_\theta, \Psi)p(\Psi)d\Psi
$$

먼저, 정책은 상대 포즈 $a_t$를 출력하며, 이는 고정된 부품의 포즈에 적용되어 절대 목표 포즈 $p^{targ}_t$를 얻습니다. 이 포즈는 행동 스케일 $\lambda$에 의해 클리핑되어 목표가 말단 장치의 현재 포즈에서 너무 멀리 떨어지지 않도록 합니다. 안정적인 제어기를 보장하기 위해 임계 감쇠 이득 $k_d = 2\sqrt{k_p}$를 사용합니다. 따라서 제어기는 최대 명령 가능한 힘의 정도를 결정하는 두 매개변수 $\lambda \times k_p$에 의존합니다.

$$
\begin{aligned}
&p^{targ}_t = \text{clip}(\text{combine}(a_t, p^{fixed}), \lambda), \\
&F^{targ} = k_p(p^{targ}_t - p^{ee}_t) - k_d v^{ee}_t
\end{aligned}
$$

두 매개변수를 모두 무작위화하여 최대 명령 가능한 힘의 범위가 $[6.4, 20.0]\text{N}$이 되도록 합니다. 제어 매개변수는 관측에 포함되지 않으므로, 정책은 힘 측정을 기반으로 행동을 조정해야 합니다. 이것은 정책이 특정 제어기 구현에 의존하는 것을 줄여줍니다. 부품들이 서로 미끄러질 때, 재료 마찰이 측면 힘에 영향을 미칩니다. 정책이 광범위한 재료에서 작동할 수 있도록 부품의 질량과 마찰을 무작위화합니다. 그리고 각 에피소드마다 각 차원에 대해 데드존 $F^{DZ}_i$가 선택되며, 이 값 미만의 명령된 힘은 0으로 고정됩니다. 즉, $|F^{applied}_i| = \max(0, |F^{targ}_i| - F^{DZ}_i)$ 입니다. 이것은 정책이 목표를 증가시켜 필요할 때 더 많은 힘을 가하거나 정상 상태 오차를 줄이는 데 도움이 됩니다.

---

### Conclusion

본 연구에서는 포즈 추정 불확실성 하에서 강건한 심투리얼 정책을 훈련하기 위한 힘 인식 방법인 FORGE를 제시하였습니다. 힘 임계값과 동역학 무작위화를 사용하여 안전한 탐색 행동을 학습하며 최대 $5\text{mm}$의 위치 추정 오차에서도 정책을 성공적으로 실행할 수 있도록 합니다. 또한 작업 성공을 예측하여 효율적인 정책 실행과 힘 임계값 조정을 가능하게 합니다. 향후 연구에서는 더 효율적인 탐색 전략을 위해 토크 감지를 조사할 계획이라고 합니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2408.04587)
