---
title: 'Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following'
date: '2025-09-28'
tags: ['embodied ai', 'paper review']
---

### Abstract

본 연구는 동적이며 비정상적인 환경에서 체화된 에이전트의 연속적인 명령어 추종 (continual instruction following) 작업을 해결하기 위해 설계된 탐색적 검색 증강 계획 (Exploratory Retrieval-Augmented Planning, ExRAP) 프레임워크를 제시합니다.

이 프레임워크는 물리적 환경을 효율적으로 탐색하고 환경 맥락 메모리 (environmental context memory)를 구축함으로써 대규모 언어 모델 LLM의 체화된 추론 능력을 향상시키며, 이를 통해 시간 변화적인 환경 맥락에 작업 계획 프로세스를 효과적으로 접지시킵니다.

ExRAP에서, 다중 연속 명령어 추종 작업이 주어졌을 때, 각 명령어는 환경 맥락 메모리에 대한 쿼리와 쿼리 결과에 조건화된 작업 실행으로 분해됩니다. 연속적이고 동시에 수행되는 이러한 다중 작업을 효율적으로 처리하기 위해, 정보 기반 탐색을 LLM 기반 계획 프로세스에 통합하는 탐색 통합 작업 계획 방식을 구현합니다. 메모리 증강 쿼리 평가와 결합된 이 통합 방식은 환경 맥락 메모리의 유효성과 환경 탐색의 부하 사이의 더 나은 균형을 허용할 뿐만 아니라, 전반적인 작업 성능을 향상시킵니다.

더 나아가, 메모리 내 지식의 본질적인 감쇠를 다루기 위해 쿼리 평가를 위한 시간적 일관성 개선 방식을 고안합니다. VirtualHome, ALFRED, CARLA를 사용한 실험을 통해, 본 논문의 접근 방식은 다양한 명령어 규모 및 유형, 그리고 비정상성 정도를 포함하는 다양한 체화된 명령어 추종 시나리오에 대한 강건성을 입증하며, 목표 성공률과 실행 효율성 모두에서 다른 최첨단 LLM 기반 작업 계획 접근 방식을 일관되게 능가합니다.

---

### Introduction

대규모 언어 모델 (LLM)의 응용은 체화된 AI (embodied AI)에서 추가적인 훈련이나 데이터 없이 공통 지식을 활용하고 이를 미지의 작업 및 도메인에 즉시 적용하는 데 필수적입니다. 연구자들은 환경 정보를 LLM의 고유한 공통 지식과 통합함으로써 작업 적응을 더욱 향상시키고 있습니다. 이 능력은 가정용 로봇 공학 및 자율 주행과 같은 분야에서 귀중한 가치를 입증하며, 체화된 에이전트가 최소한의 데이터 요구 사항으로 다양한 명령어 추종 작업을 통해 학습할 수 있도록 합니다.

체화된 에이전트에게 이러한 작업은 단순한 단일, 일회성 명령어가 아니라 다중적이고 지속적인 경우가 많으며, 사용자 요구에 대해 효과적으로 추론하고 계획하기 위해 환경 지식에 대한 지속적인 접근을 필요로 합니다. 이러한 시나리오에서, 에이전트가 계획할 때마다 상호 작용을 통해 환경 지식을 반복적으로 수집하는 효율성은 차선책일 수 있습니다. 또한, 다중 사용자 요구 사항을 효과적으로 통합하고 관리해야 하는 분명한 필요성이 있습니다.

본 논문에서는 지속적으로 변화하는 환경의 실시간 정보에 다중 작업이 의존하는 체화된 에이전트에 대한 연속적인 명령어 추종을 연구합니다. 이러한 설정은 에이전트가 동적인 변화에 적응적으로 대응하고 요구되는 작업을 수행하기 위해 환경에 대한 지속적인 탐색에 참여하도록 요구합니다. 연속적인 명령어 추종 문제를 해결하기 위해, 환경 맥락 메모리를 통합하여 LLM의 체화된 추론 능력을 향상시키도록 설계된 탐색적 검색 증강 계획 (Exploratory Retrieval-Augmented Planning, ExRAP) 프레임워크를 제시합니다.

---

### Related works

<img src="https://velog.velcdn.com/images/devjo/post/d4e72588-98d0-4c01-a790-39f669ab782f/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

#### Embodied Instruction Following

체화된 명령어 추종은 체화된 지식에 대한 이해를 바탕으로 복잡한 작업을 실행하는 것을 포함합니다. 이는 객체, 객체 간의 관계 및 역학을 포함하는 물리적 환경의 다양한 측면을 파악하고, 명령어로 지정된 작업을 성공적으로 완료하기 위해 적절한 행동 또는 기술의 순서를 계획하는 것을 목표로 합니다.

작업 계획 영역에서는 LLM의 추론 능력과 환경적 특성을 결합하기 위한 많은 연구가 있었습니다. 최근 연구는 기술의 행동 유도성 (affordances)을 활용하여 가치를 계산하고, 코드 기반 정책을 구현하며, 보상 함수를 생성하는 동시에 LLM의 향상된 작업 계획 능력을 강조했습니다. 또한, LLM의 공통 지식과 실제 객체에 대한 추론을 활용하는 LLM 중심 환경 모델링 접근 방식이 도입되었습니다. 이러한 LLM 기반 작업 계획 접근 방식은 환경, 인간 또는 다른 에이전트와의 반복적인 상호 작용을 통해 용이하게 되면서 광범위한 체화된 명령어 추종 작업에 적용되어 왔습니다.

#### RAG for LLM

검색 증강 생성 (RAG) 연구는 데이터베이스에서 작업 관련 정보를 검색하고 활용함으로써 작업을 효율적으로 실행하는 데 중점을 두었습니다. 특히, LLM이 작업에 필요한 특정 지식을 요구할 때 관련 데이터를 제안하는 검색기의 성능 향상은 검색기 훈련, RAG 프로세스에 적응하도록 LLM 미세 조정, 또는 동적 쿼리 재구성을 위해 LLM 자체 활용을 포함합니다. 체화된 작업 계획 영역에서, 최근 연구는 RAG를 작업별 시연과 통합하는 것을 채택하고 있습니다. 본 논문에서도 체화된 작업 계획에 RAG를 사용하지만, 동적인 측면을 고유하게 강조합니다. 연속적인 명령어 추종을 위해, 작업을 수행하는 것뿐만 아니라 환경의 변화와 에이전트의 환경 메모리의 지속적이고 효율적인 동기화를 보장하기 위해 에이전트 기술의 관련성 및 중요성에 우선순위를 둡니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/9b60c720-351b-469f-90b5-ed9cbb6bbebc/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

#### Continual Instruction Following

특정 환경 맥락을 기반으로 연속적이고 동시에 수행되는 체화된 작업에 대한 일련의 명령어를 고려합니다. 위 그림에서 명령어는 온도가 높으면 창문을 열어라, TV를 보면 불을 꺼라와 같은 조건부 행동을 수반합니다. 에이전트는 조건이 충족되는지 확인하기 위해 환경을 지속적으로 탐색하게 되며 확인되는 순간 관련 작업을 실행합니다. 본 논문에서 다중 체화된 작업이 환경 맥락에 조건화되고 연속적으로 수행되는 이 시나리오를 연속적인 명령어 추종이라고 지칭합니다.

조건부 명령어 $I = \{i_1, \dots, i_M\}$를 가진 연속적인 명령어 추종 작업에 대해, 시간이 지남에 따라 변하는 비정상적인 체화된 환경을 고려합니다. 연속적인 명령어의 조건은 시간이 지남에 따라 충족될 수도 있고 충족되지 않을 수도 있으며, 환경에서 지속적인 탐색을 필요로 합니다.

본 논문에서의 목표는 연속적인 명령어 추종 작업의 전반적인 성능을 극대화하는 체화된 에이전트 정책 $\pi^*$를 확립하는 것입니다. 특히, 작업 성공률 SR과 평균 대기 단계 PS의 조합으로 보상을 공식화합니다. SR은 조건이 충족된 완료된 작업의 비율이며, PS는 조건이 충족될 때마다 명령어 $i \in I_C$와 관련된 작업을 완수하는 데 필요한 평균 단계입니다. 명령어 $I$와 시점 $t$에 대해, 관찰 $o_t$에서 기술을 수행하는 에이전트 정책 $\pi^*$를 다음과 같이 공식화합니다.

$$
\pi^{*} = \underset{\pi}{\text{argmax}} \left[ \sum_{t} \text{SR}(s_t, \pi(o_t, I)) + \mathbb{E}_{i \in I_C} [-\text{PS}(\pi, i)] \right]
$$

#### Overall Framework

비정상적인 체화된 환경에서의 연속적인 명령어 추종이라는 과제를 해결하기 위해, ExRAP 프레임워크를 개발합니다. 이는 강건한 작업 성능을 보장하면서 메모리 증강 및 탐색 통합 계획 방식을 활용하여 환경 상호 작용의 필요성을 최소화하도록 설계되었습니다.

ExRAP에서, 각 조건부 명령어 $i \in I$는 두 가지 주요 구성 요소, 즉 쿼리 $q$와 실행 $e$로 분해됩니다. 쿼리는 작업 시작의 조건 역할을 하며 환경 정보에 대해 평가됩니다. 반면에 실행은 쿼리 평가 결과를 기반으로 촉발되는 물리적 상호 작용 조작을 포함합니다. 비정상적인 환경에서 쿼리를 평가하는 것은 에이전트가 끊임없이 변화하는 정보와 지속적으로 동기화해야 하는 필요성 때문에 고유한 도전 과제를 제기합니다. 이러한 동기화는 종종 지속적인 탐색을 필요로 하며, 이는 환경과의 집중적인 상호 작용을 야기합니다.

두 가지 구성 요소를 통해 이 문제를 해결합니다. 환경 맥락 메모리를 사용한 쿼리 평가 및 탐색 통합 작업 계획입니다.

전자의 경우 동적 환경을 효과적으로 표현하기 위해 시간적 체화된 지식 그래프(TEKG)를 통해 확립됩니다. 이 그래프 기반 맥락 메모리로 증강된 LLM 기반 쿼리 평가기는 조건 충족 여부를 확인하여 쿼리에 응답하고 이러한 평가에 대한 신뢰 수준을 제공합니다. 후자의 경우는 exploitation 관점에서 작업을 달성하는데 도움이 되며 exploration 관점에서도 평가의 신뢰를 높이는 기술을 계획합니다.

#### Memory Augmented Query Evaluation with Temporal Consistency

환경 맥락 메모리와 에이전트가 인지하는 관찰 모두를 시간적 체화된 지식 그래프 (TEKG)를 사용하여 표현하며, 이 메모리는 이러한 관찰의 축적을 통해 확립됩니다. 주어진 명령어에서 파생된 쿼리는 이전에 축적된 데이터 내의 본질적인 정보 감쇠를 고려하여 맥락 메모리에 대해 평가됩니다.

TEKG는 소스 개체 $s_e$, 관계 $r_e$, 타겟 개체 $t_e$, 및 시점 $t$로 구성된 사중쌍 $\tau = (s_e, r_e, t_e, t)$ 세트로 구성됩니다. TEKG 내의 특정 시점 $t$에서의 환경 맥락 메모리를 다음과 같이 정의합니다.

$$
G_t = \{\tau_1, \tau_2, \cdots, \tau_N\}
$$

현재 관찰 $o_{t+1}$을 이전에 확립된 최신 메모리 $G_t$에 통합하기 위해, 다음과 같이 업데이트 함수 $\mu$를 사용합니다.

$$
G_{t+1} = \mu(G_t, o_{t+1}) = \left( \{\tau \in G_t \mid c(\tau, \tau') = 0, \forall \tau' \in o_{t+1}\} \right) \cup o_{t+1}
$$

여기서 $c$는 $\tau$와 $\tau'$가 $\text{TV}$가 "꺼짐"과 "켜짐" 둘 다임을 나타내는 경우와 같이, 사중쌍 간의 의미론적 모순을 감지하는 함수입니다. 모순이 있으면 $1$을 반환하고, 그렇지 않으면 $0$을 반환합니다. 구성된 메모리 $G$는 연속적인 명령어 추종을 위한 지식 데이터베이스 역할을 하며, 명령어, 쿼리, 실행과 같은 특정 작업 지침과 관련된 환경 정보의 검색을 가능하게 합니다.

명령어 해석기 $\Phi_I$는 연속적인 명령어 $I = \{i_1, \dots, i_M\}$를 처리하며 쿼리 $Q$와 작업 실행 $E$로 변환합니다.

메모리 증강 쿼리 평가기 $\Phi_M$는 시간이 지남에 따라 축적된 과거 메모리인 $G_{1:t} = G_1 \cup \dots \cup G_t$를 사용하여 쿼리 $q \in Q$가 충족될 가능성 $P(q|G_t)$을 추정합니다. 메모리 증강 $\text{LLM}$ ($\Phi_{\text{LLM}}$)을 활용하여, 우리는 이전 단계의 쿼리 평가 $P(q|G_{t-1})$와 쿼리 평가의 사전 확률 $R(q|G_{t-1})$을 통합하여 쿼리 평가기 $\Phi_M$를 개발합니다. 시간이 지남에 따라 메모리에 본질적인 정보 감쇠가 있기 때문에 엔트로피 기반 시간적 일관성을 쿼리 평가의 중간 단계로 통합합니다. 시점 $t$에서의 사전 쿼리 응답의 엔트로피가 시점 $t-1$에서의 엔트로피보다 더 높아야 한다고 가정합니다.

$$
H(R(q|G_{t-1})) > H(P(q|G_{t-1}))
$$

---

### Conclusion

본 논문에서 체화된 환경에서 연속적이고 동시에 수행되는 다중 명령어 추종 작업에 대한 효율적인 통합 계획을 용이하게 하기 위해 ExRAP 프레임워크를 도입했습니다.

확장된 RAG 아키텍처를 통해, 이 프레임워크는 메모리 증강 쿼리 평가와 탐색 통합 작업 계획 방식을 통합함으로써, 효율적인 환경 탐색과 강건한 작업 완료를 모두 달성합니다. VirtualHome, ALFRED, CARLA에서 수행된 실험을 통해, ExRAP이 다양한 연속적인 명령어 추종 시나리오 전반에 걸쳐 강건성을 입증하며, 다른 LLM 중심 작업 계획 접근 방식보다 우위를 명확히 보여주었습니다.

---

### 참고 자료

[원본 경로 #1](https://proceedings.neurips.cc/paper_files/paper/2024/file/7bacd0ebd061d4694583ae0eb69ad15f-Paper-Conference.pdf)
