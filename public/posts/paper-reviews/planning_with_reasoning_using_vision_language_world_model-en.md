---
title: 'Planning with Reasoning using Vision Language World Model'
date: '2025-10-16'
tags: ['embodied ai', 'paper review']
---

### Abstract

Effective planning requires powerful world models, but high-level world models that can understand and reason about actions through semantic and temporal abstractions remain largely underdeveloped.

In this paper, we introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling based on natural videos.

Given visual observations, VLWM first reasons about overall goal achievement, then predicts trajectories where actions and world state changes intersect. These goals are extracted through iterative LLM Self-Refine conditioned on compressed future observations represented as a Tree of Captions.

This model learns both action policies and dynamics models, facilitating reactive System-1 planning via direct policy decoding and reflective System-2 planning through cost minimization.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/99e925d8-e483-43f1-b1fd-13ace2f6bfac/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

World models enable AI agents to optimize action plans internally rather than relying on trial and error in real environments, showing strong performance in planning across low-level, continuous control tasks.

However, learning world models for high-level task planning where actions involve semantic and temporal abstractions remains an unsolved challenge.

#### VLWM Proposal: Language-Based Abstraction

We propose learning world models that leverage natural language as abstract world state representations. Language inherently provides semantic abstraction and is much more computationally efficient to generate compared to raw sensory observations.

Compared to latent embeddings of JEPA (Joint Embedding Predictive Architecture)-based world models, language-based abstractions are intuitive and interpretable.

We propose performing direct world modeling as an objective based on large-scale, uncurated videos, i.e., offline data without rewards.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/c9b03ac3-5867-46fc-9574-032565f3bb0c/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### Vision-Language World Modeling

Given a video, we aim to extract structured language representations. These consist of goals (descriptions and interpretations) and procedural plans (action-state sequences).

For such video-text extraction tasks, one simple approach might be to provide the entire video to a VLM and prompt it to extract language representations. However, an impossible triangle arises: high spatial resolution for fine-grained perception, long temporal ranges across many procedural steps, and using large, smart VLMs that can follow complex instructions.

To address these challenges, we compress input videos into a dense Tree of Captions that significantly reduces data volume while preserving essential semantic information, and use LLMs to extract structured goal-plan representations.

Ideally, each node or leaf should correspond to a coherent single semantic unit and avoid spanning across semantic boundaries. Therefore, we propose generating tree structures through hierarchical feature clustering. Specifically, let $X$ be an untrimmed video and express its feature stream as $Z = \phi(X) = [z_1; \ldots; z_T] \in \mathbb{R}^{T \times d}$ where each $z_t$ is a $d$-dimensional feature vector generated by video encoder $\phi$. And we format it using depth-first search (DFS) order for input to LLMs.

#### Planning with Reasoning

<img src="https://velog.velcdn.com/images/devjo/post/4906035a-9ba9-4d10-a391-3f98ab8992ff/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

System-1 mode allows fast plan generation but lacks foresight, the ability to evaluate alternatives, or correct suboptimal decisions. Once actions are emitted, they are fixed, preventing the model from reconsidering or correcting errors. Such reactive behaviors can lead to error accumulation, especially in long-horizon or complex tasks.

To address these limitations, we introduce System-2 reflective planning, where the world model is combined with a critic module that evaluates the desirability of multiple predicted futures given goals. This will enable a reasoning process that searches for optimal plans through cost minimization.

In world model-based planning, cost functions typically quantify distances between world states resulting from candidate plans and desired goal states. In JEPA world models, this can be directly measured as $L_1$ or $L_2$ distances between fixed-dimensional embedding representations of world states. However, in VLWM, we must measure semantic distances between language-based world state representations rather than computing distances in token space.

We construct training samples by adding distracting steps sampled from valid next steps arising from consistent task continuations or unrelated tasks. The critic independently predicts three cost scores $\mathbf{C_{\text{base}}}, \mathbf{C_{\text{good}}}, \mathbf{C_{\text{bad}}}$, and the model is trained to satisfy ranking constraints $\mathbf{C_{\text{good}} < C_{\text{base}} < C_{\text{bad}}}$, encouraging it to distinguish meaningful progress from irrelevant or misleading continuations. Subsequently, we randomly shuffle steps from the base trajectory to create corrupted sequences with $\mathbf{C_{\text{shuffled}}}$ cost, generating negative samples. The critic is then trained to enforce $\mathbf{C_{\text{base}} < C_{\text{shuffled}}}$ to ensure sensitivity to procedural order and temporal consistency.

---

### Conclusion

In this study, we introduced the Vision Language World Model (VLWM), a foundation model that learns to represent and predict world dynamics directly in language space, enabling interpretable and efficient high-level planning.

By compressing raw videos into hierarchical Trees of Captions and refining them into structured trajectories of goals, actions, and world state changes, VLWM bridges the gap between perception-centric VLMs and reasoning-centric LLMs.

VLWM's dual-mode design supports both fast, reactive System-1 planning through direct policy decoding and reflective System-2 planning through cost minimization guided by a self-supervised critic, enabling the model to perform internal trial-and-error reasoning and select optimal plans. VLWM, trained on large-scale diverse educational and first-person viewpoint video corpora, establishes new state-of-the-art results on the Visual Planning for Assistance benchmark, demonstrates superior plan quality in PlannerArena human preference evaluations, and achieves top-tier performance on RoboVQA while generating interpretable action-state rollouts.

Furthermore, the critic model independently shows excellent performance on goal achievement detection and procedural planning benchmarks, highlighting the value of explicit semantic cost modeling for world model-based reasoning.

---

### References

[Original Source #1](https://arxiv.org/pdf/2509.02722)
