---
title: 'Embodied Task Planning with Large Language Models'
date: '2025-10-14'
tags: ['embodied ai', 'paper review']
---

### Abstract

상식을 갖춘 구체화된 에이전트 (embodied agents)를 장착하는 것은 로봇이 일반적인 환경에서 복잡한 인간 명령을 성공적으로 완료하는 데 중요합니다. 최근 대규모 언어 모델 (LLM)은 복잡한 작업의 계획 생성에서 에이전트에게 풍부한 의미론적 지식을 임베딩할 수 있지만, 현실 세계에 대한 정보가 부족하여 일반적으로 실행 불가능한 행동 시퀀스를 산출합니다.

본 논문에서는 에이전트가 물리적 장면 제약을 가진 접지된 계획 (grounded planning)을 위해 장면에 존재하는 객체에 따라 실행 가능한 계획을 생성하는 구체화된 작업에서의 작업 계획 에이전트 (TAsk PlAnning Agent, TaPA)를 제안합니다. 이는 LLM과 시각 인식 모델을 정렬함으로써 이루어집니다.

구체적으로, 먼저 실내 장면, 명령, 및 행동 계획의 삼중항을 포함하는 다중 모달 데이터셋을 구축합니다. 여기서 프롬프트와 장면에 존재하는 객체 목록을 GPT-3.5에 제공하여 다수의 명령과 해당 계획된 행동을 생성하도록 합니다. 생성된 데이터는 사전 훈련된 LLM의 접지된 계획 튜닝에 활용됩니다.

추론하는 동안, 접근 가능한 다른 위치에서 수집된 다중 시점 RGB 이미지로 개방형 어휘 객체 감지기를 확장하여 장면의 객체를 발견합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/48503e87-d671-4be6-ba60-4d68096b332a/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

구체화된 에이전트에게 일반적인 상식 지식을 갖추게 하여 자연어 명령을 기반으로 복잡한 작업을 수행하도록 하는 것은 가정 서비스, 의료 치료, 농업 수확과 같은 많은 응용 분야에서 바람직합니다. 제한된 훈련 샘플과 다양한 작업으로 인해, 다양한 배포 시나리오에 걸쳐 구체화된 에이전트를 직접 훈련하는 것은 실행 불가능합니다.

#### LLM의 한계와 접지된 계획의 필요성

LLM은 주변 장면을 인식할 수 없으며, 존재하지 않는 객체와의 상호 작용 요구 사항으로 인해 실행 불가능한 행동을 생성할 수 있습니다. 예를 들어, "와인 좀 주세요"라는 인간의 명령이 주어졌을 때, GPT-3.5에서 생성된 행동 단계는 "병에서 잔에 와인을 따르는 것"입니다. 현실적인 장면에는 유리잔 대신 머그잔만 있을 수 있으며, 실행 가능한 행동은 "병에서 머그잔에 와인을 따르는 것"이어야 합니다. 따라서 복잡한 작업 수행을 위한 구체화된 에이전트를 구축하려면 LLM이 생성한 작업 계획을 물리적 세계에 접지시키는 것이 필수적입니다.

#### 기존 연구의 한계

주어진 물리적 장면에서 실행 가능한 작업 계획을 얻기 위해, 많은 이전 연구들은 테이블탑 객체의 일반적인 조작 작업에 대해 장면의 시각적 단서를 고려하여 생성된 행동을 필터링하거나 정렬합니다. 이로 인해 수많은 복잡한 작업과 다양한 배포 시나리오의 요구 사항을 충족하는 데 실패합니다.

#### TaPA 프레임워크 제안

본 논문에서는 물리적 장면에서 구체화된 작업 계획 접지를 위한 TaPA (Task Planning Agent)라고 불리는 작업 계획 에이전트를 제시합니다.

샌드위치 만들기 및 테이블 세팅과 같은 복잡한 가사 작업에 대한 행동 단계를 산출하기 위한 일반적인 상식 지식을 습득하며, 이는 인간의 고수준 요구 사항을 처리하기 위한 다운스트림 탐색 및 조작 프로세스에 기초적인 명령을 제공합니다.

더 구체적으로 각 샘플이 시각적 장면, 명령, 및 해당 계획의 삼중항인 다중 모달 데이터셋을 구축합니다. 생성된 데이터셋을 활용하여, 장면의 객체 목록을 기반으로 행동 단계를 예측함으로써 사전 훈련된 LLaMA 네트워크를 미세 조정하며, 이는 작업 계획기로 사용됩니다. 추론하는 동안 객체 목록을 얻기 위해, 구체화된 에이전트는 다양한 시점에서 충분한 정보를 제공하는 RGB 이미지를 수집하기 위해 효과적으로 서 있는 지점을 방문하며, 존재하는 객체의 목록을 얻기 위해 개방형 어휘 감지기를 다중 시점 이미지에 대해 일반화합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/0d832899-d8a0-44ef-b22d-9eac151e8f8c/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### 구체화된 작업 계획 데이터 생성

대규모 비전-언어 모델 (VLM) 및 대규모 다중 모달 모델이 광범위한 복잡한 인식 작업에서 놀라운 성능을 달성했지만, 계획 에이전트를 훈련하기 위한 대규모 다중 모달 데이터셋의 부족으로 인해 현실적인 실내 장면에 접지된 구체화된 작업 계획은 여전히 ​​도전적입니다.

#### 장면 정보 임베딩 및 데이터 생성 파이프라인

구체화된 3D 장면 $X_s$가 주어지면, 모든 객체의 클래스 이름을 장면의 표현으로 직접 사용하며, 이를 $X_l$로 표기합니다.

대규모 복잡한 명령 $X_q$와 실행 가능한 해당 계획 $X_a$를 효율적으로 생성하기 위해, GPT-3.5가 객체 이름 목록 $X_l$을 기반으로 자동으로 데이터를 합성하도록 구체화된 작업 계획 시나리오를 시뮬레이션하는 프롬프트를 설계합니다. 데이터셋 생성을 위해 프롬프트에 활용된 객체 목록의 경우, 장면에 존재하는 인스턴스의 Ground Truth 레이블을 직접 사용합니다.

#### 주변 장면에 작업 계획 접지

<img src="https://velog.velcdn.com/images/devjo/post/b1e8609f-750a-474e-9906-b67cae5a5425/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

실행 가능성 제약을 가진 물리적 세계에 구체화된 작업 계획을 접지하기 위해서는 인스턴스 누락이나 오탐 없이 장면의 객체 목록을 정확하게 얻는 것이 필수적입니다. 감지기 훈련에서 보지 못한 새로운 객체가 배포 시나리오에 나타날 수 있으므로 객체 목록 획득을 위해 개방형 어휘 객체 감지기를 일반화합니다. 에이전트는 다양한 위치에서 RGB 이미지를 수집하여 시각적 장면을 인식하여 존재하는 객체를 발견합니다.

위치 선택 기준에는 순회 위치 (traversal positions), 무작위 위치 (random positions), 전체 중심점 (overall center point), 블록별 중심점 (block-wise center points)이 포함되며, 에이전트는 각 위치 선택 기준에 대해 카메라를 회전하여 다중 시점 이미지를 얻습니다. 따라서 이미지 수집 전략 $S$를 다음과 같이 공식적으로 작성합니다.

$$
S = \{(x, y, \theta) | (x, y) \in L(\lambda, A), \theta = k\theta_0\}
$$

여기서 $(x, y, \theta)$는 위치와 카메라 방향을 나타냅니다. $L(\lambda, A)$는 하이퍼파라미터 $\lambda$를 가진 위치 선택 기준을 의미하며, 모든 샘플링된 위치는 접근 가능한 영역 $A$ 내에 있어야 합니다. 카메라 회전을 위한 단위 각도는 $\theta_0$로 설정되고, $k$는 에이전트가 장면의 다른 방향에서 시각적 단서를 수집하도록 하는 정수입니다.

작업 계획기는 실행 가능한 행동 단계를 생성하기 위해 장면에 존재하는 모든 객체의 정보를 필요로 하며, 객체 목록 획득을 위해 수집된 다중 시점 RGB 이미지에 개방형 어휘 객체 감지기를 일반화합니다. 장면에 대한 예측된 객체 목록 $\hat{X}_l$은 다중 시점 이미지의 감지 결과에서 중복된 객체 이름을 제거하여 얻습니다.

$$
\hat{X}_l = R_d\left[\bigcup_i D(I_i)\right]
$$

여기서 $R_d$는 중복된 객체 이름을 제거하는 연산이고 $D(I_i)$는 장면에서 수집된 $i$번째 $\text{RGB}$ 이미지에 대해 감지된 객체 이름을 나타냅니다.

$$
X_a = \text{TaPA}(P_{\text{in}}, \hat{X}_l, X_q)
$$

인간 명령 $X_q$와 예측된 객체 목록 $\hat{X}_l$은 실행 가능한 행동 계획 $X_a$를 생성하기 위해 TaPA에서 고려됩니다.

---

### Conclusion

본 논문에서는 구체화된 작업 계획을 위한 작업 계획 에이전트 (TaPA)를 제시했으며, 여기서 실행 가능한 행동 단계는 후속 로봇 탐색 및 조작을 통해 인간의 명령을 완료하도록 생성됩니다.

먼저 시각적 장면, 명령, 및 해당 계획을 포함하는 삼중항으로 구성된 다중 모달 데이터셋을 구축했습니다. 이 데이터셋은 장면의 모든 객체 목록과 설계된 텍스트 프롬프트를 고려하여 GPT-3.5로 생성되었으며, 이는 명령 모델을 실행 가능한 행동을 생성하도록 튜닝하는 데 활용됩니다.

추론을 위해, 접근 가능한 다른 위치에서 다중 시점 RGB 이미지를 수집하고, 개방형 어휘 객체 감지 프레임워크를 활용하여 미세 조정된 명령 모델을 위한 장면의 객체 목록을 발견합니다.

광범위한 평가 결과는 TaPA가 생성된 행동 계획의 그럴듯함에 있어 최신 LLM 및 LMM을 능가함을 보여줍니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2307.01848)
