---
title: 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection'
date: '2024-09-23'
tags: ['embodied ai', 'paper review']
---

### Abstract

대규모 데이터는 최근 자연어 처리 및 컴퓨터 비전 연구의 발전에서 입증된 바와 같이 기계 학습의 필수 구성 요소입니다. 그러나 각 작업자가 한 번에 하나의 로봇만 제어할 수 있으므로, 대규모 로봇 데이터를 수집하는 것은 훨씬 더 비용이 많이 들고 느립니다.

이 비용이 많이 드는 데이터 수집 프로세스를 효율적이고 확장 가능하게 만들기 위해, 학습된 보조 정책 (learned assistive policy)을 사용하여 시연 수집 프로세스의 일부를 자동화하는 시스템인 정책 보조 원격 조작 (PATO, Policy Assisted TeleOperation)을 제안합니다.

PATO는 데이터 수집에서 반복적인 동작을 자율적으로 실행하고, 어떤 하위 작업이나 동작을 실행해야 할지 확신하지 못할 때만 인간의 입력을 요청합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/8882547a-6ea3-438c-9d17-2a77489eec56/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### 로봇 데이터 수집의 병목 현상

최근 많은 연구들은 다양한 인간 수집 시연 데이터셋으로부터 인상적인 로봇 학습 결과를 보여주었습니다.

그러나 “원격 조작 (teleoperation)”을 통한 이러한 인간 시연 수집은 지루하고 비용이 많이 듭니다. 작업은 반복적으로 시연되어야 하며, 각 작업자는 한 번에 하나의 로봇만 제어할 수 있기 때문입니다. 원격 조작 연구는 VR 컨트롤러 및 스마트폰과 같은 다양한 인터페이스를 탐색하는 데 중점을 두었지만, 데이터 수집을 확장하는 데 있어 앞서 언급된 병목 현상을 해결하지 못합니다. 따라서 현재의 원격 조작 시스템은 현대 로봇 학습 파이프라인이 요구하는 확장성을 제공하기에 부적합합니다.

#### PATO의 목표와 영감

목표는 원격 조작 동안 인간 작업자에게 보조를 제공하여 로봇 데이터 수집의 확장성을 개선하는 것입니다. 시맨틱 분할과 같은 다른 기계 학습 분야에서 영감을 얻습니다. 여기서 비용이 많이 드는 레이블링 프로세스는 인간 주석가에게 학습된 보조 시스템을 제공함으로써 실질적으로 가속화되었으며, 이는 레이블링 부담을 대폭 줄였습니다.

마찬가지로 반복적으로 시연된 동작의 제어를 자동화하고 새로운 상황에 직면하거나 어떤 동작을 실행해야 할지 확신하지 못할 때만 사용자 원격 조작을 요청할 수 있는 보조 정책 (assistive policy)을 훈련할 것을 제안합니다.

---

### Related works

#### 로봇 원격 조작

초기에는 인간 작업자가 로봇을 직접 움직이는 근접 교육 (kinesthetic teaching)이 일반적이었지만, 최근에는 원격 조작 (teleoperation)이 표준이 되었습니다. 이는 인간 작업자와 로봇을 분리하는 것이 더 편안한 인간 제어 입력을 허용하고 이미지 기반 입력을 가진 정책을 훈련하는 데 중요하기 때문입니다.

원격 조작 시스템 연구는 VR 헤드셋, 조이스틱, 스마트폰과 같은 다양한 인터페이스를 탐색하는 데 중점을 두었습니다. 그러나 이러한 작업 중 어떤 것도 원격 조작 동안 인간 작업자의 능동적인 보조를 탐색하지 않습니다.

다른 연구들은 장애가 있는 사람들이 로봇 팔을 제어할 수 있도록 학습된 임베딩 공간을 통해 저자유도 (low-DoF) 인터페이스를 사용하여 고자유도 (high-DoF) 매니퓰레이터를 제어하는 것을 조사했습니다. 이와 대조적으로, 본 논문에서 접근 방식은 더욱 확장 가능한 데이터 수집을 가능하게 할 목표로 원격 조작 프로세스의 일부를 자동화하는 보조 정책을 훈련합니다.

#### 대화형 인간-로봇 학습

로봇 학습 분야에서, 많은 접근 방식이 학습 루프에서 인간 입력을 활용하는 것을 탐색했으며 언제 이러한 입력을 활용할 지를 결정하는 다양한 방법에 중점을 두었습니다.

DAgger 알고리즘을 기반으로, 많은 연구들이 인간이 직접 개입할 시기를 결정하도록 하는 것, 앙상블 기반 지원 추정치를 사용하는 것, 모델 출력과 인간 입력 사이의 불일치를 사용하는 것, 또는 예측된 미래 보상을 기반으로 하는 위험 추정치를 사용하는 것을 조사했습니다.

---

### Methods

#### Problem Formulation

<img src="https://velog.velcdn.com/images/devjo/post/fea27f1b-1657-4179-b62b-d570a39236a7/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

데이터셋 $D$의 확장 가능한 데이터 수집을 가능하게 하기 위해 보조 정책을 사용하여 반복적인 동작의 제어를 자동화할 것을 제안합니다. 보조 정책은 로봇을 제어하고 필요한 인간 입력을 최소화하여 인간 작업자가 연속적인 간격 동안 로봇에서 주의를 돌릴 수 있도록 합니다.

보조 정책은 상태 $s$ (예: 원시 RGB 이미지)가 주어지면 행동 $a$ (예: 로봇 말단 장치 변위)를 생성하는 $\pi(a|s)$로 정의될 수 있습니다.

보조 정책 $\pi$를 훈련하기 위해, 다양한 에이전트 경험의 사전 수집된 데이터셋 $\mathcal{D}_{\text{pre}}$에 접근할 수 있다고 가정합니다.

#### 다중 모드 데이터로부터 보조 정책 학습

<img src="https://velog.velcdn.com/images/devjo/post/d1236aec-4138-4b73-8420-e988c55bf5a3/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

다양한 다중 작업 데이터 $\mathcal{D}_{\text{pre}}$로부터 보조 정책 $\pi(a|s)$를 학습하는 것은 데이터가 종종 고도로 다중 모드이며 모방하기에 장기 지평이기 때문에 도전적입니다.

하위 목표 예측기 $p(\mathbf{sg}|s, z)$와 하위 목표 도달 정책 $\pi_{\text{LL}}(\mathbf{a}_t|s_t, \mathbf{sg})$로 구성됩니다. 상태 $s$가 주어지면, 하위 목표 예측기가 저 하위 목표 $\mathbf{sg}$를 생성합니다. 그런 다음, 하위 목표와 현재 상태가 주어지면, 하위 목표 도달 정책은 다음 $L$ 타임스텝 동안 행동을 출력하여 에이전트를 하위 목표로 이끌어갑니다.

하위 목표 예측기 $p(\mathbf{sg}|s_t, z)$를 하위 목표에 대한 조건부 변이형 자동 인코더 (conditional variational auto-encoder, CVAE)로 훈련합니다.

저수준 하위 목표 도달 정책 $\pi_{\text{LL}, \phi}(\mathbf{a}|s, \mathbf{sg})$는 행동 복제 목적 함수 (behavioral cloning objective)를 통해 훈련됩니다. 하위 목표 도달 정책이 하위 목표가 주어지면 일련의 행동을 예측해야 하므로, $s_t$와 $\mathbf{sg}$를 사용하여 다음 $L$ 스텝 동안 행동을 자기 회귀적으로 예측하는 LSTM을 사용하여 구현된 순환 정책을 선택합니다.

#### 사용자 입력 요청 시점 결정

<img src="https://velog.velcdn.com/images/devjo/post/9bd021ec-a26b-482c-a694-780a2c92d752/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

구체적으로, 보조 정책이 (1) 분포 외 상태 또는 (2) 다중 모드 행동 분포로 인해 높은 불확실성을 가질 때 사용자 입력을 요청하도록 합니다. 계층적 모델은 두 클래스의 불확실성을 개별적으로 추정할 수 있도록 허용합니다.

먼저, 주어진 상태가 보이지 않는지를 추정하기 위해 분포 외 감지에 대한 이전 작업을 따라 $K$개의 저수준 하위 목표 도달 정책 앙상블 $\pi_{\text{LL}}^{(1)}(\mathbf{a}^{(1)}|s), \ldots, \pi_{\text{LL}}^{(K)}(\mathbf{a}^{(K)}|s)$을 훈련합니다. 이들 모두는 동일한 데이터 $\mathcal{D}_{\text{pre}}$를 사용하지만 다른 초기화와 배치 순서로 훈련됩니다.

그런 다음, 이 앙상블 정책에 의해 예측된 행동 $\mathbf{a}^{(1)}, \ldots, \mathbf{a}^{(K)}$ 사이의 불일치 $D(\mathbf{a}^{(1)}, \ldots, \mathbf{a}^{(K)})$ (즉, 각 차원에서 행동의 분산의 평균)는 상태가 얼마나 분포 외인지 근사하는 데 사용될 수 있으며, 이것을 정책 불확실성이라고 부릅니다. 높은 정책 불확실성을 가진 상태는 보이지 않는 상태로 간주될 수 있습니다. 보조 정책이 보이지 않는 상태에 직면할 때, 다음에 취할 행동을 결정하기 위해 사용자에게 요청합니다.

상태가 훈련 데이터에서 보였다고 결정하더라도, 보조 정책을 따르기 전에 현재 상태에 여러 가지 가능한 작업 옵션이 있는지를 추가적으로 식별합니다.

작업 불확실성, 즉 작업에 대한 보조 정책의 불확실성을 추정하기 위해, 하위 목표 예측기 $p_{\theta}(\mathbf{sg}^{(i)}|\mathbf{s}, z^{(i)})$에서 샘플링된 하위 목표 $\mathbf{sg}^{(1)}, \ldots, \mathbf{sg}^{(N)}$의 하위 목표 간 분산 $\text{Var}(\mathbf{sg}^{(1)}, \ldots, \mathbf{sg}^{(N)})$을 계산할 것을 제안합니다.

---

### Conclusion

대규모 로봇 시연 데이터셋은 로봇 학습에서 다음 돌파구를 가능하게 하는 핵심입니다. 대규모 로봇 데이터를 향한 한 단계로서, 학습된 보조 정책을 사용하여 인간 원격 조작의 일부를 자동화하고 결정적인 상태에서 능동적으로 인간 입력을 요청하는 로봇 데이터 수집을 위한 효율적이고 확장 가능한 시스템을 제안합니다.

단순성을 위해, 보조 정책을 훈련하기 위해 사전 수집된 데이터 $\mathcal{D}_{\text{pre}}$에 대한 접근을 가정합니다. 그러나 보조 정책은 이론적으로 처음부터 학습될 수 있으며 더 많은 데이터가 수집됨에 따라 지속적으로 개선될 수 있습니다. 이러한 방식으로, 작업자는 또한 시간이 지남에 따라 보조 정책에게 새로운 기술을 가르치고 그것의 능력을 그들의 필요에 맞게 조정할 수 있습니다. 지속적으로 진화하는 보조 시스템에 대한 조사를 향후 작업을 위한 흥미로운 방향으로 남겨둡니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2212.04708)
