---
title: 'Double Deep Q-Learning With Prioritized
Experience Replay for Anomaly Detection
in Smart Environments'
date: '2022-09-26'
tags: ['time-series', 'paper review']
---

### Abstract

스마트 환경에서 이상 탐지(Anomaly detection)는 개인이나 인프라에 safety-critical이 될 수 있는 드문 사건들을 다룰 때 중요합니다. 여기서 safety-critical이란, 이러한 사건들이 개인의 안전이나 인프라 보안에 위협이 될 수 있음을 의미합니다. 그러나 모니터링 센서에 의해 기록되는 데이터의 복잡하고 변동성이 큰 특성 때문에 스마트 환경에서 비정상적인 사건을 인식하는 것은 어렵습니다.

문헌에서 제안된 방법론은 흔히 domain-specific이며, 기본 데이터에 대한 편향된 가정에 영향을 받습니다. 본 연구에서는 스마트 환경에서의 이상 탐지를 위해 이중 심층 Q-학습(DDQN: Double Deep Q-Learning)이라는 심층 강화 학습 알고리즘을 적용하는 것을 제안합니다. 우리가 제안하는 이상 탐지기는 다변량 순차 시계열 데이터를 기반으로 드문 사건을 분류할 수 있는 의사 결정 함수를 직접 학습합니다.

우선순위 경험 재생(PER: Prioritized Experience Replay) 전략으로 알고리즘을 확장했으며, 이 PER 확장이 탐지 성능을 향상시킨다는 것을 보여주었습니다. 본 연구에서 제안하는 솔루션을 평가하기 위해 낙상 및 재실 감지 데이터셋을 사용했습니다.

---

### Introduction

스마트 환경에서 드문 사건을 인식하는 것은 모니터링 센서에 의해 기록되는 데이터의 복잡하고 변동성이 큰 특성 때문에 어렵습니다. 특히 데이터가 노이즈가 많고, 다변량이며, 시간 의존적인 경우 더욱 그렇습니다. 연속 마르코프 결정 과정(MDP)을 해결함으로써 강화 학습(RL) 접근 방식은 이러한 어려움을 극복할 수 있습니다. 이 논문에서 제안하는 방법은 데이터 기반 방식으로 경험을 생성하여 드문 사건을 직접 감지하도록 학습합니다. 이상 탐지에 대한 최근 연구는 딥러닝 방법론과 강화 학습 패러다임의 결합을 고려하고 있습니다.

---

### Related work

Kurt 연구진은 이상 탐지를 위한 강화 학습의 직접적인 적용을 제안했습니다. 저자들은 사이버 공격의 온라인 탐지를 위한 model free 강화 학습 접근 방식을 조사했습니다. 그들은 이상 탐지 문제를 부분적으로 관찰 가능한 마르코프 결정 과정(POMDP)으로 모델링하고, 작은 문제 크기에 대한 최적의 이상 탐지기를 생성하기 위한 모델-프리 SARSA 알고리즘의 효과를 평가했습니다. Zhong 연구진은 센서 데이터에 대한 이상 탐지를 위한 심층 액터-크리틱 강화 학습 프레임워크를 제안했습니다. 이들이 제안한 심층 액터-크리틱 에이전트는 순차 처리 데이터를 기반으로 테스트할 센서를 동적으로 선택합니다.

이후에는 심층 Q-학습(DQN)을 기반으로 한 시계열 이상 탐지를 위한 일반적인 경험 수집 프레임워크가 제안되었습니다. 이는 데이터의 시간적 종속성을 모델링하기 위해 LSTM(Long Short-Term Memory) 네트워크를 채택하고, 메모리 재생과 함께 Q-학습 알고리즘을 적용했습니다. 그러나 제안된 접근 방식의 단점은 1차원 특징 공간으로 제한된다는 것입니다.

이전에 언급된 연구들과 차이점으로 본 논문의 연구는 다변량 순차 시계열 학습 시나리오에 적응함으로써 실질적으로 확장됩니다. 또한, 정책 추정을 더 안정적으로 만들기 위해 이상 탐지를 위한 DDQN 사용을 제안합니다.

---

### Methodology

#### 1. DOUBLE DEEP Q-Learning

Q-러닝은 가장 인기 있는 강화 학습 알고리즘 중 하나이지만, 특정 조건에서 예상치 못한 높은 행동 가치(action values)를 보일 수 있다는 문제가 있습니다. 행동 가치의 과대평가(overestimation) 문제는 Q-러닝 업데이트 함수의 최대화 단계에서 발생하는데, 여기서 과대평가된 값이 자동으로 선호됩니다. 기본 DQN(Deep Q-Network) 알고리즘은 이 문제를 해결할 수 없습니다. 왜냐하면 Q-함수 추정기인 다층 퍼셉트론(MLP)이 각 순차 업데이트 작업에 대해 부트스트랩된 가치 함수를 직접적으로 표현하기 때문입니다. uncorrelated replay memory의 추가가 과대평가에 대한 억제 역할을 하지만, 과대평가는 여전히 발생하며 학습된 정책에 부정적인 영향을 미칩니다. Hasselt 연구진은 부트스트랩된 경험과 가치 함수 업데이트 간의 상관관계를 더욱 줄이기 위해 지연 업데이트 타겟 네트워크(latency update target network)를 제안했습니다.

표준 딥 Q-러닝에서 타겟 값은 다음과 같이 계산됩니다.

$$
Y_t=r_t+\gamma max_{a_{t+1}}(Q(s_{t+1}, a_{t+1}))
$$

DDQN에서는 가중치가 지연적으로 그리고 비상관 경험을 통해 업데이트되는데 다음과 같이 계산됩니다.

$$
Y_t=r_t+\gamma Q_{\theta}( \\ 
s_{t+1}, \\ 
\text {argmax}_{a_{t+1}}(Q(s_{t+1}, a_{t+1})))
$$

여기서 $Q_{\theta}$는 타겟 네트워크의 선언입니다. 우리는 행동 선택을 위해 $Q$를 사용하고, 행동 평가를 위해 $Q_{\theta}$를 사용합니다.

#### 2. Prioritized experience replay

경험 재생(Experience replay)은 딥 Q-러닝의 주요 특징 중 하나입니다. 희귀 이벤트 분류 작업에 중점을 두어, 우리는 DDQN 알고리즘을 PER(우선순위 경험 재생)로 확장할 것을 제안합니다. PER은 경험의 중요도와 관계없이 우선순위에 따라 재생 빈도와 중요도를 조절할 수 있음을 입증합니다. 주요 아이디어는 일부 경험이 다른 경험보다 에이전트에게 더 중요하다는 것입니다.

확률 값 $P(t)$에 의한 확률적 샘플링은 아래와 같이 정의합니다.

$$
P(t) = \frac{p_t^{\beta}}{\sum_k p_k^{\beta}}
$$

이 식을 통해 비편향된 샘플링 분포가 생성되며 지수 $\beta$가 우선순위에 따라 조절됨으로써 순위 값 $p_t$는 비례 계산 식인 $p_t=|\delta_t|+\epsilon$ 또는 순위 기반 우선순위 식인 $p_t=\frac{1}{\text {rank} (t)}$로 정의될 수 있습니다.

---

### Conclusion

본 논문에서는 DDQN을 PER(우선순위 경험 재생) 샘플링 전략으로 확장했으며 PER을 사용함으로써 해당 데이터셋의 클래스 불균형 문제가 덜 심해지고, 그 결과 더 강건한 탐지기가 되었음을 확인하였습니다.

occupancy detection dataset에서는 98.2%의 정확도와 96.0%의 F1-점수를 달성했습니다. 더 크고 변동성이 높은 fall detection dataset에서는 92.6%의 정확도와 70.5%의 F1-점수를 달성하여 이전 연구에서 제안된 대부분의 접근 방식을 능가했습니다.

---

### 참고 자료

[원본 경로 #1](https://ieeexplore.ieee.org/document/9786787)
