---
title: 'Graph neural networks-based Scheduler for Production
planning problems using Reinforcement Learning'
date: '2023-05-10'
tags: ['robotics', 'paper review']
---

### Abstract

강화 학습 RL은 작업장 스케줄링 문제 JSSP에서 점점 더 많이 채택되고 있습니다. 그러나 JSSP에 대한 RL은 일반적으로 기계 특징의 벡터화된 표현을 상태 공간으로 사용하여 수행됩니다. 이것은 세 가지 주요 문제를 안고 있습니다. (1) 기계 장치와 작업 순서 간의 관계가 완전히 포착되지 않고, (2) 기계/작업 수 증가에 따른 상태 공간 크기의 기하급수적 증가, 그리고 (3) 보지 못한 시나리오에 대한 에이전트의 일반화 문제입니다.

본 논문은 GraSP-RL (GRAph neural network-based Scheduler for Production planning problems using Reinforcement Learning), 즉 강화 학습을 활용한 생산 계획 문제용 그래프 신경망 기반 스케줄러라는 새로운 프레임워크를 제시합니다. 이 프레임워크는 JSSP를 그래프로 표현하고, 그래프 신경망 GNN을 사용하여 추출된 특징을 통해 RL 에이전트를 훈련시킵니다. 그래프 자체는 비유클리드 공간에 있지만, GNN을 사용하여 추출된 특징은 유클리드 공간에서 현재 생산 상태에 대한 풍부한 인코딩을 제공합니다. 그 핵심에는 GNN에 적용되는 맞춤형 메시지 전달 알고리즘이 있습니다. GNN에 의해 인코딩된 노드 특징은 이후 RL 에이전트가 다음 작업을 선택하는 데 사용됩니다.

더 나아가, 우리는 스케줄링 문제를 분산 최적화 문제로 간주하며, 학습 에이전트는 모든 생산 단위에 개별적으로 할당되고 에이전트는 다른 모든 생산 단위에서 수집된 경험으로부터 비동기적으로 학습합니다.

GraSP-RL은 이후 30개의 작업과 4대의 기계를 가진 복잡한 사출 성형 생산 환경에 적용됩니다. 목표는 생산 계획의 최대 완료 시간 makespan을 최소화하는 것입니다. 우선 순위 디스패치 규칙 알고리즘 및 타부 탐색 TS 및 유전 알고리즘 GA과 같은 메타휴리스틱과 비교 및 분석됩니다.

---

### Introduction

제조 또는 생산 스케줄링 작업은 특히 글로벌 경쟁 시장에서 작업 현장의 수익성과 생산성을 높이기 위해 제조 기업으로부터 점점 더 많은 관심을 받고 있습니다. 작업장 스케줄링 문제 (JSSP, Job Shop Scheduling Problem)는 한정된 수의 자원에 규정된 시간 내에 작업을 할당하는 문제이며, 이 할당은 예를 들어 생산 비용 절감, 설정 시간 단축, 계획된 시간 단축 등 하나 이상의 목표를 최적화하는 방식으로 이루어집니다. 이러한 목표는 생산 프로세스 간의 시간적 관계와 공유 제조 자원의 제약 조건을 고려하여 각 작업을 실행할 적절한 시간을 도출함으로써 충족됩니다.

더 나아가, 생산 환경의 정보가 변경될 때마다 최적화를 반복해야 합니다. 즉, 위의 아키텍처는 전이 가능한 지식 (transferable knowledge)을 포착하지 못합니다. 강화 학습 접근 방식을 사용하면 지식 포착 문제를 극복할 수 있습니다.

GraSP-RL을 통해 생산 환경을 그래프로 모델링하여 기계와 버퍼 간의 내부 관계를 명시적으로 포착하고 맞춤형 그래프 신경망 GNN을 사용하여 기계 노드의 풍부한 특징 인코딩을 추출합니다. 각 기계 노드에 분산 학습 (distributed learning)과 기계의 지역적 특징 ($local features)만 사용합니다.

---

### Related Work

스케줄링 문제는 NP-난해로 알려져 있으며, 이는 다항 시간 내에 해결될 수 없다고 간주된다는 것을 의미합니다. 초기에는 정확한 해법 (exact solution)을 제공하는 데 중점을 두었지만, 연구의 초점은 근사 해법 (approximate solutions)으로 바뀌었습니다. 따라서 스케줄링 문제를 해결하는 방법은 정확한 방법과 근사 방법 두 가지로 나뉩니다. 

exact solution에는 정수 계획법 (integer programming)과 분기 한정법 (branch and bound)이 포함되는 반면, approximate solutions에는 유전 알고리즘 (GA)과 타부 탐색 (TS)이 포함됩니다. 근사 방법은 합리적인 시간 내에 좋은 해법을 제공하지만, '차원의 저주 (curse of dimensionality)'로 인해 어려움을 겪습니다. 즉, 문제에 기계나 작업이 추가됨에 따라 문제 크기가 기하급수적으로 증가합니다. 또한, 이러한 알고리즘에는 나중에 재사용할 수 있는 지식이 포착되지 않습니다. 즉, 이 알고리즘들은 초기 조건을 기반으로 계획을 세우며, 이 조건들이 변경되면 거의 최적의 계획을 찾기 위해 알고리즘을 다시 실행해야 합니다.

#### Scheduling with Reinforcement Learning

최근 RL의 발전은 거의 최적의 계획을 세울 수 있을 뿐만 아니라, 계획 지식을 포착하여 재사용할 수 있게 하는 매우 흥미로운 길을 열어주었습니다. RL은 바둑, 체스와 같은 보드 게임 및 로봇 공학에서 초인적인 성능을 달성했습니다. 이는 특정 작업을 잘 학습하고 수행할 수 있을 뿐만 아니라, 일반화도 잘 할 수 있으며, 다중 작업 학습이 가능함을 입증했습니다. 딥 신경망 능력의 발전 또한 RL의 응용 분야에서 경계를 확장하는 데 도움이 되었습니다.

더 나아가, 모델-프리 다중 에이전트 접근 방식을 사용하여 적응형 반응형 스케줄링 에이전트를 개발했습니다. 이러한 연구들은 여전히 차원의 저주로 인해 어려움을 겪지만, 다중 에이전트 설정을 사용하여 어느 정도 완화될 수 있습니다. 단일 에이전트에서 다중 에이전트로의 전환은 개별 에이전트가 학습 중에 행동을 변경함에 따라 각 에이전트의 환경은 비마르코프적 (non-markovian)이 되어 부분적으로 관찰 가능한 마르코프 결정 프로세스 (POMDP)를 초래합니다. 초기 접근 방식은 다른 에이전트의 행동이 집단적 보상을 개선하기 위해 이루어진다고 가정하는 $\text{Q}$-학습을 기반으로 하는 협력적 설정을 사용합니다.

#### Using Graph Neural Networks

위에 언급된 모든 RL 발전은 합성곱 신경망 CNN, MLP를 사용하며, 이는 성능을 달성하기 위해 특징 공학 (feature engineering)을 사용합니다. 또한, 딥러닝 방법은 비유클리드 공간 (non-Euclidean space)의 데이터에는 잘 작동하지 않으며, 본질적으로 크기 불변도 순열 불변도 아닙니다. 스케줄링 환경에 단일 기계 또는 작업이 추가되면 이전에 훈련된 상태 개념이 더 이상 유효하지 않기 때문에 에이전트를 처음부터 완전히 다시 훈련해야 합니다.

---

### Method

<img src="https://velog.velcdn.com/images/devjo/post/718dc73e-6e88-42ac-a7ab-a430e1db2891/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### JSSP modeling

주기가 시작되는 시점은 기계가 비어 있고 버퍼에서 작업이 로드될 때입니다. 생산 환경의 현재 데이터는 먼저 해당 노드 및 엣지 정보를 포함하는 환경 상태의 그래프를 구성하는 데 사용됩니다. 구성된 그래프는 GNN을 통해 전달됩니다. 이웃 정보는 모든 노드에서 집계됩니다. 이 단계를 거치면 기계와 나머지 작업 간의 관계가 각 노드의 관점에서 포착됩니다. 현재 비어 있는 기계의 버퍼 $X_8$의 특징 벡터가 RL 에이전트 PPO에게 전달되어 행동 (action)을 취합니다. 이 행동은 해당 기계 버퍼에서 어떤 작업을 기계에 로드해야 하는지 환경에 알려줍니다. 다음 기계가 비게 되면 주기가 반복됩니다.

#### Node features and dynamic edges

기계 노드 $X_m$는 현재 처리 중인 작업의 원핫 인코딩 벡터입니다.

$$
\begin{aligned}
&e_i(x) = \mathbb{1}_A(x) := \begin{cases} 1, & \text{if } x \in A \\ 0, & \text{if } x \notin A \end{cases} \\
&X_m = e_i(j_m), \quad \text{where } j_m \text{ is the current job.}
\end{aligned}
$$

기계 버퍼 노드 $X_{mb}$의 경우 해당 버퍼가 보유하는 모든 작업의 원핫 인코딩 벡터의 합계입니다.

$$
X_{mb} = \sum_{n=0}^{j_{mb}} e_i(n), \quad \text{where } j_{mb} \text{ is the set of jobs.}
$$

노드 특징은 기계 또는 기계 버퍼에 존재하는 작업에 따라 업데이트되는 반면, 엣지는 기계에 존재하는 작업을 기반으로 동적으로 생성됩니다. 예를 들어, $X_1$에 존재하는 작업이 순차적으로 $X_4$와 $X_5$ 기계에서 작업을 수행해야 하는 경우, 엣지 ${e_{X_1 X_4}, e_{X_4 X_2}, e_{X_2 X_5}}$가 생성됩니다. 이렇게 생성된 노드 특징 $X$와 엣지 $E$는 JSSP의 현재 상태를 그래프 $G = (X, E)$로 표현합니다. 그런 다음 그래프 $G$는 GNN으로 전달되어 해당 기계에 대한 인코딩된 특징을 추출합니다.

#### Multi-agent reinforcement learning

이제 노드 특징은 그래프 구조의 비유클리드 공간과 비교하여 더 낮은 차원의 유클리드 공간으로 인코딩됩니다. JSSP에 그래프 표현을 사용하고 GNN을 사용하여 그래프를 처리하는 데는 두 가지 이점이 있습니다.

첫째, 전역 최적화 문제를 지역 최적화 문제로 축소합니다. 즉, 에이전트와 환경 간의 상호 작용의 어느 시점에서든, 우리는 현재 비어 있는 기계에만 집중합니다. $S'$은 RL 에이전트가 분산 방식으로 환경에 작용할 수 있도록 하며, 종단 간 훈련 가능합니다. 여기서 분산이란 동일한 에이전트가 여러 기계에서 행동을 취하는 것을 의미합니다. OSS 및 RJSSP에서의 GraSP-RL의 일반화 성능은 GNN에 의해 수행되는 강력한 이웃 집계 덕분이라고 할 수 있습니다. 이는 기계가 추가되더라도 에이전트가 사용하는 노드 특징이 인접 노드의 정보만 포함하도록 보장하여 지역 최적화 문제로 만듭니다.

둘째, 이전 접근 방식과 달리 상태 공간은 작업 수 증가에 따라 선형적으로만 확장되며, 차원 수는 기계 수와 분리됩니다. 따라서 우리의 접근 방식은 신경망의 크기를 더 작게 만들어 더 나은 샘플 효율성을 제공합니다.

---

### Conclusion

JSSP 접근 방식은 주로 중앙 집중식이며 문제 크기가 증가함에 따라 확장성이 좋지 않습니다. 심지어 RL 접근 방식조차 현재 상태의 벡터화된 표현을 사용하며 JSSP의 그래프 특성에 내재된 귀납적 편향 ($\text{inductive bias}$)을 고려하지 않습니다.

이 문제를 극복하기 위해 먼저 JSSP 환경을 이분 그래프 ($\text{bipartite graph}$)로 설정한 다음, GNN 을 통해 처리하고, 나아가 RL에서 기계의 메시지가 전달된 노드 특징을 사용하여 행동을 취합니다. RL 에이전트의 정보는 기계의 관점에서만 오기 때문에, 이는 지역적 정보를 학습하고 인접 노드에 상대적으로 선행 버퍼 노드에 있는 사용 가능한 작업에 대해 행동을 취합니다. 이것은 기계 간의 관계를 포착할 수 있게 하며, 또한 분산된, 분권화된 학습을 기계 장치에서 직접 가능하게 하는 동시에 보지 못한 시나리오에 잘 일반화되게 합니다.

계획된 해법의 효과는 TS 및 GA와 같은 메타휴리스틱 알고리즘과 FIFO와 같은 우선 순위 디스패치 규칙과 비교됩니다. 결과에서 보여주듯이, 에이전트는 JSSP 작업에서 비교 대상 중 가장 좋은 계획을 제공하며 뛰어난 성능을 보입니다. OSS ($\text{Open Shop System}$) 및 RJSSP ($\text{Reactive JSSP}$)와 같은 새로운 문제 클래스에서 에이전트에 대한 추가 테스트는 에이전트가 FIFO 및 무작위 행동보다 더 나은 성능을 보이며, 추가 훈련 없이 TS 및 GA와 비교 가능한 성능을 제공함을 보여줍니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2009.03836)
