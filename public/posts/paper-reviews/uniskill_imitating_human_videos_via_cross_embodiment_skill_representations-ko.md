---
title: 'UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations'
date: '2025-12-01'
tags: ['embodied ai', 'paper review']
---

### Abstract

Generalist policies에 대한 포괄적이고, 편향되지 않으며, 비교 가능한 평가는 고유하게 도전적입니다. 기존 로봇 벤치마킹 접근 방식은 일반적으로 고정된 평가 작업 및 환경을 지정하거나 중앙 집중식 “로봇 챌린지”를 개최함으로써 강력한 표준화에 의존하며, 광범위한 작업과 환경에 걸쳐 범용 정책을 평가하는 데 쉽게 확장되지 않습니다.

본 연구에서, 실제 세계에서 범용 로봇 정책의 확장 가능한 평가를 위한 새로운 접근 방식인 RoboArena를 제안합니다. 고정된 작업, 환경, 또는 위치를 중심으로 평가를 표준화하는 대신 평가자의 분산 네트워크를 통해 평가를 크라우드 소싱 (crowd-source)할 것을 제안합니다.

중요하게도, 평가자는 자신이 평가할 작업과 환경을 자유롭게 선택할 수 있어 다양성의 쉬운 확장을 가능하게 하지만, 정책 쌍에 대해 이중 맹검 평가 double-blind evaluations를 수행해야 합니다. 그런 다음, 다양한 작업과 환경에 걸쳐 쌍별 비교에서 선호도 피드백을 집계하여 정책 순위를 도출할 수 있습니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/ed90d36a-e288-49a5-9375-d029fe9e1e69/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

인간 비디오로부터 학습하는 것은 로봇별 데이터의 희소성에 대한 확장 가능한 접근 방식을 제공함으로써 로봇 학습의 중심 패러다임으로 부상했습니다. 인간 비디오에는 인간-객체 상호 작용과 같은 일상적인 행동이 포함되어 있으며, 이것은 로봇 학습을 위한 풍부한 기술 원천을 제공할 수 있습니다.

인간 비디오를 로봇이 실행 가능한 기술 표현으로 변환하는 것은 전통적으로 쌍을 이루는 인간-로봇 데이터셋 또는 사전 정의된 의미론적 기술 레이블에 의존해 왔으며, 이 둘 모두 확장하기 어렵습니다. 최근 접근 방식은 명시적인 쌍 지정이나 레이블링 없이 교차 구현체 기술 표현을 학습함으로써 이러한 요구 사항을 우회하는 것을 목표로 합니다.

이러한 목적을 위해, 본 논문에서는 로봇이 보이지 않는 인간 시연을 일련의 로봇이 실행 가능한 기술 표현으로 변환할 수 있도록 대규모 인-더-와일드 비디오 데이터로부터 교차 구현체 기술 표현을 학습하기 위한 확장 가능한 접근 방식인 범용 기술 표현 (UniSkill, Universal Skill representations)을 제안합니다.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/8111dda0-db1c-4948-96d9-2eb0608d203a/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### Problem Formulation

기술 조건부 로봇 정책 $\pi(o_t, z_t)$가 다른 구현체 (예: 인간)에서 나온 길이 $N_p$의 프롬프트 비디오 $\mathbf{V}^p = \{I^p_1, \ldots, I^p_{N_p}\}$**에서 시연된 행동을 복제하는 교차 구현체 모방을 목표로 합니다. $I^p_t$와 $o_t$는 각각 시간 $t$에서의 프롬프트 비디오의 프레임과 로봇 관찰을 나타냅니다. 프롬프트 비디오는 어떤 행동 주석 없이 오직 원시 픽셀 데이터만 포함합니다.

모방을 달성하기 위해 프롬프트 비디오 내의 프레임 쌍 $(I^p_t, I^p_{t+k})$에서 구현체 불가지론적 기술 표현 $z_t$를 추출합니다. 여기서 $k$는 프레임 사이의 시간적 거리입니다. 이 기술 표현 $z_t$는 그런 다음 로봇의 정책 $\pi(o_t, z_t)$를 조건화하는 데 사용되어 비디오 프롬프트에서 시연된 행동을 복제할 수 있도록 합니다.

#### 범용 기술 표현 학습

역 기술 역학(ISD)모델과 순방향 기술 역학(FSD)모델을 도입하여 비디오의 동작 패턴의 구현체 불가지론적 특성을 완전히 활용하며, 이 모델은 대규모, 다중 구현체, 레이블 없는 비디오 데이터셋 $\mathcal{D}_u$에서 훈련됩니다.

$$
z_t = \text{ISD}(I_t, I_{t+k})
$$

원시 RGB 프레임에만 의존하면 시연자의 외모나 장면 맥락과 같은 구현체별 세부 정보의 인코딩으로 이어질 수 있으며, 이것이 구현체 불가지론적 $z_t$의 학습을 방해할 수 있음을 발견했고 각 프레임에 대한 깊이 맵을 생성함으로써 깊이 정보를 통합합니다.

순방향 기술 역학 모델은 $I_t$와 $z_t$가 주어지면 미래 프레임 $I_{t+k}$를 예측합니다.

$$
I_{t+k} = \text{FSD}(I_t, z_t)
$$

#### 범용 기술 조건부 정책

다음 단계는 현재 관찰 $o_t$를 수신하고 $z_t$를 기술 조건화 신호로 활용하는 로봇 정책 네트워크 $\pi_{\phi}(a_{t:t+h} | o_t, z_t)$를 훈련하는 것을 포함합니다.

기술 조건부 정책을 훈련하기 위해 먼저 $\mathcal{D}_a$에서 두 관찰 $o_t$와 $o_{t+k}$를 샘플링하고 사전 훈련된 ISD를 사용하여 기술 표현 $z_t = \text{ISD}(I_t, I_{t+k})$를 추출합니다.

정책 $\pi_{\phi}$는 행동 $a_{t:t+h}$를 예측하며 여기서 $h$는 행동 지평을 나타냅니다.

$$
\phi^{\ast} = \text{argmax}_{\phi} \mathbb{E}_{(o_t, o_{t+h}, a_{t:t+h})\sim\mathcal{D}_a} \left[ \log \pi_{\phi}(a_{t:t+h} | o_t, z_t) \right]
$$

---

### Conclusion

본 논문에서, 훈련 중에 장면 정렬된 교차 구현체 데이터셋에 의존하지 않고 교차 구현체 도전 과제를 성공적으로 해결하는 새로운 접근 방식인 UniSkill을 제안합니다. 이전 연구와 달리, UniSkill은 구현체에 걸쳐 일반화되는 공유 기술 표현을 학습하기 위해 다양한 구현체에 걸친 레이블 없는 대규모 비디오 데이터셋을 활용합니다.

구현체 불가지론적 역학을 기술 표현으로 효과적으로 인코딩하여, 구현체 불일치에도 불구하고 정책이 비디오 프롬프트에서 행동을 복제할 수 있도록 허용합니다.

하지만 고정된 기술 간격에 의존하며, 이것은 인간과 로봇 시연 사이의 다양한 실행 속도에 적응하는 능력을 제한합니다. 가변적인 기술 지속 시간을 허용하면 구현체에 걸친 동작 속도의 차이를 처리하는 데 유연성을 향상시킬 수 있습니다.

그리고 급격한 시점 변화를 보이는 비디오, 특히 1인칭 시점 인간 비디오에서 어려움을 겪습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2505.08787)
