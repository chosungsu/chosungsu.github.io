---
title: 'Attention-based Priority Learning for Limited Time Multi-Agent
Path Finding'
date: '2025-02-13'
tags: ['robotics', 'paper review']
---

### Abstract

최근 로봇 응용 분야에서 중요함에도 불구하고, 제한된 시간 내에 대규모 다중 에이전트 경로 찾기(MAPF) 문제를 해결하는 것은 여전히 난제로 남아 있습니다. 최근의 학습 기반 방법들은 기존 방법들보다 더 잘 확장되지만, 최적 상태에 미치지 못하며 대규모 사례에서 제한된 시간 내에 낮은 성공률을 보이는 경우가 많습니다. 이러한 한계는 종종 그들의 블랙박스(black-box) 특성에서 비롯됩니다.

본 연구에서는 이러한 과제를 명시적으로 해결하기 위해 우선순위 계획(prioritized planning)을 학습 기반 방법과 통합하는 하이브리드 접근법을 제안합니다. 우리는 우선순위 계획을 마르코프 결정 과정(Markov Decision Process)으로 공식화하고 강화 학습(reinforcement learning) 기반의 우선순위 계획 패러다임을 도입합니다. 이를 통해, 우리는 에이전트 간의 충돌/차단 관계를 학습하고 차단 없는 우선순위를 제공하기 위해 새로운 합성 점수 기반 주의 네트워크(S2AN, Synthetic Score-based Attention Network)를 개발합니다.

우선순위 메커니즘을 통합하고 향상된 다중 에이전트 협력 전략을 위해 새로운 주의 기반 신경망을 활용함으로써 확장성을 희생하는 대신 솔루션의 완전성(completeness)을 향상시키고 선형 시간 복잡도(linear time complexity)를 유지하여, 대규모 MAPF 작업을 위한 견고한 길을 제공합니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/4b5ec4e4-51af-4ece-b87b-db0cd42b0d2c/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

다중 에이전트 경로 찾기(MAPF)는 여러 에이전트가 총 이동 시간을 최소화하면서 미리 할당된 목표 지점까지 충돌 없이 경로를 계획해야 하는 핵심적인 다중 에이전트/로봇 문제입니다. 그러나 차원의 저주(curse of dimensionality)로 인해, 대규모 MAPF를 제한된 시간 내에 해결하는 것은 여전히 중요한 난제로 남아 있습니다. 예를 들어, 게임 시나리오에서는 많은 수의 에이전트가 충돌 없는 경로를 신속하게 찾고 플레이어의 몰입도를 유지할 수 있도록 실시간 계산 및 경로 렌더링이 필요합니다. 본 연구에서는 대규모 MAPF에 중점을 두고, 기계 학습을 통해 우선순위 계획(prioritized planning)의 성능을 향상시켜 높은 완전성(completeness)과 계산 시간 사이의 효율적인 균형을 맞추는 것을 목표로 하는 접근법을 제시합니다.

MAPF 알고리즘은 비학습(non-learning) 및 학습(learning) 접근법으로 분류할 수 있습니다. 비학습 알고리즘은 체계적 탐색 알고리즘(systematic search algorithms), 규칙 기반 알고리즘(rule-based algorithms), 그리고 우선순위 기반 알고리즘(prioritized algorithms)으로 더 나눌 수 있습니다.

CBS 같은 체계적 탐색 알고리즘은 NP-난해(NP-Hard)한 특성으로 인해, 이들은 대규모 시나리오를 처리할 때 어려움에 직면하며 최악의 경우 지수 시간(exponential-time)을 보입니다. 반면에, MAPF-LNS2와 PBS 같은 규칙 기반 알고리즘은 다항식 시간(polynomial time) 내에 솔루션을 찾는 것을 보장하지만, 실제 응용 분야에서는 계획 시간이나 메모리 문제에 부딪힐 수 있습니다.

이와 대조적으로, 학습 기반(learning-based) 방법은 전역 정보를 활용하고 라우팅 문제와 같은 많은 다른 응용 분야에서 높은 성능을 달성하는 데 뛰어난 능력을 보여주었습니다. 중앙 집중식 다중 에이전트 강화 학습(MARL)은 최적성이나 완전성을 보장할 수 없습니다. 그들의 블랙박스 특성으로 인해, 학습 기반 방법은 행동 일관성 및 하위 수준 최적성이 부족합니다. 이는 종종 모든 에이전트를 목표 지점으로 안내하는 데 연장된 시간을 요구하게 만듭니다.

이러한 한계를 해결하기 위해, 본 논문은 학습과 우선순위 계획을 결합한 다중 에이전트 경로 찾기(MAPF)를 위한 새로운 패러다임을 소개합니다. 기계 학습의 힘을 활용하여 전역 정보에 대해 추론하고 에이전트에 대한 고품질의 우선순위를 직접 생성합니다. 이러한 우선순위는 에이전트를 위한 거의 최적의 하위 수준 경로를 신속하게 찾는 데 사용되며, 따라서 높은 완전성과 낮은 시간 복잡도로 문제를 해결합니다.

---

### Related work

#### 1. MAPF Algorithms

최소 비용, 충돌 없는 솔루션을 찾는 데 중점을 둔 MAPF 문제를 해결하기 위해 수많은 알고리즘이 개발되었습니다. 최적의 MAPF 해결책은 NP-완전(NP-complete)이며, 종종 확장성 문제에 직면합니다. 제한적 준최적(bounded suboptimal) 해결사는 솔루션의 비용이 주어진 최적 값의 범위 내에 있음을 보장하며, 상대적으로 빠른 속도로 작동합니다. 비제한적 준최적(unbounded suboptimal) 솔루션은 빠른 계산 속도를 제공하며 대규모 에이전트 시나리오에서 훌륭한 성능을 보여, 연구 및 산업 분야 모두에 응용됩니다.

고전적인 비학습 기반 접근법 (Classic non-learning-based approaches)에는 규칙 기반 알고리즘(rule-based algorithms), 무한한 경계 요소를 가진 제한적 준최적 알고리즘(bounded-suboptimal algorithms), 우선순위 계획 알고리즘(prioritized planning algorithms)을 포함합니다.

#### 2. 우선순위 계획 (Prioritized Planning)

우선순위 계획 알고리즘은 완전하거나 최적적이지는 않지만, MAPF 문제를 해결하기 위한 계산적으로 저렴하고 매우 효과적인 접근법을 나타냅니다. 각 에이전트에는 고유한 우선순위가 할당되고, 에이전트는 사전에 정의된 전역 우선순위에 따라 순서가 정해집니다. 가장 높은 우선순위를 가진 에이전트는 자신의 개별 최적 경로를 먼저 계산할 수 있습니다.

Attention routing problem은 MAPF와 유사성을 공유하며, 또한 NP-난해입니다. 백트래킹은 전체 우선순위 공간의 반복적인 탐색을 필요로 합니다. 이러한 방법들은 실행 가능한 솔루션이 발견될 때까지 공간의 하위 집합을 반복적으로 탐색합니다. CBS와 유사하게, 생성된 경로에서 발생하는 충돌을 기반으로 공간을 분할하며, P-완전(p-complete)임이 증명됩니다.

---

### Method

마르코프 결정 과정(MDP) 프레임워크 내에서 우선순위 문제를 다루는 본 논문에서의 접근법을 설명합니다. 문제를 MDP로 공식화하고, 이어서 학습 기반 모델이 MAPF 문제와 관련된 데이터 내의 패턴과 관계를 학습할 수 있도록 하는 두 가지 중요하고 시간 효율적인 특징을 강조합니다. 다음으로, 이러한 중요한 특징들로부터 잠재 정보를 추출하고 인코딩-디코딩 방식으로 높은 완전성을 가진 우선순위 시퀀스를 제공하도록 학습하는 S2AN를 제안합니다.

#### 1. Markov Decision Process Formulation

<img src="https://velog.velcdn.com/images/devjo/post/a8d21cb5-c577-473e-8486-ab6e2e593c64/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

MAPF 사례는 세 가지 기본 구성 요소, 즉 장애물 지도, 시작 위치, 그리고 목표 위치로 구성됩니다. 초기 상태 $𝑠1$에서, 상위 수준 우선순위 계획자는 $𝑎1$과 같은 에이전트를 행동으로 선택합니다. 이어서, 하위 수준 단일 에이전트 경로 계획자는 이 에이전트의 경로를 계산하며, 이전에 계획된 에이전트의 경로를 부드러운 이동 장애물(soft-moving obstacles)로 간주합니다. 목표는 실행 가능한 솔루션을 찾는 것입니다. 우리는 충돌을 최소화함으로써 이를 달성합니다. 새로운 충돌이 감지되면, $𝑅(𝑠𝑡, 𝑎𝑡)$는 새로운 충돌 에이전트의 음수 값과 같습니다. 에피소드가 종료되고 충돌 없는 솔루션이 발견되면, 우리는 긍정적인 에피소드 보상을 제공합니다.

#### 2. Feature Design

<img src="https://velog.velcdn.com/images/devjo/post/a9c1925b-2f49-4eea-96ca-cbe7f160fd5b/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

Feature design은 MAPF 문제를 해결하기 위한 학습 기반 모델의 잠재력을 최대한 발휘하는 열쇠입니다. 유해 특징(harmful feature)과 목표 행렬(target matrix)을 포함하는 두 가지 잘 설계된 특징을 사용합니다.

유해 특징은 에이전트의 목표가 이웃을 차단하고 있는지 여부를 결정하기 위해 지역 연결성을 평가합니다. 이 특징이 에이전트의 목표가 유해한지 아닌지를 구별하는 방법을 보여주는 예시를 제공합니다. 실제로, 목표 차단(goal blocking)은 흔한 발생입니다. 한 에이전트의 목표가 다른 에이전트를 방해할 때, 그 우선순위는 낮아져야 합니다. 그러나 모든 차단 쌍을 수동으로 감지하는 것은 시간이 많이 소요될 수 있습니다. 결과적으로, 우리는 학습 기반 모델에서 제공된 유해 특징을 활용하여 이러한 차단 관계를 자율적으로 식별하는 것을 목표로 합니다.

반면에, 또 다른 특징인 목표 행렬은 MAPF-LNS2에서 영감을 받은 최소 목표 방문 $A^*$ 알고리즘(MTVA)로부터 파생하였습니다. 탐색은 경로를 찾기 위해 열린 집합에서 최소 $𝑓$ 값을 가진 노드를 탐색하는 것을 우선시합니다. $𝑓$는 노드의 목표까지의 휴리스틱 거리와 이동 거리의 합입니다. 그러나 MTVA에서는 최소 $𝑓$ 값에만 초점을 맞추는 대신, 다른 에이전트의 목표에 대한 방문 횟수$(𝑡𝑣)$가 가장 적은 노드를 탐색합니다. 여러 노드$(|𝑁|)$가 최소 $𝑡𝑣$ 값을 공유하는 경우, $|𝑁|$ 내에서 가장 작은 $𝑓$ 값을 가진 노드를 선택합니다.

#### 3. Policy Network

S2AN은 Transformer 기반으로 구축되었으며 하이브리드 초기 임베딩, 주의 기반 인코더, 합성 점수 기반 디코더로 구성됩니다.

하이브리드 초기 임베딩에서는 raw features(장애물 지도, 시작 위치, 그리고 목표 위치)를 고차원 특징 공간에 매핑함으로써 얻어집니다. 사례 특징(instance features)과 도메인 특정 특징인 유해 특징(harmful feature)을 따로 처리합니다. ResNet을 사용하여 사례 특징을 $𝑑_1$차원 벡터인 $𝑥_{𝑖1}$로 투영합니다. 다른 한편으로, 우리는 선형 계층(linear layer)을 활용하여 부울 변수인 유해 특징을 $𝑑_2$차원 벡터인 $𝑥_{𝑖2}$로 매핑합니다. 이어서, 이 두 벡터는 연결되어 에이전트 $𝑖$의 $𝑑$차원 하이브리드 초기 임베딩 $𝑥_𝑖$를 형성합니다.

주의 기반 인코더는 $X_1=\{x_i\}_{1}^{n}$이라는 초기 임베딩의 output을 input으로 받아 모든 에이전트에 대한 고급 임베딩 행렬인 emb를 생성합니다. Transformer와 유사한 인코더 구조를 사용하고 $L$개의 attention layers로 구성됩니다. 에이전트 간의 관계를 더 효과적으로 포착하기 위해 통신을 용이하게 하는 멀티 헤드 주의(MHA, multi-head attention) 하위 계층과, 그 사이에 ReLU 활성화 함수가 있는 두 개의 선형 투영 계층으로 구성된 피드 포워드(FF, feed-forward) 하위 계층으로 구성됩니다. 각 하위 계층 다음에는 스킵 연결(skip-connection)과 배치 정규화(BN, batch normalization)가 따릅니다. 궁극적으로 emb=$x^{L+1}$을 출력으로 얻습니다.

합성 점수 기반 디코더는 임베딩 행렬 𝑒𝑚𝑏를 기반으로, 우리의 합성 점수 기반 디코더는 자기 회귀(autoregressive) 방식으로 MAPF 문제에 대한 솔루션을 구성합니다.

#### 4. Reinforcement learning

강화학습의 손실은 보상 기대치의 음수$(L_{RL}=-E_{\tau \sim \pi_{\theta}}[R])$로 정의됩니다. 그릴고 손실을 최소화하기 위한 기울기는 다음과 같습니다.

$$
\nabla_{\theta} L_{RL}=-E_{\tau}[\sum_{t=1}^T G_t \nabla_{\theta} log \pi_{\theta}(a_t|s_t)], \\
L_{entropy}=-Entropy(\pi_{\theta})
$$

여기서 $𝑇$는 총 시간 단계이며 에이전트의 수 $𝑛$과 같습니다. $\tau$는 정책 $\pi_{\theta}$에 의해 생성된 선택된 행동 시퀀스입니다. 모델이 너무 빨리 준최적 정책으로 수렴하는 것을 방지하기 위해 엔트로피 손실이 도입됩니다.

#### 5. Reachability Repair Algorithm

전통적인 비분산형 방법에서, 계획된 궤적은 종종 완전히 실행 가능하거나 충돌로 인해 실행될 수 없는 이진 결과를 제시합니다. 반면에, 분산형 방법, 특히 학습 기반 방법은 자주 충돌 없는 솔루션을 산출합니다. 실행 가능한 솔루션을 찾는 데 실패할 때, 그들은 충돌 없는 솔루션을 출력하지만, 모든 에이전트가 목표에 도달하지는 않습니다. 따라서 한계를 해결하기 위해 빠르고 효과적인 후처리(post-processing) 방법을 제공하는 것이 필요하다고 생각하였습니다.

모든 충돌 에이전트에 대한 경로를 재계획하여, 그들이 목표에 도달하는 대신 다른 에이전트와의 충돌을 피하도록 보장하는 것을 포함합니다. 이는 충돌 에이전트를 위해 SIPPS를 반복적으로 실행하고, 충돌을 피하기 위해 목표를 시작 지점으로 조정함으로써 달성될 것입니다.

구체적으로, 먼저 탐욕적인 방식으로 행동을 디코딩하여 초기 솔루션 $P = (a_1, P_{a1}) \cup (a_2, P_{a2}) \cup \dots \cup (a_n, P_{an})$를 얻으며 솔루션 $𝑃$에서 충돌하는 모든 에이전트 집합 $𝐶$를 선택합니다. 충돌이 없으면, 실행 가능한 솔루션이 발견되어 𝑃로 반환될 것입니다. 그렇지 않으면, 솔루션을 복구해야 합니다.

---

### Experiment

세 가지 측면에 대해 광범위한 실험적 평가를 수행했습니다.

1.동일한 최대 계획 시간 하에서 다양한 MAPF 알고리즘의 성공률(success rates)을 비교

2.현재의 학습 방법들과 비교하여 성공률, 에피소드 길이(episode length), 도달률(reach rate)을 비교하여 장점을 제시

3.알고리즘의 두드러진 구성 요소의 효과를 비판적으로 평가하기 위해 절제 연구(ablation study)를 시작

훈련 절차에서, 배치 크기는 64로 설정되었고 초기 임베딩 네트워크를 위해, ResNet18을 사용하여 지도를 인코딩하였습니다. 인코더와 디코더 네트워크 모두에서, 128차원 특징을 가진 8개의 주의 헤드를 통합하고 인코더는 2개의 계층으로 구성합니다.

에이전트 수가 200개를 초과할 때 S2AN은 성공률 측면에서 PRIMAL을 능가합니다. DHC는 강력한 휴리스틱 지도 특징을 활용하지만, 빈 지도 시나리오에서는 그 효능이 현저히 감소하여 더 낮은 성공률과 도달률을 초래합니다. 평균 에피소드 길이에 관해서는, S2AN은 일관성 없는 행동을 피하는 능력 때문에 주로 우수합니다.

엔트로피 손실이 생략될 때, 테스트 보상에서 상당한 57% 감소를 관찰합니다. 그럼에도 불구하고, 이 성능은 무작위 정책에 비해 여전히 더 나으며, 이는 네트워크가 탈출하기 어려운 준최적 정책으로 빠르게 수렴할 수 있음을 시사합니다. raw features의 부재는 59%의 감소를, 유해 특징의 부재는 최소한의 영향만을 미칩니다.

---

### Conclusion

본 논문에서, 제한된 시간 내에 대규모 MAPF(Multi-Agent Path-Finding)를 해결하는 과제를 다루는 것을 목표로 하는 효과적인 학습 기반 패러다임을 제안합니다. 우선순위 계획을 주의 기반 학습 알고리즘(attention-based learning algorithm)에 통합함으로써, 선형 시간 복잡도 내에서 완전성과 솔루션 품질을 모두 향상시키는 포괄적인 다중 에이전트 협력 전략을 달성했습니다. 동시에, 학습 메커니즘을 우선순위 계획 프레임워크에 통합하고 우선순위 계획을 마르코프 결정 과정(Markov Decision Process)으로 공식화함으로써, 이 방법은 MAPF에 대한 더 정보에 입각한 탐색 과정을 가능하게 합니다.

실험은 주의 기반 우선순위 학습 방법이 계산 시간과 완전성 사이의 균형을 달성하며, 대규모 경로 찾기 작업에서 성공률, 도달률, 그리고 솔루션 품질 측면에서 다양한 학습 기반 계획자들을 능가함을 입증합니다. 더욱이, PIBT 및 제한된 시간의 PBS와 비교하여, 본 논문에서 제안한 방법은 중간에서 대규모의 밀집된 장애물 시나리오에서 높은 완전성을 보여주며, 다양한 MAPF 알고리즘을 가속화하는 데 적합합니다.

---

### 참고 자료

[원본 경로 #1](https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1993.pdf)



