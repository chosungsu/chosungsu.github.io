---
title: 'Embodied Task Planning with Large Language Models'
date: '2025-10-14'
tags: ['embodied ai', 'paper review']
---

### Abstract

Equipping embodied agents with common sense is crucial for robots to successfully complete complex human commands in general environments. While recent Large Language Models (LLMs) can embed rich semantic knowledge in agents for generating plans for complex tasks, they lack information about the real world and typically produce infeasible action sequences.

In this paper, we propose a Task Planning Agent (TaPA) for embodied tasks that generates executable plans based on objects present in scenes for grounded planning with physical scene constraints. This is achieved by aligning LLMs with visual perception models.

Specifically, we first build a multimodal dataset containing triplets of indoor scenes, commands, and action plans. Here, we provide GPT-3.5 with prompts and lists of objects present in scenes to generate multiple commands and their corresponding planned actions. The generated data is utilized for grounded planning tuning of pre-trained LLMs.

During inference, we extend an open-vocabulary object detector to multi-viewpoint RGB images collected from accessible different positions to discover objects in scenes.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/48503e87-d671-4be6-ba60-4d68096b332a/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

Equipping embodied agents with general common-sense knowledge to perform complex tasks based on natural language commands is desirable in many applications such as household services, medical treatment, and agricultural harvesting. Due to limited training samples and diverse tasks, directly training embodied agents across various deployment scenarios is infeasible.

#### Limitations of LLMs and Need for Grounded Planning

LLMs cannot perceive surrounding scenes and may generate infeasible actions due to interaction requirements with non-existent objects. For example, given a human command "Please give me wine," action steps generated from GPT-3.5 include "pouring wine from a bottle into a glass." A realistic scene may only have mugs instead of wine glasses, and the executable action should be "pouring wine from a bottle into a mug." Therefore, grounding task plans generated by LLMs into the physical world is essential for building embodied agents to perform complex tasks.

#### Limitations of Prior Work

To obtain executable task plans in given physical scenes, many previous studies filter or align generated actions by considering visual cues of scenes for common manipulation tasks of tabletop objects. This fails to meet the requirements of numerous complex tasks and diverse deployment scenarios.

#### TaPA Framework Proposal

In this paper, we present a task planning agent called TaPA (Task Planning Agent) for embodied task planning grounding in physical scenes.

It acquires general common-sense knowledge to produce action steps for complex household tasks such as making sandwiches and setting tables, providing fundamental commands for downstream navigation and manipulation processes to handle high-level human requirements.

More specifically, we build a multimodal dataset where each sample is a triplet of visual scenes, commands, and corresponding plans. Utilizing the generated dataset, we fine-tune a pre-trained LLaMA network by predicting action steps based on object lists in scenes, which serves as a task planner. During inference, to obtain object lists, the embodied agent effectively visits standing points to collect RGB images that provide sufficient information from various viewpoints, and generalizes an open-vocabulary detector to multi-viewpoint images to obtain lists of existing objects.

---

### Methods

<img src="https://velog.velcdn.com/images/devjo/post/0d832899-d8a0-44ef-b22d-9eac151e8f8c/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

#### Embodied Task Planning Data Generation

While large Vision-Language Models (VLMs) and large multimodal models have achieved remarkable performance on a wide range of complex perception tasks, embodied task planning grounded in realistic indoor scenes remains challenging due to the lack of large-scale multimodal datasets for training planning agents.

#### Scene Information Embedding and Data Generation Pipeline

Given an embodied 3D scene $X_s$, we directly use class names of all objects as the representation of the scene, denoted as $X_l$.

To efficiently generate large-scale complex commands $X_q$ and executable corresponding plans $X_a$, we design prompts that simulate embodied task planning scenarios where GPT-3.5 automatically synthesizes data based on object name lists $X_l$. For dataset generation, we directly use Ground Truth labels of instances present in scenes for object lists utilized in prompts.

#### Grounding Task Planning to Surrounding Scenes

<img src="https://velog.velcdn.com/images/devjo/post/b1e8609f-750a-474e-9906-b67cae5a5425/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

To ground embodied task planning into the physical world with feasibility constraints, accurately obtaining object lists in scenes without instance omissions or false positives is essential. Since new objects unseen during detector training may appear in deployment scenarios, we generalize an open-vocabulary object detector for acquiring object lists. The agent collects RGB images from various positions to perceive visual scenes and discover existing objects.

Position selection criteria include traversal positions, random positions, overall center point, and block-wise center points, and the agent rotates the camera at each position selection criterion to obtain multi-viewpoint images. Therefore, we formally write the image collection strategy $S$ as:

$$
S = \{(x, y, \theta) | (x, y) \in L(\lambda, A), \theta = k\theta_0\}
$$

where $(x, y, \theta)$ represents position and camera orientation. $L(\lambda, A)$ denotes a position selection criterion with hyperparameter $\lambda$, and all sampled positions must be within accessible area $A$. The unit angle for camera rotation is set to $\theta_0$, and $k$ is an integer that enables the agent to collect visual cues from different directions of the scene.

The task planner requires information about all objects present in scenes to generate executable action steps, and we generalize an open-vocabulary object detector to collected multi-viewpoint RGB images to acquire object lists. The predicted object list $\hat{X}_l$ for the scene is obtained by removing duplicate object names from detection results of multi-viewpoint images.

$$
\hat{X}_l = R_d\left[\bigcup_i D(I_i)\right]
$$

where $R_d$ is an operation that removes duplicate object names and $D(I_i)$ represents detected object names for the $i$-th RGB image collected from the scene.

$$
X_a = \text{TaPA}(P_{\text{in}}, \hat{X}_l, X_q)
$$

Human command $X_q$ and predicted object list $\hat{X}_l$ are considered in TaPA to generate executable action plan $X_a$.

---

### Conclusion

In this paper, we presented a Task Planning Agent (TaPA) for embodied task planning, where executable action steps are generated to complete human commands through subsequent robot navigation and manipulation.

We first built a multimodal dataset consisting of triplets containing visual scenes, commands, and corresponding plans. This dataset was generated by GPT-3.5 considering lists of all objects in scenes and designed text prompts, which is utilized to tune command models to generate executable actions.

For inference, we collect multi-viewpoint RGB images from accessible different positions and leverage an open-vocabulary object detection framework to discover object lists in scenes for fine-tuned command models.

Extensive evaluation results show that TaPA outperforms state-of-the-art LLMs and LMMs in terms of plausibility of generated action plans.

---

### References

[Original Source #1](https://arxiv.org/pdf/2307.01848)
