---
title: 'GNN-Transformer Task Planning Enhanced with Semantic-Driven Data Augmentation'
date: '2025-10-10'
tags: ['embodied ai', 'paper review']
---

### Abstract

자연어는 인간이 로봇과 상호 작용하는 가장 직관적인 수단이므로, 자연어 명령 기반 작업 계획은 오랜 연구 분야였습니다. 대규모 언어 모델 (LLM)은 언어 및 상식에 대한 이해를 향상시켜 작업 계획을 크게 개선했습니다. 그러나 현재 방법론은 다음과 같은 몇 가지 문제점에 직면하고 있습니다.

$\Rightarrow$ 물리적 환경에 대한 깊은 이해가 부족합니다.

$\Rightarrow$ 성능이 프롬프트 예시에 크게 의존합니다.

$\Rightarrow$ LLM이 너무 크고 특정 작업에 맞춤화되지 않습니다.

$\Rightarrow$ 계획 비용이 여전히 높습니다.

이러한 문제를 극복하기 위해, 의미론적 환경을 활용하고 이력 상태 데이터를 통합하여 작업 수준 행동을 예측하도록 설계된 GNN-Transformer 작업 계획기 (GNN-Transformer Task Planner, GTTP)를 소개합니다.

GTTP 아키텍처는 GNN 계층을 사용하여 확장 가능하며, 트랜스포머 계층은 작업 진행 상황 이해를 용이하게 합니다. 또한, 텍스트 인코더를 사용하여 환경을 임베딩함으로써 시뮬레이션된 데이터셋에서 훈련되고 실제 시나리오에 직접 적용될 수 있습니다.

---

### Introduction

<img src="https://velog.velcdn.com/images/devjo/post/43601d5e-b169-447f-8777-7e8f1c328bfd/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

#### LLM을 활용한 작업 계획 연구의 발전과 한계

Huang et al. (2022) 및 Singh et al. (2023)에서는 LLM이 프롬프트 엔지니어링을 통해 고수준 계획기 역할을 하며, 허용 가능한 행동, 로봇의 상태, 환경을 프롬프트에 통합합니다. 그러나 이 접근 방식은 원하는 결과를 얻기 위해 정확하게 설계된 프롬프트를 필요로 하며 프롬프트 내의 예시에 크게 의존한다는 제약이 있습니다. 게다가, 이러한 계획기들은 실행 가능성 보장 및 계획 최적화에 어려움을 겪습니다.

이러한 한계를 극복하기 위해 상식 지식을 통해 탐색 효율성을 높이면서 몬테카를로 트리 탐색 방법이 사용됩니다.

#### GTTP 프레임워크 제안 및 기여

객체에 대한 의미론적 정보를 통합하는 장면 그래프를 입력으로 받으며, 이는 텍스트 임베딩 모듈을 통해 인코딩됩니다. 이 정보는 GNN에 의해 처리되어 변화하는 객체 수를 수용하고, 트랜스포머 모델과 결합하여 이력 상태를 고려합니다. 이 계획기는 실행을 위한 가장 확신 있고 실행 가능한 고수준 계획을 선택합니다. GTTP의 장점은 다른 모델 대비 우수한 성능, 최대 $715$개 객체까지의 확장성, 뛰어난 sim-to-real 전이성을 포함합니다.

모델을 훈련하려면 상당한 양의 데이터가 필요하며, 이는 현재 불충분합니다. 이를 극복하기 위해 단 $45$개의 시드 데이터로부터 $14,666$개의 데이터 포인트를 자동으로 생성할 수 있는 LLM 기반 의미론적 데이터 증강 방법을 제안합니다. 이 방법은 의미론적 객체 정보를 사용하여 작업 계획을 자동으로 증강하고 언어 명령으로 주석을 달 뿐만 아니라, 데이터의 품질을 보장하기 위해 상식적 유효성을 평가합니다. 이 접근 방식은 인적 노력을 크게 줄이고, 훈련 강건성을 높이며, 대규모 데이터셋 개발의 효율성을 개선합니다.

---

### Methods

명령 주도 고수준 작업 계획 문제를 해결하기 위해, 입력이 목표 명령 $I$와 상태 그래프 이력 $g_{0:t}$으로 구성되는 GNN-Transformer 작업 계획기를 소개합니다.

<img src="https://velog.velcdn.com/images/devjo/post/22dae9cd-3eb3-40fa-a3dc-08c3c2e7fc56/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

목표 명령 $I$와 장면 그래프 $g_t$는 각각 명령 인코더 $\phi_I$와 장면 그래프 인코더 $\phi_g$에 의해 인코딩됩니다. 그 후, 인코딩된 목표 명령과 장면 그래프는 이전 상태 그래프의 이력과 함께 하위 목표 계획 네트워크 $F_{\pi}$에 공급되어 진행 중인 작업 시퀀스의 맥락을 기반으로 잠재적인 하위 목표의 확률 분포를 추정합니다. 결과적으로, 하위 목표 선택기는 가장 높은 확률을 가진 실행 가능한 하위 목표 $\pi_{t+1}$를 선택합니다.

#### Encoding Scene Graphs and Instructions

장면 그래프 $g_t$의 각 노드 $v_t^i$와 엣지 $e_t^{ij}$는 위 이미지에 나타난 바와 같이 JSON 형식의 환경에 대한 의미론적 정보를 포함합니다. 장면 그래프 인코더 $\phi_g$는 텍스트 임베딩 모델을 활용하여 $v_t^i$와 $e_t^{ij}$를 각각 노드 특징 $\hat{v}_t^i$와 엣지 특징 $\hat{e}_t^{ij}$라고 불리는 384차원 벡터로 변환합니다. 임베딩된 장면 그래프 $\hat{g}_t$는 원래의 그래프 구조를 보존하여 객체 간의 의미론적 관계를 유지합니다.

명령 문장 $I$는 명령 인코더 $\phi_I$에 입력되어, 문장 임베딩 벡터 $u$와 토큰 임베딩 집합 $l_{0:m}$으로 구성된 명령 특징 $\hat{I}$을 생성합니다($m$은 토큰의 수). 본 논문에서는 Sentence-BERT (Reimers and Gurevych 2019)가 상태 인코더 $\phi_g$와 목표 인코더 $\phi_I$ 모두에 채택되어 동일한 잠재 공간을 공유하도록 보장합니다.

#### Subgoal Planning Network Architecture

인코딩된 상태 그래프의 이력 $\hat{g}_{0:t}$과 인코딩된 명령 특징 $\hat{I}$을 처리하여 하위 목표 $\pi_{t+1}$를 계획하는 하위 목표 계획 네트워크를 제안합니다.

그래프 특징 추출기는 GNN 계층, 그래프 토큰 생성기, 진행 상황 맥락 융합 모듈을 사용하여 인코딩된 장면 그래프와 인코딩된 명령을 모두 해석합니다. 주어진 장면 그래프 $\hat{g}_{0:t}$와 목표 명령 $u$로부터 작업 관련 특징을 추출하기 위해, GNN 계층의 스택을 사용했습니다.

세 가지 업데이트 함수를 설계하였고 엣지 업데이트 함수 $\rho_t$는 다음과 같이 공식화되고 

$$
\tilde{e}_{t}^{ij} = \rho_e(\hat{v}_t^i, \hat{v}_t^j, \hat{e}_t^{ij}, u) = g(\hat{v}_t^i \| \hat{v}_t^j \| \hat{e}_t^{ij} \| u)
$$

여기서 $g(\cdot)$는 다층 퍼셉트론 (MLP) 함수이고 $\|$는 벡터 연결입니다. 업데이트된 각 엣지 $\tilde{e}_{t}^{ij}$는 연결된 노드와 제공된 명령 특징 $u$로부터 정보를 집계합니다.

다음으로 노트 특징 $\tilde{v}_t^i$는 함수 $\rho_v$에 의해 업데이트됩니다.

$$
\tilde{v}_t^i = \rho_v(\hat{v}_t^i, N_t^i, u) = g\left(\hat{v}_t^i \| \frac{1}{|N_t^i|} \sum_{k \in N_t^i} g(\hat{v}_t^k \| \tilde{e}_{t}^{ki}) \| u\right)
$$

여기서 $N_t^i$는 $v_t^i$의 이웃 노드 집합입니다. 업데이트된 노드 $\tilde{v}_t^i$는 주변 환경 정보를 포함합니다.

마지막으로, $u$는 그래프의 전역 특징으로 삽입되며, 모든 업데이트된 노드를 단순히 평균하여 업데이트됩니다.

$$
\tilde{u}_t = \rho_u(\tilde{V}_t, u) = g\left(u \| \frac{1}{|\tilde{V}_t|} \sum_{i \in \tilde{V}_t} \tilde{v}_t^i\right)
$$

여기서 $\tilde{V}_t$는 업데이트된 노드 임베딩 집합입니다. 본 논문에서 $\text{GNN}$ 계층의 수 $N_{\text{GNN}}$은 $3$으로 설정되고, 업데이트된 노드, 엣지, 및 명령 특징의 차원은 각각 $256, 128, 64$로 줄어듭니다.

#### Subgoal Predictor

하위 목표 예측기는 다음 행동과 다음 목표 객체에 대한 확률 분포($p(a_{t+1})$ 및 $p(o_{t+1})$)를 생성하기 위해 이력 상태 정보를 해석합니다. 전반적인 구조는 일반 트랜스포머 아키텍처의 수정된 버전입니다.

처음에, 작업 인코더 (Task Encoder)는 위치 임베딩과 함께 상태 토큰의 시퀀스를 소비하여 진행 중인 작업의 이력 정보를 인코딩하는 작업 토큰을 생성합니다. 이 작업 토큰은 위치 임베딩과 요소별로 추가된 후, 순차적 이력 특징을 전달하기 위해 행동 디코더와 객체 디코더 모두에 공급됩니다. 행동 디코더는 상태 토큰을 사용하여 MLP 및 소프트맥스 계층을 통해 각 입력 상태에 대한 행동 확률을 출력합니다. 객체 디코더는 업데이트된 노드 임베딩 $\tilde{V}_t$를 사용하여 그들 중에서 중요한 목표 객체를 찾습니다.

---

### Conclusion

본 논문에서 소개된 아키텍처는 장면으로부터 의미론적 정보를 해석하고 다양한 수의 객체를 관리하기 위한 GNN 계층과, 이력 프로세스를 고려하기 위한 트랜스포머 계층을 결합합니다. 또한, 최소한의 사람 개입으로 대규모의 고품질 훈련 데이터셋 구축을 가능하게 하는 의미론 기반 증강 방법을 제시합니다.

실험은 제안된 방법이 다양한 길이의 작업 전반에 걸쳐 기준 알고리즘보다 상당히 우수하며 강건성을 유지함을 입증합니다. 더욱이, 시뮬레이션 데이터만으로 훈련된 계획기는 실제 환경에서 효과적임을 입증했습니다. 이는 뛰어난 성능, 복잡한 환경에서의 확장성, 효과적인 sim-to-real 전이성을 보여줍니다.향후 연구에서는 작업 계획의 다양성을 넓히고 검증 프로세스를 개선하기 위해 증강 방법을 정제하는 것을 목표로 합니다. 또한, 로봇 행동 유도성 (affordances)을 고려하고 실패 시 재계획을 지원하도록 GTTP를 향상시킬 계획입니다.

---

### 참고 자료

[원본 경로 #1](https://ojs.aaai.org/index.php/AAAI/article/view/33598)
