---
title: 'ReVLA, Reverting Visual Domain Limitation of Robotic Foundation Models'
date: '2025-12-03'
tags: ['embodied ai', 'paper review']
---

### Abstract

최근 대규모 언어 모델의 발전과 대규모 로봇 데이터셋에 대한 접근은 로봇 모델을 다양한 작업, 장면, 로봇 양식에 적응할 수 있는 일반론자로 변모시키는 로봇 공학의 패러다임 전환을 촉발시켰습니다. 본 연구에서 세 가지 기존 로봇 파운데이션 모델의 시각적 일반화 능력을 연구하고, 해당 평가 프레임워크를 제안합니다.

기존 모델이 시각적 분포 외 시나리오에 대한 강건성을 보이지 않는다는 것을 보여줍니다. 이것은 훈련 데이터의 제한된 변동 또는 파국적 망각으로 인해 비전 파운데이션 모델의 도메인 제한으로 이어질 수 있습니다. 더 나아가 두 개의 사전 훈련된 비전 파운데이션 모델을 사용하며 따라서 분포 외 실험에 일반화할 것으로 예상되는 OpenVLA를 탐구합니다. 그러나 OpenVLA에서 DINO-v2가 깊이 회귀라는 작업을 수행하는 데 실패하는 것을 통해 파국적 망각을 보여줍니다.

앞서 언급된 시각적 파국적 망각의 문제를 극복하기 위해 모델 병합에 기초한 점진적 백본 복원 접근 방식 (gradual backbone reversal approach)을 제안합니다. 이것은 초기 훈련 중에 시각적 백본의 적응을 필요로 하는 OpenVLA가 그것의 시각적 일반화 능력을 회복할 수 있도록 합니다. 이러한 능력을 회복함으로써 시각적 OOD 작업에서 잡기 및 들어 올리기에 대해 OpenVLA보다 각각 77%와 66% 개선됩니다.

---

### Introduction

비전 언어 모델을 기반으로 하는 일반주의 로봇 파운데이션 모델은 작업, 구현, 환경 전반에 걸쳐 일반화되는 민첩한 정책을 개발하는 유망한 경로를 제공합니다. 이러한 기대는 주로 Llama와 같은 기본 대규모 언어 모델의 역량에 기초하며 비전 인코더를 LLM에 장착하여 인식과 추론이 필요한 복잡한 작업을 수행할 수 있도록 합니다. 로봇의 행동 궤적을 출력하는 로봇 파운데이션 모델의 훈련은 대량의 로봇별 데이터에 대한 훈련을 필요로 합니다.

사전 훈련된 비전 모델 자체는 다양한 설정에 걸쳐 잘 일반화하는 것으로 알려져 있지만, 로봇 파운데이션 모델을 훈련하는 동안의 그것들의 적응은 파국적 망각으로 인해 시각적 도메인 제한으로 이어질 수 있습니다. 이러한 훈련 접근 방식의 영향을 정량화하고 로봇 파운데이션 모델의 전반적인 일반화 능력을 평가하기 위해 SIMPLER 환경을 기반으로 한 현실적인 분포 외 평가 프레임워크를 개발합니다. 이 평가 프레임워크는 OOD 객체와 시각적 교란 요소를 사용하는 도전적인 시각적 작업을 포함합니다. SIMPLER가 시뮬레이션된 로봇 공학 평가에서 두 가지 주요 도전 과제인 제어 및 시각적 격차를 해결하기 때문에 선택했습니다.

이 평가지표에서 시뮬레이션된 환경과 실제 환경 사이의 제어 격차를 줄이는 목표는 시뮬레이션에서 수행된 정책 행동이 실제 로봇에 적용될 때 로봇의 말단 효과기에서 유사한 결과를 생성하도록 보장하여 실제와 시뮬레이션 격차를 최소화하는 것입니다. 그리고 시뮬레이터와 실제 관찰 사이의 시각적 외관의 불일치는 도메인 이동을 야기하며 이것은 학습된 정책의 행동을 손상시킬 수 있습니다. 따라서 SIMPLER는 재현 가능하고, 저비용이며, 빠른 시뮬레이션에서 로봇 파운데이션 모델을 평가하고 실제 로봇 실험에 대한 모델 성능의 순위 상관 관계를 보존하는 것을 입증한 현실적인 설정입니다.

---

### Methods

OpenVLA에 사용된 비전 파운데이션 모델을 다양한 비전 언어 작업의 대규모 코퍼스에 사전 훈련된 Prismatic의 원래 상태로 되돌리는 것을 목표로 합니다.

#### Catastrophic Forgetting in OpenVLA

OpenX 데이터셋은 로봇 시연의 100만 개 이상의 에피소드를 포함하는 로봇 파운데이션 모델 훈련을 위한 중요한 기반이며 다양한 양식에 걸쳐 작동할 수 있는 일반주의 모델을 훈련하기 충분한 광범위한 로봇 구현, 작업, 궤적에 걸친 다양성을 포함합니다. 그러나 OpenX 데이터셋에 존재하는 변동은 시각적 및 장면 다양성보다는 주로 작업 복잡성과 구현에 초점을 맞추고 있습니다. 대조적으로 대규모 VLM은 주로 시각적 다양성을 강조하는 데이터셋에 대해 훈련되며 DINO-V2와 같은 비전 파운데이션 모델은 다양성과 일반화 가능성을 더욱 향상시키는 부분적으로 개인적인 데이터에 대해 훈련됩니다.

이러한 시각적 다양성의 부족은 로봇 파운데이션 모델을 훈련할 때 특정한 도전 과제를 제시합니다. RT-2는 WebLI 데이터셋에서 공동 훈련함으로써 이러한 도전 과제에 접근하지만 훈련 요구 사항을 상당히 증가시키며 이는 오픈 모델에 대한 제한 요소입니다. 모델의 망각 위험은 기본 Prismatic 아키텍처에서 취하는 접근 방식과는 대조적으로 최대 성능에 도달하기 위해 비전 인코더와 LLM 모두를 동시에 미세 조정해야 하기 때문에 OpenVLA에서 더욱 증폭됩니다.

시각적 인코더에서 파국적 망각을 입증하기 위해 선형 탐색과 조밀 예측 트랜스포머 헤드를 사용하여 깊이 추정을 위한 DINO-v2 특징을 활용합니다. 깊이 정보로 회귀하는 작업이 주어졌을 때 OpenVLA에서 훈련된 비전 인코더는 DPT에 대해 일정한 출력으로 붕괴하고 선형 탐색을 위해 세부 사항이 없는 깊이 맵을 생성하여 공간적 이해의 손실을 반영하는 반면 사전 훈련된 DINO-v2 특징은 특히 DPT로 고품질 깊이 맵을 생성합니다.

#### ReVLA: Reverting the Vision Backbones

비전 인코더의 파국적 망각이라는 도전 과제에 접근하면서 고품질 로봇 궤적을 보장하는 OpenVLA의 훈련 프로토콜을 유지하면서 사전 훈련된 비전 인코더와 그것들의 내재된 능력으로 복귀하기 위한 접근 방식으로 ReVLA를 제안합니다. 전체 OpenX 데이터셋에 대해 훈련된 OpenVLA의 가장 강력한 버전을 기반으로 작업하며 시뮬레이터와 동일한 구현을 사용하는 Fractal RT-1 로봇 행동 데이터셋을 사용하여 인코더 복원을 수행합니다.

이 아키텍처는 훈련 중에 시각적 인코더 복원이 수행됩니다. 복원 프로세스는 한 모델의 가중치가 두 번째로 다르게 훈련된 모델로 점진적으로 통합되는 연속적인 모델 병합 접근 방식에서 영감을 받았습니다.

$$
\mathcal{F}_{\theta} = (1 - \alpha) \theta_{\text{OpenVLA}} + \alpha \theta_{\text{Pretrained}}
$$

여기서 $\alpha$는 사전 훈련된 DINO 가중치의 혼합 값의 가중치입니다. 계산 효율성을 위해, 선형 전환은 $n$ 단계마다 $\frac{1}{k}$ 값으로 $\alpha$를 변경하는 단계별 커리큘럼으로 구현됩니다. 총 훈련 단계는 $N = k \times n$입니다. 본 연구에서, $N = \text{100K}$ 단계와 $n = \text{10k}$ 단계를 선택합니다.

---

### Conclusion

본 연구에서는 로봇 일반주의 파운데이션 모델의 시각적 도메인 제한을 평가하고 해결했으며 로봇 데이터셋에 미세 조정할 때 비전 인코더에서 발생하는 파국적 망각에 초점을 맞추었습니다. 전체 아키텍처의 훈련을 필요로 하는 모델을 위해 시각적 백본을 사전 훈련된 상태로 복원하는 새로운 접근 방식인 ReVLA를 제안했습니다. 이 복원 접근 방식은 OpenVLA에 존재하는 파국적 망각을 성공적으로 완화하여 시각적 일반화 능력을 복원했습니다.

Fractal 데이터셋에서 수행되었으며, 이것은 특히 구현 전반에 걸쳐 로봇 공학의 다양한 실제 시나리오를 완전히 대표하지 못할 수 있습니다. 또한, 모델 병합 기술의 탐구는 로봇 데이터셋으로부터의 추가 데이터를 통합함으로써 훨씬 더 큰 성능 향상을 가져올 잠재력을 가지고 있습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2409.15250)
