---
title: 'RLAD: Time Series Anomaly Detection through Reinforcement Learning and Active Learning'
date: '2022-09-19'
tags: ['time-series', 'paper review']
---

### Abstract

심층 강화 학습(DRL)과 능동 학습(active learning)을 활용하여 실제 시계열 데이터의 이상 징후를 효율적으로 학습하고 적응하는 새로운 반지도(semi-supervised) 시계열 이상 탐지 알고리즘을 소개합니다. 제안된 모델인 RLAD는 관측 시퀀스를 생성하는 기본 메커니즘에 대해 어떠한 가정도 하지 않으며, 비정상적인 패턴에 대한 경험을 바탕으로 탐지 모델을 지속적으로 적응시킵니다.

또한, RLAD는 매개변수를 수동으로 조정할 필요가 없으며, 비교한 모든 최신 방법들(비지도 및 반지도를 포함하여)을 여러 성능 지표에서 능가합니다. 더 구체적으로, 단 1%의 레이블만으로 F1 점수에서 최고의 비지도 접근 방식보다 1.58배 우수하며, 다른 실제 데이터셋에서는 단 0.1%의 레이블만으로 최대 약 4.4배까지 우수한 성능을 보였습니다.

---

### Introduction

대부분의 이상 탐지 알고리즘은 응용 분야별 데이터 속성과 밀접하게 결합되어 있습니다. 모델은 특정 응용 분야를 위해 설계되며, 강력한 사전 지식이나 광범위한 실험을 기반으로 임계값, 특징 추출 처리 및 하이퍼파라미터가 설정됩니다. 이러한 속성에는 다변량 시계열 설정에서의 다중 스케일 시간 종속성, 주파수 도메인의 이상 현상 및 기타 관련 통계 모멘트가 포함됩니다.

최근 딥러닝 기술이 문헌에 소개되었습니다. 예를 들어, SR-CNN은 시계열 데이터를 시각적 맥락으로 변환하고 공간 상관관계를 사용하여 이상 현상을 식별합니다. 최근에는 정보 이론적 기준을 사용하여 이상 현상을 찾는 새로운 딥러닝 기반의 반지지도 기법인 Deep-SAD가 등장했습니다.

또한, 이상 탐지 알고리즘은 두 가지 주요 과제를 극복해야 합니다. 첫째, 이상 현상이 드물어 모델 훈련이 어렵다는 점과 둘째, 많은 실제 응용 분야에서 비정상적인 특성을 가지며, 즉 측정값을 생성하는 기본 메커니즘이 시간이 지남에 따라 변한다는 점입니다. 레이블이 부족한 문제를 해결하기 위해 비지도 접근 방식을 취하는 많은 제안들이 문헌에 있었습니다. 그러나 비지도 알고리즘은 실제로는 성능이 매우 저조한 경향이 있으며, 반지지도 및 완전 지도 방법에 의해 크게 능가됩니다. 지도 모델은 레이블을 사용할 수 있을 때 매우 잘 작동하지만, 레이블이 거의 없거나 전혀 없을 때는 잘 작동하지 않습니다. 레이블을 쉽게 사용할 수 있더라도, 기본적으로 기본 분포가 정상 상태라고 가정합니다. 분포가 변경되면 다시 훈련해야 합니다.

이러한 과제를 해결하기 위해 우리는 지도 모델을 훈련하는 데 필요한 레이블의 일부만을 사용하여 효율적으로 학습할 수 있는 알고리즘을 설계해야 합니다. 두 번째 과제를 해결하기 위해서는 적응형 설계를 고려해야 합니다. 즉, 데이터를 더 많이 관찰하고 그 안에 있는 이상 현상의 분포에 대해 더 많이 학습함에 따라 변화에 적응하는 알고리즘이 필요합니다. 우리는 강화 학습과 능동 학습을 결합하여 새로운 알고리즘인 RLAD를 소개합니다. 이상 탐지를 위해 심층 강화 학습과 능동 학습을 결합한 최초의 사례입니다. 이상 탐지를 위해 강화 학습을 사용하는 기술이 있지만, 상당한 수의 레이블이 지정된 샘플 없이는 훈련하기 어렵습니다. 또한, 능동 학습을 사용하는 기술도 있으며, 레이블의 일부만으로 반지지도 모델에 비해 경쟁력 있는 성능을 보여줍니다.

---

### Related work

#### 1. Traditional approach

대부분의 전통적인 접근 방식은 일반적으로 K-최근접 이웃(KNN), 국부 이상치 계수(LOF), 국부 상관 적분(LOCI), Isolation Forest(iForest)와 같은 거리 및 밀도 기반의 단순한 머신러닝 알고리즘입니다. One-Class SVM(OCSVM), 주성분 분석(PCA) 등도 이 분야에서 흔히 사용됩니다. 이러한 접근 방식은 일반적으로 시간 효율적이지만, 실제 적용에서 높은 성능을 달성하지 못하는 경향이 있습니다.

#### 2. Supervised approach

최근 딥러닝은 이상 탐지 분야에서 많은 성공을 거두었습니다. 대부분의 딥러닝 기반 접근 방식은 일반적으로 데이터에서 특징을 추출하고 분류 모델을 구축하여 이상 현상을 식별합니다. 지도 딥 이상 탐지는 정상 및 이상 데이터 인스턴스의 레이블을 사용하여 심층 지도 이진 또는 다중 클래스 분류기를 훈련하는 것을 포함합니다.

#### 3. Unsupervised approach

고급 비지도 학습 기술이 이 분야에서 더욱 인기를 얻고 있습니다. 이들은 레이블을 필요로 하지 않으며, 비정상적인 지점이 정상 분포에서 더 큰 편차를 보인다고 가정합니다. 그러나 이러한 방법들이 달성하는 성능은 상당히 낮습니다. 많은 실제 이상 탐지 응용 분야에서 또는 인간 전문가의 가용성이 있을 때와 같이 사전 지식(예: 소수의 레이블이 지정된 이상 현상)을 활용하기 어렵기 때문입니다. 또한, 이들은 정상 패턴이 정상 상태라는 가정을 요구하며, 입력 분포의 변화에 적응할 수 없습니다.

#### 4. Semi-supervised approach

반지도 접근 방식은 훈련을 위해 레이블이 지정된 데이터의 일부를 사용합니다. 몇 개의 레이블이 지정된 이상 현상을 활용하여 더 관련성 있는 특징을 학습하는 REPEN을 소개했습니다. Deep-SAD는 이상 탐지에 대한 정보 이론적 관점과 사전 훈련을 위한 오토인코더 모델을 사용했습니다. 그러나 이러한 방법들은 훈련 데이터에 있는 레이블이 지정된 이상 현상의 수에 크게 의존합니다. 하지만 이상 탐지 문제에서 이상 현상은 드뭅니다. 알 수 없는 샘플 집합이 주어졌을 때, 필요한 양의 이상 현상을 필터링하기 위해 많은 양의 알 수 없는 샘플을 검사해야 합니다. 이는 레이블을 더 비싸게 만듭니다.

#### 5. Reinforcement learning based approach

강화 학습은 자기 개선 특성 때문에 명확한 정상 패턴이 없는 시퀀스 문제를 해결할 수 있습니다. 하지만 여러 연구에서 심층 강화 학습 또는 역강화 학습 기법을 적용했지만 성능을 만족시키지 못했으며 완전히 레이블이 지정된 데이터가 필요하다는 한계가 보였습니다.

---

### Preliminaries

#### 1. Reinforcement learning

우리는 이상 탐지 문제를 튜플 $< S, A, P_a, R_a, \gamma >$로 표현할 수 있는 마르코프 결정 과정(MDP)으로 간주합니다. $S$는 환경 상태의 집합을 나타냅니다. $A$는 RL 에이전트가 취하는 행동의 집합입니다. $P_a(s, s')$는 상태 $s$에서 행동 $a$가 수행되어 상태 $s'$로 이어질 확률을 의미합니다. $R_a(s, s')$는 행동 $a$를 취함으로써 에이전트가 받는 즉각적인 보상입니다. $\gamma \in [0, 1]$는 할인 인자입니다.

가치 함수(Value Function)는 $V^\pi(s) = E[\sum_{t=0}^\infty \gamma^t R_t | s_0 = s]$로 정의되며, 상태 $s$에서 시작하는 예상 보상 합계를 나타냅니다. MDP에서 에이전트는 누적 미래 보상을 최대화하는 제어 정책을 학습하려고 합니다.

#### 2. Deep Q network

Q 학습(Q learning)은 유명한 가치 기반 알고리즘입니다. 에이전트는 행동-가치 함수 $Q(s, a)$를 학습하고 특정 상태에서 행동을 취하는 것이 얼마나 좋은지 예측합니다. 대상 값(Target Value)은 다음과 같이 정의됩니다.

$$
\text {target}=R(s,a,s')+\gamma max_{a'} Q_k(s',a')
$$

그리고 Q 함수는 다음과 같이 업데이트됩니다.

$$
Q_{k+1}(s,a) \leftarrow (1−α)Q_k(s,a)+\alpha*\text{target}
$$

불행히도 Q 학습은 행동-가치 함수가 신경망과 같은 비선형 함수로 근사될 때 불안정하거나 심지어 발산하기도 합니다. 따라서 Deepmind에서 강화 학습과 심층 신경망을 결합한 Deep Q 네트워크(DQN)라는 방법이 제안되었으며, 더 복잡한 문제를 처리할 수 있음이 입증되었습니다. DQN의 핵심 요소는 경험 재생(experience replay)과 대상 네트워크(target network)입니다.

경험 재생은 튜플 $< s, a, r, s' >$의 전이(transition)를 저장합니다. 여기서 $s$는 상태, $a$는 행동, $r$은 행동 $a$를 취함으로써 얻는 보상, $s'$는 행동 후 다음 상태입니다. 에이전트는 각 반복에서 미니 배치를 무작위로 샘플링하여 훈련됩니다. 이는 샘플 간의 상관관계를 줄이고 데이터 효율성을 향상시킵니다.

대상 네트워크는 비정상적인 대상 값 문제를 처리하기 위해 설계되었습니다. 우리는 Q 값을 학습하기 위해 신경망을 구축하지만, 대상 값이 매우 불안정하여 훈련 수렴을 어렵게 만듭니다. 따라서 훈련 과정을 가속화하기 위해 대상 값을 임시로 고정하도록 정확히 동일한 구조를 가진 또 다른 신경망인 대상 네트워크가 설계되었습니다.

#### 3. Active learning

능동 학습(Active learning)은 모델이 사용자(human expert)와 적극적으로 상호작용하여 원하는 학습 경험을 얻을 수 있도록 하는 특정 머신러닝 알고리즘입니다.

훈련 세트 $L = (X, Y)$와 레이블이 지정되지 않은 인스턴스 풀 $U = (x_1, x_2, \dots, x_n)$이 있다고 가정해 봅시다. 레이블이 지정되지 않은 데이터는 일반적으로 레이블이 지정된 데이터보다 저렴합니다. 따라서 U는 매우 클 수 있습니다. 능동 학습은 U에서 레이블이 지정되지 않은 샘플을 선택하고 쿼리 함수 Q를 통해 인간 전문가에게 수동으로 레이블을 지정하도록 요청합니다. 일반적으로 선택된 샘플은 현재 모델 C에 더 가치 있는 정보를 가지고 있습니다. 학습자는 새로 레이블이 지정된 인스턴스에서 가장 많은 훈련 이점을 얻을 것입니다. 새로 레이블이 지정된 인스턴스 집합 $L_{new} = (X_{new}, Y_{new})$는 훈련 세트 P에 추가되어 다음 반복에서 훈련에 사용됩니다. 능동 학습의 목표는 합리적인 수의 쿼리로 좋은 분류기 모델 C를 달성하는 것입니다.

---

### Method

#### 1. Preprossesing and Warm up

주어진 시계열 시퀀스 $TS = [t_1, t_2, \dots, t_n]$를 크기 ω의 슬라이딩 윈도우를 통해 다음과 같은 상태 집합으로 분리합니다.

$$
S=[(t_1, ..., t_w), (t_2, ..., t_{w+1}), ..., (t_{n-w}, ..., t_{n})]
$$

각 상태(샘플)는 크기 $ω$를 가지며, 상태 $S_1$의 레이블은 마지막 시점 $t_ω$에 의해 결정됩니다. 여기서 0은 정상, 1은 이상을 나타내며, -1은 레이블이 지정되지 않음을 나타냅니다. 모든 값은 MaxMinScaler에 의해 $[0,1]$ 범위로 정규화됩니다.

재생 메모리(Replay memory)는 심층 강화 학습의 핵심 구성 요소입니다. 이는 RL 에이전트가 일부 기억과 경험을 저장하는 데 도움을 주어 학습 및 훈련 속도를 높입니다. 일반적으로 재생 메모리는 현재 전이(상태, 행동 및 관련 보상 포함)를 N의 용량으로 저장합니다. RL 에이전트는 손실 최소화를 위해 재생 메모리에서 미니 배치를 무작위로 선택하며, 이는 샘플 간의 상관관계를 줄입니다. 그러나 일반적인 재생 메모리는 정규 분포를 가지며 데이터 다양성이 부족합니다. 우리는 sklearn을 기반으로 Isolation Forest 분류 모델을 구축하여 잠정적인 이상치 탐지를 수행합니다. iForest는 가장 대표적인 이상 샘플과 정상 샘플을 선택하는 데 도움을 줍니다. 우리는 이 샘플들을 RL 에이전트에 보내고 이 샘플들의 전이(transition)로 재생 메모리를 채우도록 강제합니다.

#### 2. Deep reinforcement learning

이상 탐지 문제의 경우, 인스턴스를 상태(state)로 간주합니다. 행동(action)은 RL 에이전트가 이상 여부를 예측하는 것입니다. $a=1$은 이상 예측을 나타내고 $a=0$은 정상 예측을 나타냅니다. 보상 함수(Reward function)는 진양성(True Positives), 진음성(True Negatives), 위음성(False Negatives), 위양성(False Positives)에 대해 $(r_1, r_2, -r_1, -r_2)$로 설정됩니다.

일반적으로 정책 기반 강화 학습 알고리즘은 연속적인 행동 공간을 가진 문제를 처리하는 데 사용됩니다. 우리의 행동은 이산 이진 값 $\in [0, 1]$이기 때문에, 우리는 값 기반 알고리즘인 DQN을 강화 학습 방법으로 선택합니다. 우리는 Long Short Term Memory(LSTM) 신경망인 $Eval_N$을 RL 에이전트의 "두뇌"로 채택하여 상태 $S$를 받으면 Q-값을 생성하며, 이는 Q 함수를 따릅니다. $Eval_N$과 정확히 동일한 아키텍처를 가진 또 다른 신경망 $Target_N$을 구축합니다. $Target_N$은 $Eval_N$보다 업데이트 속도가 낮으며, $Target_N$은 항상 매개변수를 고정한다고 볼 수 있습니다.

탐색-활용 균형(exploration and exploitation trade-off)을 위해 엡실론 감소(epsilon decay) 전략이 채택됩니다. 탐욕 계수 $\epsilon$는 RL 에이전트가 Q 함수를 기반으로 행동을 취해야 하는지 또는 전체 환경 공간을 탐색하기 위해 무작위로 행동을 취해야 하는지를 결정하는 데 사용됩니다. 엡실론 감소 정책은 시간이 지남에 따라 탐색에 할당된 비율을 줄이려고 합니다. 이는 최적의 회귀를 제공할 수 있습니다.

워밍업 모듈의 끝에서 초기 강화 학습 모델 $\Theta_{rl}$이 훈련됩니다.

#### 3. Active learning

완전히 레이블이 지정된 데이터가 실제 세계에서 대부분 비용이 많이 들기 때문에, 본 연구에서는 시스템에 능동 학습 구성 요소를 도입했습니다. 이는 RL 에이전트가 환경을 탐색하고 경험을 학습할 뿐만 아니라, 탐색 중 경험을 기반으로 질문/쿼리를 던질 수 있는 능력을 부여합니다. 마진 샘플링(margin sampling)을 능동 학습 전략으로 선택합니다.  마진이 작을수록 모델이 샘플이 이상인지 비이상인지 식별하는 데 더 불확실하다는 것을 의미합니다. 각 에피소드에서 능동 학습은 심층 강화 학습으로부터 레이블이 지정되지 않은 인스턴스 집합 $S_{unlabeled}$를 받습니다. 각 에포크에서 상태 $s$에 대해 RL 에이전트는 두 가지 행동 옵션을 가집니다. $a_0$, $a_1$ 이들의 $q$ 값과 최소 마진은 다음과 같이 공식화될 수 있습니다.

---

### Conclusion

본 논문에서는 시계열 이상 탐지를 위한 RLAD를 소개했습니다. 이는 심층 강화 학습과 능동 학습을 결합하여 정상 패턴 가정에 대한 의존성과 레이블 가용성 문제를 줄이려는 이상 탐지 분야의 첫 시도입니다. 실험 결과, RLAD는 실제 및 합성 데이터셋 모두에서 극히 적은 수의 샘플에만 레이블을 지정하고도 뛰어난 성능을 달성했습니다.

---

### 참고 자료

[원본 경로 #1](https://arxiv.org/pdf/2104.00543)
