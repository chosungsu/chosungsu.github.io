---
title: 'Transformer'
date: '2022-08-23'
tags: ['Deep Learning', 'lecture']
---

### Attention method

Since LSTM/GRU still did not completely solve long sequences, the attention method emerged. Although it was possible to set up simultaneous input and output through encoder-decoder structure, there were limitations in memory capacity.

Therefore, it was thought that something was needed to remember which words were input in the middle and passed through hidden states, like human cognitive abilities.

The attention method has values called $(Q, K, V)$ and uses query as context and key-value pairs. And the attention value is a weighted average of values. In seq2seq models, $Q$ is the hidden state at time $t$ in the decoder, $K$ is the hidden state at all times in the encoder, and $V$ is the hidden state at all times in the encoder.

<img src="https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

That is, the initial hidden state $s_0$ in the decoder is the beginning of the query. And the inner product with each hidden state $h_1, \cdots, h_n$ in the encoder $([s_t^Th_1, \dots, s_t^Th_T])$ is called attention score $e_t$, and softmax is applied to make coefficients $\alpha_t$. Then the same hidden state will be used as value, and this is multiplied by $\alpha_t$ to create a weighted sum, generating attention value $a_t$ as one vector. This value is combined with query and projected through fc-layer and tanh to adjust the size again and convert to $\hat{s}_0$.

---

### Transformer

Assuming that input $x$ can be split into multiple elements and they are organically related to each other, the concept of self-attention was created to help understand context through individual representation.

In the above attention method, $(Q, K, V)$ was predetermined, but in transformer, $(Q, K, V)$ is created by linear combination of direct input values $x_1, \dots, x_n$ and learned parameters, linear weights $W_q, W_k, W_v$. And another learnable parameter $W_o$ is mapped with attention value $\sum V_i$ to return to input values.

For example, if there are inputs $x_1, x_2, x_3$, $Q_1$ is created by linear combination of $W_q$ from $x_1$, and then $K_1, K_2, K_3$ are generated by linear combination of $W_k$ from each input value. Then query and key are inner producted and softmax is applied to obtain probabilities so that the total sum becomes 1. Next, $W_v$ is linearly combined to produce $V_1, V_2, V_3$, and if this is multiplied by the total sum of each probability and $W_o$, $Z_1$ of the same size as the original can be created. This is said to be a transformed state where the size is the same but the context is transformed.

#### 1. Token aggregation

After projecting input $x$ through transformer blocks to create $z_1, \dots, z_n$, the simplest integration is to take the average. The $z$ obtained by taking the average can be used to select classifier $f$ and regressor $g$ to predict results. However, this results in losing all individual meanings obtained through transformer.

<img src="https://miro.medium.com/1*GUd19qrm7YxnhE0ZQJybVw.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

Since it must depend on the attention mechanism, a classification token $[CLS]$ was designed to be added as a dummy token to the input for use in integrated embedding. If the existing input was 5, then 6 inputs including this dummy token are projected into transformer. Then a total of 6 $z$ sets are created. Among them, $z_0$ where $[CLS]$ is main will be calculated by relatively evenly combining other elements. Therefore, this is a method of performing sequence level prediction by placing a classifier on top of this. And if a classifier is placed on other tokens, token level prediction can be performed.

#### 2. Architecture

<img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

I will explain by dividing it into a total of 8 steps.

$\rightarrow$ step 1: Input embedding is a sequence of tokens, and each token is a vector of the same size.

$\rightarrow$ step 2: Unlike RNN, there is no concept of order, so positional encoding is used so that the same word has different expressive power even when placed in different positions. Through sinusoidal encoding, the periodicity of sin and cos functions is utilized to handle long sequences in evaluation, and generalization is possible even for sequences of lengths not learned. $PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}), PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$

$\rightarrow$ step 3: There is a multi-head attention block inside the encoder transblock, which plays the role of embedding. Each input word is used as query and all words are used as key to calculate similarity, then all words are used as value to calculate weighted sum. Finally, through softmax, it is calculated to be the same size as $\text{softmax}(\frac{Q \times K^T}{\sqrt{d_k}}) \times V = Z$.

$\rightarrow$ step 4: After embedding is completed inside the encoder transblock, there is feed forward, which adds fc-layer to contextualized embedding. $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$ allows tokens to proceed separately without affecting each other, having dependency only on their own tokens. Residual connection and layer normalization are added on top of step 2 and step 3.

$\rightarrow$ step 5: In decoder input, output sequences are generated autoregressively through the $z$ set given by the encoder.

$\rightarrow$ step 6: There is masked multi-head attention in the decoder transblock, which receives $t$ inputs including $<sos>$ token at time $t$ and masks from the next time point with $[MASK]$, a dummy token with no information.

$\rightarrow$ step 7: Only query enters from the decoder side, and key and value are received from the encoder side to proceed with multi-head attention. Therefore, the current word received as input is used as query, and the entire sentence that grasped context from the encoder is used as key and value. At this time, time $t$ and later are not masked.

$\rightarrow$ step 8: Finally, a classifier is built with softmax and scores are created with one-hot encoding.

When all these processes are completed, $<EOS>$ is marked as the final token and prediction is finished.

#### 3. BERT

<img src="https://resources-public-blog.modulabs.co.kr/blog/prd/content/263018/BERT.png" alt="Example Image" style="display: block; margin: 0 auto; height:150;" />

There are two input sentences, and $[CLS]$ is inserted at the beginning and $[SEP]$ is inserted at the section that distinguishes each sentence and at the end. And segment embedding and position embedding are applied.

As a method of learning, first there is Masked Language Modeling (MLM). About 15% of all tokens are randomly masked, and the resulting embedding at this position is classified. This is a task of grasping the context of organic words within one sentence. Next is Next Sentence Prediction (NSP). This learns by attaching two sentences where 50% of the training data has continuous flow or the rest do not, and the result is said to be very good at about 98%. Therefore, recently, pre-trained BERT has become a basic option.

---

### References

[Original source #1](https://youtu.be/o3UEbQ24zhQ?si=NF1hNkEgzfmxUFx9)

[Original source #2](https://youtu.be/0csOwWpc4Ik?si=SqC_e9uMkIHCXYz-)