---
title: 'Dimensions and subspaces'
date: '2023-03-20'
tags: ['Linear algebra', 'lecture']
---

### Properties of basis and dimensions

A subset $S = \{v_1, v_2, … , v_s\}$ of $R^n$ becomes a basis if it satisfies the following two conditions:

-$S$ is linearly independent

-$span(S) = R^n$

To show the linear independence of vectors on $R^n$, when $A = [x_1 : x_2 :  … : x_m]$ is a matrix with $x_i$ as column vectors and $c = [c_1, …, c_m]^T$ are constants, the homogeneous system of equations $Ac = 0$ must have a unique solution $c = 0$. In particular, when $m = n$, if $detA \ne 0$, it has a unique solution.

When a set $S$ is a basis of $R^n$, a set $T$ of $r(>n)$ vectors is always linearly dependent, which shows that if $T$ is linearly independent, then $r(≤n)$ always holds. And if both sets $S$ and $T$ are bases, then $m = n$.

When a set $S$ is a basis of $R^n$, the number of vectors belonging to $S$ is called the dimension and is denoted by $dimR^n$.

---

### Basic space and dimension theorem

The eigenspace $\{x \in R^n | Ax=\lambda x\}$ for eigenvalue $\lambda$ of an $n*n$ matrix $A$ is a subspace of $R^n$. Also, the solution set of the homogeneous system of equations $Ax=0$ is a subspace. This is called the solution space or null space and is denoted by $Null(A)$.

In the basis and dimension of the solution space, using Gauss-Jordan elimination, let matrix $[B:0]$ be the RREF of the augmented matrix $[A:0]$ of the linear system of equations, and matrix $B$ has $r(1 ≤ r ≤ n)$ non-zero solutions from the first row. At this time, if $r = n$, then $Ax=0$ has only the solution $x=0$, making the dimension of the solution space 0. If $r < n$, then by changing the positions of variables if necessary to maintain generality, there are $n-r$ free variables (dimensions). For an $m*m$ matrix $A$, the dimension of the solution space (null space) of $Ax=0$ can be denoted as $nullity(A)$. That is, $dim(Null(A)) = nullity(A)$.

For example, when $A = \begin{bmatrix} 1 & 1 & 0 & 2 \\ -2 & -2 & 1 & -5 \\ 1 & 1 & -1 & 3 \\ 4 & 4 & -1 & 9 \end{bmatrix}$ is transformed to RREF, it becomes $\begin{bmatrix} 1 & 1 & 0 & 2 & : 0 \\ 0 & 0 & 1 & -1 & : 0 \\ 0 & 0 & 0 & 0 & : 0 \\ 0 & 0 & 0 & 0 & : 0 \end{bmatrix}$, so the general solution $x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} -x_2-2x_4 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} -s -2t \\ s \\ t \\ t \end{bmatrix} = s\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \end{bmatrix} + t\begin{bmatrix} -2 \\ 0 \\ 1 \\ 1 \end{bmatrix}$ has a basis equal to the values multiplied by $s$ and $t$, and the dimension is 2.

For any matrix $A$, the row rank and column rank of $A$ are equal ($rank(A) = r(A) = c(A)$). According to the dimension theorem, $dim(lmT) = rank(A), dim(kerT) = nullity(A)$ also holds.

---

### Rank theorem

By the fundamental theorem of dimension, for any $m*n$ matrix $A$, dim Row $(A)$ = dim Col $(A)$. Since dim Row $(A) ≤ m$ and dim Col $(A) ≤ n$, and $rankA$ equals both dimensions, we can see that $rankA ≤ min\{m, n\}$ holds.

And by the rank theorem, $rankA + nullityA$ = number of columns of A ($n$), $rankA + nullityA^T$ = number of rows of A ($m$) also holds.

A necessary and sufficient condition for a square matrix $A$ of size $n$ to be invertible is $rankA = n$. The reason is that the meaning of an invertible matrix is that $Ax=0$ has only the trivial solution, so $nullityA = 0$, and by the dimension theorem, it must be $n$.

While $rank(AB) ≤ min\{rankA, rankB\}$ holds, when an invertible matrix $A$ is multiplied by any matrix $B$, since an invertible matrix does not decrease rank, $rank(AB) = rankB = rankBA$ also holds.

---

### Orthogonal projection theorem

<img src="https://velog.velcdn.com/images/devjo/post/83dad265-0bbb-440f-8451-4e28dec06661/image.png" alt="Example Image" style="display: block; margin: 0 auto; height:200;" />

The orthogonal projection onto a one-dimensional subspace of $R^n$ is uniquely expressed for all x with respect to a non-zero vector a as follows:

$x = proj_{<a>}x + w = ta + w = p + w$

Here, $p$ is $ta=\frac{x \cdot a}{|a|^2}a$. The above equation is called the orthogonal projection of vector x onto the subspace generated by a (orthogonal proj of $R^n$).

If we let $a$ be a column vector, then $T(x) = proj_{<a>}x = Px$ and $P=\frac{1}{a^Ta}aa^T$, and the symmetric matrix $P$ has $rank=1$.

The orthogonal projection onto a general subspace is uniquely expressed as $x = x_1 + x_2$ ($x_1 = proj_{w}x \in W, x_2 = x-x1 \in W^{\bot}$). Here, $w^{\bot}$ is the set of vectors perpendicular to $W$.

Let $W$ be a subspace of $R^n$ and when the column vectors of $M$ form a linearly independent basis of $W$, for all $x$, $proj_{w}x = M(M^TM)^{-1}M^Tx$ holds.

For example, to find the standard matrix of the orthogonal projection onto the plane $x - 4y + 2z = 0$, first the general solution is $\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 4t_1 - 2t_2 \\ t_1 \\ t_2 \end{bmatrix} = t_1\begin{bmatrix} 4 \\ 1 \\ 0 \end{bmatrix} + t_2\begin{bmatrix} -2 \\ 0 \\ 1 \end{bmatrix}$, so $M=\begin{bmatrix} 4 & -2 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$ and $P$ can be calculated according to the above formula.

---

### Gram-Schmidt process

Among the subspaces of $R^n$, $\{0\}$ and $R^n$ are called trivial subspaces. Every subspace of $R^n$ has an orthonormal basis.

For vectors $x_1, x_2, …, x_n$ in a set $S \in R^n$, if any two vectors are orthogonal, it is called an orthogonal set, and in particular, if all vectors belonging to an orthogonal set $S$ have magnitude 1, it is called an orthonormal set. An orthogonal set S is linearly independent. The reason is that when any $c_1, c_2,…,c_k \in R$ are linearly combined with $x$ to give 0, and when $x_i$ is multiplied on both sides, the left side becomes $x^2$ form, and since $x \ne 0$, eventually any $c$ must be 0.

The Gram-Schmidt process proceeds as follows:

-$y_1 = x_1$

-Let the subspace generated by $y_1$ be $w_1$, then $y_2 = x_2 - proj_{w_1}x_2 = x_2 - \frac{x_2 \cdot y_1}{|y_1|^2}y_1$

-Repeat this process up to $n$

The set $T=\{y_1, y_2, …, y_n\}$ obtained from the above steps is an orthogonal set where all vectors are perpendicular to each other.

---

### Coordinate vector

If a set $\alpha = \{x_1, x_2, …, x_n\}$ is a basis of $R^n$, then every vector $x = c_1x_1 + c_2x_2 + … + c_nx_n$ belonging to $R$ is uniquely expressed. At this time, the scalars $c$ are called the ordered basis, and the coordinates of $x$. And the vectorization of the ordered basis is called the coordinate vector ($[X]_\alpha$).

For example, when $x_1$ = (1,1,0), $x_2$ = (1,1,1), $x_3$ = (0, 1, -1), to find the coordinate vector with respect to the basis $\alpha = \{x_1, x_2, x_3\}$ of $R^3$, since $x = (1, 2, 3) = c_1x_1 + c_2x_2 + c_3x_3$, we have $\begin{cases} c_1 + c_2 = 1 \\ c_1 + c_2 + c_3 = 2 \\ c_2 - c_3 = 3 \end{cases}$, and by calculation, we can find that $[x]_n = \begin{bmatrix} -3 \\ 4 \\ 1 \end{bmatrix}$.

---

### References

[Original source #1](http://matrix.skku.ac.kr/2015-Album/BigBook-LinearAlgebra-2015.pdf)