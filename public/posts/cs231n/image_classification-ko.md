---
title: 'Image classification'
date: '2024-01-26'
tags: ['cs231n', 'lecture']
---

### semantic gap

컴퓨터는 이미지를 받아들일 때 숫자로 된 다량의 그리드를 형성하게 되고 RGB에 따른 3가지의 채널을 갖게 됨에 대한 문제점이 생기게 됩니다.

-뷰포인트

-광원 및 조명

-모습의 변형

-배경에 뭍히거나 가려지는 현상

-이외에도 배경에 일부가 잘린 현상, 같은 인풋 종류 중에서도 다른 특징(색상, 크기, 질감 등등) 등이 실제 겪을 수 있는 문제점입니다.

---

### data-driven approach

먼저 데이터셋을 만들 필요가 있다는 접근법입니다. 이미지와 라벨을 모두 포함하는 광범위한 데이터셋을 통해 학습하고 예측하는 일련의 과정을 일컫습니다.

---

### nearest Neighbor & knn

맨하튼 거리기법(L1)을 통해 test image와 train image간의 픽셀 값 차이를 계산하여 더합니다. 이를 사용할 때 n번의 샘플에 소모되는 계산복잡도는 train에서는 고려할 필요없이 O(1)이지만 예측 시에는 반복을 하기 때문에 O(n)만큼 소요됩니다. 따라서 이 알고리즘은 train에서는 빠르지만 예측에서 느리기 때문에 좋지 않습니다.

다른 방법으로는 유클리디안 거리기법(L2)을 통해 test image와 train image간의 픽셀 값 차이 합의 제곱근이 있습니다.

두 기법을 사용한 결과의 바운더리를 살펴보면 L1은 coordinate axis를 따릅니다. L2는 그렇지 않으며 스무스한 경계를 나타냅니다. 또한 L1은 coordinate 한 성격을 갖고 있어 이미지 픽셀의 의미를 알고 있는 경우에 사용하기 적합하다고 할 수 있습니다.

다만 KNN알고리즘은 test 시에 매우 느린 연산속도와 차원의 저주에 의하여 인풋이미지에 여러 추가처리를 가하더라도 L1, L2거리기법을 통한 픽셀 벡터 분석은 동일한 결과를 도출하고 이는 적합하지 않다는 단점으로 이미지데이터 분석에 사용되지 않습니다.

---

### linear

$$
f(x,W)=Wx + b
$$

4사분면의 대각선에 위치한 구역에 존재할 경우, 1<=L2 norm<=2와 같이 원형일 경우, 세개의 구역일 경우는 한개의 선형 hyperplane으로 구분이 어렵다.

---

### hyperparameter

앞서 살펴본 알고리즘 KNN의 예를 들면 K값이나 거리기법을 설정하는 것이 하이퍼파라미터라고 볼 수 있는데 이는 학습에 영향을 주며 학습을 통해 최적의 값을 얻을 수 없고 인간이 설정해야 합니다. 이런 이유로 항상 최적의 하이퍼파라미터를 찾으려는 노력이 필요하며 KNN에서 K를 1로 설정하면 모든 학습 데이터에 온전히 작동하지만 궁극적인 학습 목적이 우리가 모르는 데이터를 예측하고자 함이므로 해서는 안되는 경우라는 것을 명심해야 합니다. 

다른 경우를 보면 일반적으로 train/test split을 통해 우리가 알고있는 선의 데이터를 나누어 최적의 경우를 찾기도 하는데 이 역시도 모르는 데이터를 예측하기에는 좋은 선택이 아님을 명심해야 합니다. 그렇다면 과연 최적의 split방법은 무엇인가라는 질문에 대해 바로 train/val/test split이라고 답을 합니다. 

이 때의 학습 경로는 train을 통해 학습시킨 이후 val 단계에서 하이퍼파라미터를 찾아 test에 적용하는 순으로 진행합니다. 또 좋은 방법으로는 Cross validation기법이 있습니다. 이는 train을 train/val로 단순 split 하기보다 여러개의 fold로 나누고 val로 가장 적합한 fold를 찾는 방식입니다.

---

### 참고 자료

[원본 경로 #1](https://youtu.be/OoUX-nOEjG0?si=HRvH-K6F4qufcvU1)



