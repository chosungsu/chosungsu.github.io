---
title: 'Text numeral normalization'
date: '2025-02-07'
description: 'Develop an LLM-based NLP model that transforms Korean-written numerals (e.g., “삼십이”) into standard numeric forms (e.g., “32”)'
tags: ['seq2seq', 't5', 'llm']
github: 'https://github.com/username/project'
---

### Project Overview  
This project aimed to develop an LLM-based NLP model that transforms Korean-written numerals (e.g., “삼십이”) into standard numeric forms (e.g., “32”).  
The goal was to enhance information usability by normalizing numeric expressions within scripts generated by AI Humans, enabling downstream tasks like dialogue understanding and structured data extraction.

---

### Problem Definition  
Korean numerals are often expressed in diverse and irregular ways in text (e.g., “스무개”, “한두 번”), making it difficult to normalize them using rule-based methods.  
The core idea was to utilize __contextual understanding__ powered by LLMs, since the same expression could mean different numbers depending on context.

---

### Approach & Technical Stack  

#### 🧠 Model Design and Application  
- Applied a __Seq2Seq model__ based on __pko-t5-large__, optimized for sentence understanding and transformation  
- Transformed full sentences to generate the appropriate normalized numeric form using context-aware generation

#### 🔧 Data Preprocessing and Learning Strategy  
- Collected diverse Korean numeric expressions and __standardized irregular patterns__ to improve training quality  
- Applied a __custom loss function__ to enhance attention to numeral positions and improve contextual understanding  
- Used GPT to __augment the dataset__ with diverse sentence structures, improving generalization performance

---

### Results & Achievements  

- __7% improvement__ in accuracy (ACC) after standardizing irregular numeral patterns  
- An additional __10% improvement__ through GPT-based data augmentation  
- Outperformed rule-based approaches by over 15% accuracy on some sentence types

---

### Conclusion  
This project demonstrated the potential of LLMs in extracting and normalizing numeric expressions in Korean, outperforming traditional rule-based methods.  
In the future, we aim to expand this approach to domain-specific texts such as finance, healthcare, and legal documents.
